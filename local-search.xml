<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Data Analysis for Economics Intro</title>
    <link href="/2021/03/17/Hello-World/"/>
    <url>/2021/03/17/Hello-World/</url>
    
    <content type="html"><![CDATA[<h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>终于有一个相对轻松的学期了，就在这里记录一下茅家铭老师的微观计量课程。这是本学期观感最好的一门课程了，它在泰式英语，连环逼问的门门课程中太过鹤立鸡群，老师也是WISE一顶一的男神，教学资源也是相当好的。（教学网站：<a href="https://github.com/jiamingmao/data-analysis">点击这里</a>。）于是想在这里用通俗一些的话来过一下老师上课的一些内容，然后推导一些公式，也防止后面摸鱼摸得什么都忘了。这也是我的第一篇博文，是有很多不足之处的，写下来也很生疏，是我脑子里意识流的Intro part，详细还是参见茅神的课程吧。</p><h3 id="Data-Science"><a href="#Data-Science" class="headerlink" title="Data Science"></a>Data Science</h3><p>​                               <strong>Machine learning $\to$ Statistics $\to$ Econometrics</strong></p><p>从左到右是一个从结果推断(Prediction)向因果推断(Causality)加强的链。基于统计学知识，前者主要观测特征，后者想找出作用机理。而机器学习模型的准确度实际上是大大超过传统的一些计量方法的，比如Panel Data Regression和Logit Model。但是这里的缺陷在于很难表示出经济学的一些insights，类如随机森林等算法，很难有一个经济学标准的interpretation，而这些却是计量中很需要的。这个问题的出现主要是因为Relevance不能说明Causality，两者的终极目标也因此分开。</p><h3 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h3><p>$x$ 和 $y$ 是两个变量，寻找这两个变量的关系应该是寻找他们的联合概率分布 $p(x,y)$。而实际上我们更多需要的是从 $x$ 得到 $y$ 的过程，即建立一个方程 $f(x)$ 来测定 $y$。</p><h4 id="损失函数选择"><a href="#损失函数选择" class="headerlink" title="损失函数选择"></a>损失函数选择</h4><p>范数选择是由 $f(x)$ 引出的问题，是一个如何衡量”best”的问题。因为当得到预测方程时，我们总有$e = y-f(x)$, 误差 $e$ 越大是对预测模型越不好的。几种损失函数所得到的$f(x)$是完全不同的。</p><h5 id="L1与MAE"><a href="#L1与MAE" class="headerlink" title="L1与MAE"></a>L1与MAE</h5><p>L1范数是$|x_1|+|x_2|+\cdots+|x_n|$, 这里的损失函数是MAE，即：$\frac{1}{n}\sum |e_i|$，MAE是L1正则化的代表 (Lasso回归就是利用了L1的正则化)。最小化MAE产生的将是中位数。具体可以参见<a href="https://www.zhihu.com/question/46664595/answer/102314891">知乎-子元</a>的回答。</p><p>MAE求解的下降速度统一。不同与MSE，会对误差小的项有大的包容性，MAE一视同仁。对于一些噪声异常值，MAE没有很大的反应，这是它相较于MSE优秀的地方。</p><h5 id="L2与MSE"><a href="#L2与MSE" class="headerlink" title="L2与MSE"></a>L2与MSE</h5><p>L2 范数是 $\sqrt{(x_1^2+\cdots+x_n^2)}$ ,这里用于估算损失的函数为MSE，即：$\frac{1}{n}\sum_{i}^ne_i^2\to E[(y-f(x))^2]$ 。L2范数和MSE的最小化过程实际上是一样的，MSE是L2正则化的一个代表 (Ridge回归就是用了L2的正则化)。如果选择这一种方法，得到 $f(x)$ 的过程就是最小化MSE的过程。这一个过程即是对目标函数$f(x)$求导的过程，最后的结果即是$f(x)$ 。<br>$$<br>\partial \sum (y_i-f)^2 = \sum(y_i-f)=0\<br>\to f=\frac{1}{n}\sum{y_i}=E(y|x)<br>$$<br>用MSE的一大优点就是好算。相比于其他的方法，可能还会遇到稀疏等问题，而这个只需要求导就可以得到了。例如<a href="https://www.yuque.com/xiaolin-cuwhx/czefxk/dno66q#6bYBd">线性回归</a>。</p><p>另一个优势是MSE对于异常值是很敏感的。因为次方的关系，一旦 $y-f(x)&gt;1$ ，那么MSE对与这类异常值的惩罚是要比MAE高很多的，这样的性质让MSE有防止过拟合的功效。因为这个代价函数会让$f(x)$尽可能贴近异常值，这会给异常值更大的权重。直觉上来讲，只有平均数可以让每个 $y-f(x)$ 都尽可能地小，从而让MSE最小。</p><p>![](C:\Users\Junwei Lin\blog\blog\themes\fluid\source\img\intro\MAE和MSE.png)</p><h5 id="0-1-Loss"><a href="#0-1-Loss" class="headerlink" title="0-1 Loss"></a>0-1 Loss</h5><p>这里需要建立一个Indicator Function, $\hat{y}$ 是预测值。<br>$$<br>I(y) = \begin{cases}<br>0, y = \hat{y}\<br>1, y\neq \hat{y}<br>\end{cases}<br>$$</p><p>我们将损失函数记为$\sum_i I(y_i)$，自然，能达到最小损失函数的 $\hat{y}$ 是我们想要的。能达到这一要求的是众数(mode)。这一直觉是，找到最有可能是正确的 $y$ 值。</p><p>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>微观计量</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
