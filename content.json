{"pages":[],"posts":[{"title":"Challenge: Ordered Logit","text":"Ordered Logit is in the family of ordinal regression. It‚Äôs widely used when facing ordered categorical outcomes. For example, in bond ratings, there are $AAA, AA, A, B$ etc. Of course, we can simply apply multinomial logistics model to fit it. However, we may lose the information of the ranking in the dependent variables. As a result, ordered logit (or ordered probit) is the ‚Äòevolution‚Äô version. Mechanism of Ordered LogitReminder of Multinomial LogisticsIf we consider the binary logit model, the function should be, Pr(y=1|x) = \\frac{\\exp(x'\\beta)}{1+\\exp(x'\\beta)}, which implies that, \\ln \\frac{Pr(y=1|x)}{Pr(y=0|x)} = x'\\betaExpanding the kind of thoughts to the multinomial part, there is the soft-max function for n choices, \\sigma_j(x'\\beta_j) = \\frac{\\exp(x'\\beta_j)}{\\sum_{i=1}^n\\exp(x'\\beta_i)}The core thinking is that we can pick any two categories to do a binary logistic regression. Even if each pair has different underlying classification criteria (which generates different $\\beta$), we can still merge these regressions together to form the soft-max function. Therefore, the fraction of probabilities is, \\ln \\frac{Pr(y=j|x)}{Pr(y=k|x)} = x'(\\beta_j-\\beta_k)And we use the MLE (Maximum Likelihood Estimation) method to estimate it. Parallel Regression AssumptionNevertheless, if ordered logit is desired, then we cannot belike soft-max function to have many different $\\beta$-s. Simply speaking, the dependent variables of ordinal regression always contain the rank information. Rank always implies comparisons. For example, is it reasonable or fair for a bond credit rating firm to have different grading criteria onto the very bond that is not evaluated yet? The answer is certainly no. We call the assumption as Parallel Assumption or Proportional Odds Assumption. TestsIt is a strong assumption so that we unify the coefficients among categories. Practically, we have many methods to test whether the dataset can pass the assumption. Brant test, Wald test and LR test are the most popular ways to verify it. If not all variables pass the examinations, then a significant result will be thrown out. Generally, Wald test is looser than Brant test. Here, Wald test will be introduced briefly. Wald test is a kind of likelihood ratio test but lowers much more computation burdens than that within LR. WikiPediaen.wikipedia.org/wiki/Wald_test Wald test performs one-by-one in finding the bad variables in ordered logit model within Stata. H_0: \\hat{\\beta} = \\beta_0\\\\ H_1: \\hat{\\beta} \\ne \\beta_0\\\\ W = \\frac{(\\hat{\\beta}-\\beta_0)^2}{Var(\\hat{\\beta})}\\to \\chi^2, where $\\hat{\\beta}$ was found as the maximizing argument of the unconstrained likelihood function and it is compared with a hypothesized one $\\beta_0$. We hope that null hypothesis passes so an insignificant p-value is desired. Details of both tests can tell what variables violate the assumption. It‚Äôs also called Partial Proportional Odds and we will talk about the situation later. Distinguish CategoriesThresholdsYou must have questions about how to distinguish categories under the circumstance of $\\beta$-s being the same. Notice that it indicates that every category shares the same Logit function. And a idea comes out. One variable increases, it would result in a shift toward either end of the spectrum of ordinal responses. Said another way, the probability of responding toward either end of the spectrum would increase as the predictor variables change in a given direction. Therefore, we assume some thresholds $\\alpha$ aiming at dividing the rank for categories. For example, if an index model for a single latent variable $y_i^*=x_i‚Äô\\beta+u_i$ and $u_i$ follows the Gumbel, then, y_i = j \\hspace{1em} \\text{if} \\hspace{1em} \\alpha_{j-1}\\le y_i^*\\le \\alpha_jSuppose it is a health survey with three ($J$) levels, then there should be two ($J-1$) intercepts. What should be kept in mind is that we don‚Äôt know the thresholds beforehand and thresholds are unlikely to be averagely divided. A good example is that good health might have little differences with normal health, but normal health can be much better than unhealthy. Thus, the probability of individual $i$ choosing $j$ (just like the normal health case in the graph) is, Pr(y_i=j) = Pr(\\alpha_{j-1}-x_i'\\beta\\le u_i\\le \\alpha_j-x'_i\\beta) = F(\\alpha_j-x'_i\\beta)-F(\\alpha_{j-1}-x_i'\\beta) , where $F = \\frac{\\exp(z)}{1+\\exp(z)}$, for $u_i$ follows standard $\\text{Gumbel}$ distribution. (Of course, if $u_i$ is distributed under the standard normal, then the $F$ can be the $cdf$ of it. And that is the ordered probit.) Particularly, $Pr(y_i=1)=F(\\alpha_1-x‚Äô\\beta)$ and $Pr(y_i=J) = 1-F(\\alpha_{J-1}-x‚Äô\\beta)$. And the fraction of probability is, \\ln \\frac{Pr(y_i=j|x_i)}{Pr(y_i=k|x_i)} = \\alpha_j-\\alpha_{j-1} - (\\alpha_k - \\alpha_{k-1})As you can see that the equation is $\\beta$-free because it satisfies the parallel assumption. EstimationOnce we have the probability function, we can estimate it by MLE. The log-likelihood function is, \\ln \\mathcal{L}(\\beta, \\vec{\\alpha}|x_i,y_i) = \\sum_{j=1}^J [y_i=j]\\ln [F(\\alpha_{j-1}-x'\\beta)-F(\\alpha_j-x'\\beta)], where $[y_i=j]$ is an Iverson bracket. If we continue using the core thinking in the multinomial logistics here, we can separate each $\\alpha$ independently and the binary (smaller than $j$ or larger than $j$ are the two groups) logistics function is easy to get, \\ln\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)} = \\alpha_j -x'\\beta\\\\ \\Rightarrow Pr(y_i\\le j) = \\frac{\\exp(\\alpha_j-x'\\beta)}{1+\\exp(\\alpha_j-x'\\beta)}And here, we can operate it as if we were doing a binary logistics regression but for many times. InterpretationCoefficientsOne way to interpret the coefficients is via a proportional odds ratio. Say, you can interpret it mainly on the latent variable then interpret it in the exponential way. For example, one ordered logit regression outcome is, \\ln (\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)}) = \\alpha_j + 0.2*\\text{Gender}, where $\\text{Gender}=\\begin{cases}1, \\text{if Male}\\ 0, \\text{if Female} \\end{cases}$. The odds ratio for gender is $\\exp(-\\beta)=\\exp(0.2)=1.22$. Say, the latent variable will increase $0.2$ so will the likelihood of a higher rank. Then the odds of rating for a man is 1.22 times higher than that for a woman. Marginal EffectsAn advantage of the ordered logit regression is that we can estimate the marginal effect of each regressor on the probability that individual $i$ chooses alternative $j$ ($p_{ij}$), \\frac{\\partial p_{ij}}{\\partial x_r} = \\beta_r[f(\\alpha_{j-1}-x'\\beta)-f(\\alpha_j-x'\\beta)]=\\beta_r\\Delta f, where $f$ is the $pdf$ of logit function and $x_r$ is the $r^{th}$ regressor. And the marginal effects across all alternatives for individual $i$ sums up to zero. This interesting property inherits from logit-like functions (like logit function and soft-max function). \\begin{aligned} \\sum_{j=1}^J \\frac{\\partial p_{ij}}{\\partial x_r} &= \\beta_r [f(\\alpha_1-x'\\beta)+f(\\alpha_2-x'\\beta)-f(\\alpha_1-x'\\beta)+\\cdots\\\\ &+f(\\alpha_{J-1}-x'\\beta)-f(\\alpha_{J-2}-x'\\beta)-f(\\alpha_{J-1}-x'\\beta)]\\\\ &=\\beta_r*0=0 \\end{aligned}The interpretation of marginal effect is that each unit increase in independent variable increase/decrease the probability of selecting alternative $j$ by the amount expressed as a percent (for probability is percentage). We can see from the graph that $\\Delta f$ is actually the difference of boundary, if selecting ‚ÄòNormal Health‚Äô. Generalized Ordered LogitRemember that we remained a problem called Parital Proprtional Odds? And we‚Äôd like to talk about it now. If some $\\beta$-s differ across different categories, what should we do? Here comes the Generalized Ordered Logit, which looses the strong parallel assumption while somehow remaining the rank information in regression formula. In Parallel Assumption, we talked about some test like Brant test, Wald test and you can immediately find the variables that violate the assumption. Then, we can reconstruct our formula based on ordered logit model, \\ln\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)} = \\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j}\\\\ \\Rightarrow Pr(y_i\\le j) = \\frac{\\exp(\\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j})}{1 + \\exp(\\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j})}, where the undermark $\\text{not}$ represents not violate. The $\\alpha$-s here inherits from the idea. It is still the threshold to represent the ranks of categories even if every binary logistics doesn‚Äôt share the same $pdf$. What‚Äôs more, the estimation or interpretations are still the same. We can say that ordered logit is a particular of generalized ordered logit. NOTE! In some software packages, Stata as the example, if you use the command gologit to do a generalized ordered logit, there will be a totally different result with ologit even if no $\\beta$ varies. This is because gologit is a user-written package, and the model is, \\ln\\frac{Pr(y_i>j)}{1-Pr(y_i>j)} = \\alpha_j + x'_{\\text{not},i}\\beta_{\\text{not}} + x'_{\\text{vio},i}\\beta_{\\text{vio},j}, where the greater-than sign matters and change all the results. So plz be very careful! Empirical ExampleAbout the DatasetLong and Freese (2006) present data from the 1977/1989 General Social Survey. Respondents are asked to evaluate the following statement: ‚ÄúA working mother can establish just as warm and secure a relationship with her child as a mother who does not work.‚Äù Responses were coded as 1 = Strongly Disagree (1SD), 2 = Disagree (2D), 3 = Agree (3A), and 4 = Strongly Agree (4SA). Practically, the number should impart the rank information. Explanatory variables are yr89 (survey year; 0 = 1977, 1 = 1989), male (0 = female, 1 = male), white (0 = nonwhite, 1 = white), age (measured in years), ed (years of education), and prst (occupational prestige scale). In this example, we will use 3A as the base outcome. Assumption TestAs we mentioned in tests, there are many can figure out whether the dataset satisfies the parallel assumption. 1oparallel And the result is shown below, Test Method Chi2 df P&gt;Chi2 Wolfe Gould 48.91 12 0.000 Brant 49.18 12 0.000 score 48.42 12 0.000 likelihood ratio 49.2 12 0.000 Wald 48.55 12 0.000 For the reason why the degree of freedom is 12, it is because we have 6 variables and each of them provides a $\\chi^2$. A significant test statistic provides evidence that the parallel regression assumption has been violated. 1auto lrf store(gologit2) It‚Äôs an optional command following after the gologit2 regression and we show it here in advance. In the detail, you will find that only yr89, male violate the assumption. variables p-value white 0.7136 ed 0.1589 prst 0.2046 age 0.0743 yr89 0.00093 male 0.00002 Ordered LogitHowever, in order to perform the ordered logit, we kick those two bad variables out. 1ologit warm white age ed prst Here is the result, Variables Coef. white -.4478496* age -.0186559* ed .078407* prst .0053391 /cut1 -2.056959 /cut2 -.2866278 /cut3 1.525569 Generalized Ordered LogitIf we‚Äôd like to take all variables into account, we should use the generalized ordered logit since it doesn‚Äôt require a $\\beta$ that across all categories. And before we operate it in Stata, we must consider the true model that gologit give, which we talked at the note. 1gologit2 warm yr89 male white age ed prst, auto lrf store(gologit2) Note that gologit2 is used when some but not all variables are parallel in $\\beta$. And the result is, Variables 1SD 2D 3A yr89 0.984* 0.534* 0.326* male -0.333* -0.693* -1.098* white -0.383* -0.383* -0.383* age -0.022* -0.022* -0.022* ed 0.067* 0.067* 0.067* prst 0.006* 0.006* 0.006* _cons 2.122* 0.602* -1.048* The model parameterization dictates the interpretation of the odds ratio. Take the odds ratio for gender as example, using Stat‚Äôs estimates, the odds ratio for gender is $exp(-\\beta_{male})$, which is 0.7168, 0.5, 0.3353 when calculating $P(Y&gt;1)$, $P(Y&gt;2)$, $P(Y&gt;3)$. The impact of gender shows heterogeneity. The odds of rating score higher than 1, 2, 3 is respectively 0.7168, 0.5, 0.3353 times lower for man than it is for women. We calculate the accuracy and find it is $43.26\\%$. Comparison to Multinomial LogisticsWe also implement a multinomial logistics regression on the dataset for you to show the difference. 1mlogit warm yr89 male white age ed prst The accuracy here is $42.4\\%$, which is lower than the generalized ordered logit. Thus, generalized ordered logit is more efficient when facing a category with rank. ReferencesList of Works Richard Williams, Generalized Ordered Logit Models Agresti, Alan. ‚ÄúCategorical Data Analysis.‚Äù New York: Wiley, 2002. Jiaming Mao, Data Analysis for Economics Cornell Consulting Unit #91 Econometrics Academy, Ordered Logit and Ordered Probit Picture References&nbsp;&nbsp; Pixiv by Alcxome","link":"/2021/05/04/Challenge-Ordered-Logit/"},{"title":"Data Analysis for Economics Intro","text":"Áªà‰∫éÊúâ‰∏Ä‰∏™Áõ∏ÂØπËΩªÊùæÁöÑÂ≠¶Êúü‰∫ÜÔºåÂ∞±Âú®ËøôÈáåËÆ∞ÂΩï‰∏Ä‰∏ãËåÖÂÆ∂Èì≠ËÄÅÂ∏àÁöÑÂæÆËßÇËÆ°ÈáèËØæÁ®ã„ÄÇËøôÊòØÊú¨Â≠¶ÊúüËßÇÊÑüÊúÄÂ•ΩÁöÑ‰∏ÄÈó®ËØæÁ®ã‰∫ÜÔºåÂÆÉÂú®Ê≥∞ÂºèËã±ËØ≠ÔºåËøûÁéØÈÄºÈóÆÁöÑÈó®Èó®ËØæÁ®ã‰∏≠Â§™ËøáÈπ§Á´ãÈ∏°Áæ§ÔºåËÄÅÂ∏à‰πüÊòØWISE‰∏ÄÈ°∂‰∏ÄÁöÑÁî∑Á•ûÔºåÊïôÂ≠¶ËµÑÊ∫ê‰πüÊòØÁõ∏ÂΩìÂ•ΩÁöÑ„ÄÇÔºàÊïôÂ≠¶ÁΩëÁ´ôÔºöÁÇπÂáªËøôÈáå„ÄÇÔºâ‰∫éÊòØÊÉ≥Âú®ËøôÈáåÁî®ÈÄö‰øó‰∏Ä‰∫õÁöÑËØùÊù•Ëøá‰∏Ä‰∏ãËÄÅÂ∏à‰∏äËØæÁöÑ‰∏Ä‰∫õÂÜÖÂÆπÔºåÁÑ∂ÂêéÊé®ÂØº‰∏Ä‰∫õÂÖ¨ÂºèÔºå‰πüÈò≤Ê≠¢ÂêéÈù¢Êë∏È±ºÊë∏Âæó‰ªÄ‰πàÈÉΩÂøò‰∫Ü„ÄÇËøô‰πüÊòØÊàëÁöÑÁ¨¨‰∏ÄÁØáÂçöÊñáÔºåÊòØÊúâÂæàÂ§ö‰∏çË∂≥‰πãÂ§ÑÁöÑÔºåÂÜô‰∏ãÊù•‰πüÂæàÁîüÁñèÔºåÊòØÊàëËÑëÂ≠êÈáåÊÑèËØÜÊµÅÁöÑIntro partÔºåËØ¶ÁªÜËøòÊòØÂèÇËßÅËåÖÁ•ûÁöÑËØæÁ®ãÂêß„ÄÇ Ê≠£Êñá1&lt;span style=&quot;text-align:center&quot;&gt;Machine learning $\\to$ Statistics $\\to$ Econometrics&lt;/span&gt; ‰ªéÂ∑¶Âà∞Âè≥ÊòØ‰∏Ä‰∏™‰ªéÁªìÊûúÊé®Êñ≠(Prediction)ÂêëÂõ†ÊûúÊé®Êñ≠(Causality)Âä†Âº∫ÁöÑÈìæ„ÄÇÂü∫‰∫éÁªüËÆ°Â≠¶Áü•ËØÜÔºåÂâçËÄÖ‰∏ªË¶ÅËßÇÊµãÁâπÂæÅÔºåÂêéËÄÖÊÉ≥ÊâæÂá∫‰ΩúÁî®Êú∫ÁêÜ„ÄÇËÄåÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÂáÜÁ°ÆÂ∫¶ÂÆûÈôÖ‰∏äÊòØÂ§ßÂ§ßË∂ÖËøá‰º†ÁªüÁöÑ‰∏Ä‰∫õËÆ°ÈáèÊñπÊ≥ïÁöÑÔºåÊØîÂ¶ÇPanel Data RegressionÂíåLogit Model„ÄÇ‰ΩÜÊòØËøôÈáåÁöÑÁº∫Èô∑Âú®‰∫éÂæàÈöæË°®Á§∫Âá∫ÁªèÊµéÂ≠¶ÁöÑ‰∏Ä‰∫õinsightsÔºåÁ±ªÂ¶ÇÈöèÊú∫Ê£ÆÊûóÁ≠âÁÆóÊ≥ïÔºåÂæàÈöæÊúâ‰∏Ä‰∏™ÁªèÊµéÂ≠¶Ê†áÂáÜÁöÑinterpretationÔºåËÄåËøô‰∫õÂç¥ÊòØËÆ°Èáè‰∏≠ÂæàÈúÄË¶ÅÁöÑ„ÄÇËøô‰∏™ÈóÆÈ¢òÁöÑÂá∫Áé∞‰∏ªË¶ÅÊòØÂõ†‰∏∫Relevance‰∏çËÉΩËØ¥ÊòéCausalityÔºå‰∏§ËÄÖÁöÑÁªàÊûÅÁõÆÊ†á‰πüÂõ†Ê≠§ÂàÜÂºÄ„ÄÇ Statistics$x$ Âíå $y$ ÊòØ‰∏§‰∏™ÂèòÈáèÔºåÂØªÊâæËøô‰∏§‰∏™ÂèòÈáèÁöÑÂÖ≥Á≥ªÂ∫îËØ•ÊòØÂØªÊâæ‰ªñ‰ª¨ÁöÑËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É $p(x,y)$„ÄÇËÄåÂÆûÈôÖ‰∏äÊàë‰ª¨Êõ¥Â§öÈúÄË¶ÅÁöÑÊòØ‰ªé $x$ ÂæóÂà∞ $y$ ÁöÑËøáÁ®ãÔºåÂç≥Âª∫Á´ã‰∏Ä‰∏™ÊñπÁ®ã $f(x)$ Êù•ÊµãÂÆö $y$„ÄÇ ÊçüÂ§±ÂáΩÊï∞ÈÄâÊã©ËåÉÊï∞ÈÄâÊã©ÊòØÁî± $f(x)$ ÂºïÂá∫ÁöÑÈóÆÈ¢òÔºåÊòØ‰∏Ä‰∏™Â¶Ç‰ΩïË°°Èáè‚Äùbest‚ÄúÁöÑÈóÆÈ¢ò„ÄÇÂõ†‰∏∫ÂΩìÂæóÂà∞È¢ÑÊµãÊñπÁ®ãÊó∂ÔºåÊàë‰ª¨ÊÄªÊúâ$e = y-f(x)$, ËØØÂ∑Æ $e$ Ë∂äÂ§ßÊòØÂØπÈ¢ÑÊµãÊ®°ÂûãË∂ä‰∏çÂ•ΩÁöÑ„ÄÇÂá†ÁßçÊçüÂ§±ÂáΩÊï∞ÊâÄÂæóÂà∞ÁöÑ$f(x)$ÊòØÂÆåÂÖ®‰∏çÂêåÁöÑ„ÄÇ L1‰∏éMAEL1ËåÉÊï∞ÊòØ$|x_1|+|x_2|+\\cdots+|x_n|$, ËøôÈáåÁöÑÊçüÂ§±ÂáΩÊï∞ÊòØMAEÔºåÂç≥Ôºö$\\frac{1}{n}\\sum |e_i|$ÔºåMAEÊòØL1Ê≠£ÂàôÂåñÁöÑ‰ª£Ë°® (LassoÂõûÂΩíÂ∞±ÊòØÂà©Áî®‰∫ÜL1ÁöÑÊ≠£ÂàôÂåñ)„ÄÇÊúÄÂ∞èÂåñMAE‰∫ßÁîüÁöÑÂ∞ÜÊòØ‰∏≠‰ΩçÊï∞„ÄÇÂÖ∑‰ΩìÂèØ‰ª•ÂèÇËßÅÁü•‰πé-Â≠êÂÖÉÁöÑÂõûÁ≠î„ÄÇ MAEÊ±ÇËß£ÁöÑ‰∏ãÈôçÈÄüÂ∫¶Áªü‰∏Ä„ÄÇ‰∏çÂêå‰∏éMSEÔºå‰ºöÂØπËØØÂ∑ÆÂ∞èÁöÑÈ°πÊúâÂ§ßÁöÑÂåÖÂÆπÊÄßÔºåMAE‰∏ÄËßÜÂêå‰ªÅ„ÄÇÂØπ‰∫é‰∏Ä‰∫õÂô™Â£∞ÂºÇÂ∏∏ÂÄºÔºåMAEÊ≤°ÊúâÂæàÂ§ßÁöÑÂèçÂ∫îÔºåËøôÊòØÂÆÉÁõ∏ËæÉ‰∫éMSE‰ºòÁßÄÁöÑÂú∞Êñπ„ÄÇ L2‰∏éMSEL2 ËåÉÊï∞ÊòØ $\\sqrt{(x_1^2+\\cdots+x_n^2)}$ ,ËøôÈáåÁî®‰∫é‰º∞ÁÆóÊçüÂ§±ÁöÑÂáΩÊï∞‰∏∫MSEÔºåÂç≥Ôºö$\\frac{1}{n}\\sum_{i}^ne_i^2\\to E[(y-f(x))^2]$ „ÄÇL2ËåÉÊï∞ÂíåMSEÁöÑÊúÄÂ∞èÂåñËøáÁ®ãÂÆûÈôÖ‰∏äÊòØ‰∏ÄÊ†∑ÁöÑÔºåMSEÊòØL2Ê≠£ÂàôÂåñÁöÑ‰∏Ä‰∏™‰ª£Ë°® (RidgeÂõûÂΩíÂ∞±ÊòØÁî®‰∫ÜL2ÁöÑÊ≠£ÂàôÂåñ)„ÄÇÂ¶ÇÊûúÈÄâÊã©Ëøô‰∏ÄÁßçÊñπÊ≥ïÔºåÂæóÂà∞ $f(x)$ ÁöÑËøáÁ®ãÂ∞±ÊòØÊúÄÂ∞èÂåñMSEÁöÑËøáÁ®ã„ÄÇËøô‰∏Ä‰∏™ËøáÁ®ãÂç≥ÊòØÂØπÁõÆÊ†áÂáΩÊï∞$f(x)$Ê±ÇÂØºÁöÑËøáÁ®ãÔºåÊúÄÂêéÁöÑÁªìÊûúÂç≥ÊòØ$f(x)$ „ÄÇË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËøôÈáå $y_i$ ÊòØÁªôÂÆö $x$ ‰πãÂêéÁöÑÊï∞ÊçÆÁÇπÔºåÂõ†Ê≠§$f(x)=f$,ÊòØ‰∏Ä‰∏™ÂÆöÂÄº„ÄÇ \\partial \\sum (y_i-f)^2 = \\sum(y_i-f)=0\\\\ \\to f=\\frac{1}{n}\\sum{y_i}=E(y|x)Áî®MSEÁöÑ‰∏ÄÂ§ß‰ºòÁÇπÂ∞±ÊòØÂ•ΩÁÆó„ÄÇÁõ∏ÊØî‰∫éÂÖ∂‰ªñÁöÑÊñπÊ≥ïÔºåÂèØËÉΩËøò‰ºöÈÅáÂà∞Á®ÄÁñèÁ≠âÈóÆÈ¢òÔºåËÄåËøô‰∏™Âè™ÈúÄË¶ÅÊ±ÇÂØºÂ∞±ÂèØ‰ª•ÂæóÂà∞‰∫ÜÔºåÊ±ÇÂØºÁöÑÂÖ∑‰ΩìËøáÁ®ãÊòØÈúÄË¶ÅÊúâÂØπ$f(x)$ÁöÑÈ¢ÑÂÖà‰º∞ËÆ°ÁöÑ„ÄÇ‰æãÂ¶ÇÔºå‰∏ÄÂÖÉÁ∫øÊÄßÂõûÂΩí„ÄÇ Âè¶‰∏Ä‰∏™‰ºòÂäøÊòØMSEÂØπ‰∫éÂºÇÂ∏∏ÂÄºÊòØÂæàÊïèÊÑüÁöÑ„ÄÇÂõ†‰∏∫Ê¨°ÊñπÁöÑÂÖ≥Á≥ªÔºå‰∏ÄÊó¶ $y-f(x)&gt;1$ ÔºåÈÇ£‰πàMSEÂØπ‰∏éËøôÁ±ªÂºÇÂ∏∏ÂÄºÁöÑÊÉ©ÁΩöÊòØË¶ÅÊØîMAEÈ´òÂæàÂ§öÁöÑÔºåËøôÊ†∑ÁöÑÊÄßË¥®ËÆ©MSEÊúâÈò≤Ê≠¢ËøáÊãüÂêàÁöÑÂäüÊïà„ÄÇÂõ†‰∏∫Ëøô‰∏™‰ª£‰ª∑ÂáΩÊï∞‰ºöËÆ©$f(x)$Â∞ΩÂèØËÉΩË¥¥ËøëÂºÇÂ∏∏ÂÄºÔºåËøô‰ºöÁªôÂºÇÂ∏∏ÂÄºÊõ¥Â§ßÁöÑÊùÉÈáç„ÄÇÁõ¥Ëßâ‰∏äÊù•ËÆ≤ÔºåÂè™ÊúâÂπ≥ÂùáÊï∞ÂèØ‰ª•ËÆ©ÊØè‰∏™ $y-f(x)$ ÈÉΩÂ∞ΩÂèØËÉΩÂú∞Â∞èÔºå‰ªéËÄåËÆ©MSEÊúÄÂ∞è„ÄÇ ‰∏ãÂõæÈáåÔºåÂ∑¶ËæπÊòØMAEÔºåÂè≥ËæπÊòØMSE„ÄÇ 0-1 LossËøôÈáåÈúÄË¶ÅÂª∫Á´ã‰∏Ä‰∏™Indicator Function, $\\hat{y}$ ÊòØÈ¢ÑÊµãÂÄº„ÄÇ I(y) = \\begin{cases} 0, y = \\hat{y}\\\\ 1, y\\neq \\hat{y} \\end{cases}Êàë‰ª¨Â∞ÜÊçüÂ§±ÂáΩÊï∞ËÆ∞‰∏∫$\\sum_i I(y_i)$ÔºåËá™ÁÑ∂ÔºåËÉΩËææÂà∞ÊúÄÂ∞èÊçüÂ§±ÂáΩÊï∞ÁöÑ $\\hat{y}$ ÊòØÊàë‰ª¨ÊÉ≥Ë¶ÅÁöÑ„ÄÇËÉΩËææÂà∞Ëøô‰∏ÄË¶ÅÊ±ÇÁöÑÊòØ‰ºóÊï∞(mode)„ÄÇËøô‰∏ÄÁõ¥ËßâÊòØÔºåÊâæÂà∞ÊúÄÊúâÂèØËÉΩÊòØÊ≠£Á°ÆÁöÑ $y$ ÂÄº„ÄÇ Learning fÂØπ‰∫é$f(x)$ÁöÑÂÖ∑‰ΩìÂØªÊâæÊñπÊ≥ïÔºåÊúâ‰∏§‰∏™ÊñπÊ≥ï„ÄÇ Parametric MethodÂÆÉÈúÄË¶ÅÂØπÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÊúâÂü∫Êú¨ÁöÑÁªèÈ™åÂà§Êñ≠ÔºåËøô‰∏ÄÂà§Êñ≠ÊòØÁî®‰∫éÁªôÂÆömodel/hypothesisÁöÑ„ÄÇËøôÂèØËÉΩ‰ºöÂØºËá¥ÊúâWrong Functional FormËøô‰∏Ä‰∏™ÈóÆÈ¢ò„ÄÇÂØπ‰∫éÂ§çÊùÇÁöÑÊï∞ÊçÆÂ∞ÜË¶ÅÈù¢ÂØπapproximation-generalization tradeoff„ÄÇ‰ΩÜËøô‰∏ÄÂ•ΩÂ§ÑÊòØ‰ªñ‰ª¨ÁöÑÁªèÊµéÂ≠¶ÊÑè‰πâÊòØÊúâÊçÆÂèØÂæ™ÁöÑ„ÄÇÂú®Ê†∑Êú¨ÊØîËæÉÂ∞ëÔºåÁª¥Â∫¶ËæÉÂ§öÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèÇÊï∞‰º∞ËÆ°ÊòØË°å‰πãÊúâÊïàÁöÑ„ÄÇÂêéÈù¢‰∏ªË¶Å‰ºöËÆ®ËÆ∫ÁöÑÊòØÂèÇÊï∞‰º∞ËÆ°Ê≥ï„ÄÇ Nonparametric MethodËøôÊòØÈúÄË¶ÅÂæàÂ§öÊï∞ÊçÆÊîØÊíëÊâçËÉΩÂ§ü‰ΩøÁî®ÁöÑ‰∏ÄÁßçÊñπÊ≥ï„ÄÇÂú®MSE‰∏ãÔºåÊ†πÊçÆ$E(y|x)$ÔºåÈùûÂèÇÊï∞‰º∞ËÆ°ÈÉΩÊòØ‰ª•ÂèñÂπ≥ÂùáÂÄº‰∏∫Âü∫Á°ÄÊù•Âª∂‰º∏ÁöÑ„ÄÇ ÂΩìÊüêÂ§ÑÁöÑÊï∞ÊçÆÁÇπ‰∏çÂ§üÂÖÖË∂≥ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞±ÈááÈõÜËøô‰∏Ä‰ΩçÁΩÆÈôÑËøëÁöÑÁÇπÔºåÂç≥Nearest Neighborhood method„ÄÇ ‰ΩÜÊòØËøô‰∏ÄÊñπÊ≥ï‰∏ç‰∏ÄÂÆöÈùûÂ∏∏ÂÆûÁî®„ÄÇËøô‰∏ÄÊñπÊ≥ïÂèóÂà∞È´òÁ∫¨Â∫¶ÁöÑ‚ÄúËØÖÂíí‚ÄùÔºöÂç≥ÔºåÊØèÂ§ö‰∏Ä‰∏™Áª¥Â∫¶ÔºåneighborhoodÁöÑÁ©∫Èó¥‰ºöË¢´ÂéãÊ¶®Âú∞Êõ¥Â∞èÔºåËøôÊ†∑neighborhoodÈáåÁöÑÁÇπÊï∞ÈáèÂ∞±‰ºöÊõ¥Â∞ëÔºåËøôÊ†∑ÂèñÂà∞ÁöÑÂπ≥ÂùáÂÄºÔºàÁîöËá≥ÂèØËÉΩÂèñ‰∏çÂá∫Âπ≥ÂùáÂÄºÔºâÊòØ‰∏çÂ§üÂèØ‰ø°ÁöÑ„ÄÇ Goodness of FitÊãüÂêà‰ºòÂ∫¶ÊòØÂú®È¢ÑÊµã‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÂ±ûÊÄß„ÄÇÂÆÉÂíåÁ≥ªÊï∞ÁöÑÊòæËëóÊÄßÊ≤°ÊúâÂÖ≥Á≥ªÔºåÊòØ‰∏Ä‰∏™Ë°°ÈáèÈ¢ÑÊµãÂá∫ÁöÑÊ®°ÂûãÂíåÁªôÂÆöÁöÑÊï∞ÊçÆÁÇπÂàÜÂ∏ÉÈïøÂæóÂ§öÂÉèÁöÑ‰∏Ä‰∏™ÊåáÊ†á„ÄÇÂú®ËÆ°Èáè‰∏≠ÔºåÊàë‰ª¨ÊúâË∞ÉÊï¥RÊñπËøô‰∏ÄÊåáÊ†áÔºö$\\bar{R}^2 = 1- \\frac{n-1}{n-k-1}\\frac{SSR}{TSS}$. Âç≥ÊÆãÂ∑ÆË∂äÂ∞èÔºåÊãüÂêàË∂äÂ•Ω„ÄÇ ÈÄöÂ∏∏Âú∞ÔºåÂõ†‰∏∫Âè™Êúâ‰∏Ä‰∏™Êï∞ÊçÆÈõÜÔºåÂ¶ÇÊûúÂÖ®ÈÉ®Áî®‰∫éËÆ≠ÁªÉÊ®°ÂûãÔºåÈÇ£‰πàÂÖ∂ÂÆûÂæàÈöæÊúâÁü•ÈÅìÊ®°ÂûãÁöÑÊôÆÈÄÇÊÄß„ÄÇÊâÄ‰ª•‰∏Ä‰∏™Êï∞ÊçÆÈõÜ‰ºöË¢´ÂàíÂàÜÊàêËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ„ÄÇËÆ≠ÁªÉÈõÜ‰∫ßÁîüÊ®°Âûã$g$ÔºåÂÖ∂‰∏≠ÁöÑËØØÂ∑Æ‰∏∫in-sample error„ÄÇËÄåÊµãËØïÈõÜ‰∏≠Ôºå‰ΩøÁî®$g$Êù•ÁÆóÂá∫ËØØÂ∑ÆÔºå‰∏∫out-sample error„ÄÇÊµãËØïÈõÜ‰∏≠ÁöÑËØØÂ∑ÆÔºà‰ª£Ë°®generalizationÔºâÊòØÊàë‰ª¨ÈáçÁÇπË¶ÅÂÖ≥Ê≥®ÁöÑ„ÄÇÂÖ¨Âºè‰∏∫$e_{TE} =\\frac{1}{M}\\sum (y_i-\\hat{f})^2 \\to E[(y-g)^2]$ÔºåÂÆûÈôÖ‰∏ä‰πüÊòØMSE„ÄÇ ÂΩìÁÑ∂ÔºåÂ¶ÇÊûúËøêÁî®Â§çÊùÇÁöÑÊ®°ÂûãÔºåÈÇ£‰πàin-sample errorÊòØË¶ÅÂáèÂ∞èÁöÑÔºåËÄåÊ®°ÂûãÁöÑÁ≤æÂ∫¶Ë∂äÂ§ßË∂äÂèØËÉΩÂØºËá¥out-sample errorÂºÇÂ∏∏ÁöÑÂ§ßÔºå‰ªéËÄåÂ§±ÂéªÊôÆÈÄÇÊÄß„ÄÇËøô‰∏ªË¶ÅÁúã$f(x)$ÁöÑÂáÜÁ°ÆÂàÜÂ∏É„ÄÇËøôÂ∞±ÊòØËøáÊãüÂêà‰∏éÊ¨†ÊãüÂêàÈóÆÈ¢òÔºå‰πüÊòØÂç≥approximation-generalization tradeoff„ÄÇÂú®Hoeffding‚Äôs inequalityÈáåÔºåÊú∫Âô®Â≠¶‰π†ÁöÑÂÖ≥ÈîÆÂú®‰∫éËÆ©$E_{in} = E_{out}$Ôºå‰∏î$E_{out}=0$„ÄÇÊç¢ËÄåË®Ä‰πãÔºåËøôÂ∞±Ë¶ÅÊ±Ç$E_{in}=0$Ôºå$E_{out}\\to E_{in}$„ÄÇÂâçËÄÖÊòØapproximationÔºåÂêéËÄÖÊòØgeneralization„ÄÇ Â¶ÇÊûú‰ªéout-sample errorÊù•ÁúãÔºåÊàë‰ª¨ÂèØ‰ª•Êää‰ªñÂàÜËß£ÊàêBiasÂíåVarianceÔºö \\begin{aligned} E_{out}(g) &= E[(g(x)-f(x))^2]\\\\ &=Var(g-f)+E^2(g-f)\\\\ &=Var(g)+bias^2 \\end{aligned}Ê≥®ÊÑèËøôÈáåÁöÑ$g$ÊòØÊ†πÊçÆËÆ≠ÁªÉÈõÜÂæóÂà∞ÁöÑÔºàÂÖàËßÇÊµãÔºåÂêéÊåëÈÄâÂá∫ÊúÄÈÄÇÂêàÁöÑÂáΩÊï∞$g$ÔºâÔºå‰ªñÂêåÊó∂condition on xÂíåD(ËÆ≠ÁªÉÈõÜ)„ÄÇ ÊâÄ‰ª•ÔºåÂ¶ÇÊûú‰ªéÈ¢ÑÊµãËØØÂ∑ÆÊù•ÁúãÊãüÂêàÂ•ΩÂùèÔºåËøôÂ∞±ÊòØËëóÂêçÁöÑBias-Variance Tradeoff. CausalityÂú®StatisticsÈáåÔºåÊàë‰ª¨Êõ¥ÂÖ≥Ê≥®ÁöÑÊòØRelevance„ÄÇÁÑ∂ËÄåÂú®ËÆ°Èáè‰∏≠ÔºåÊàë‰ª¨Êõ¥Ë¶ÅÂÖ≥Ê≥®Causality„ÄÇ Seeing vs. DoingSeeing ÊòØÁõ¥Êé•ËßÇÊµãÂéÜÂè≤Êï∞ÊçÆÔºåËÄådoingÊòØphysicallyËøõË°åÂä®‰ΩúÔºåÂæóÂà∞ÁªìÊûú„ÄÇ Êàë‰ª¨Áî®Ê∞îÂéãËÆ°‰Ωú‰∏∫‰æãÂ≠ê„ÄÇSeeingÔºöÊ∞îÂéã‰ΩéÁöÑÊó∂ÂÄôÊØîÊ∞îÂéãÈ´òÊó∂ÂÆπÊòì‰∏ãÈõ®„ÄÇDoingÔºöÊâãÂä®Ë∞ÉÊï¥Ê∞îÂéãËÆ°Ôºå‰∏çËÆ∫ÊÄé‰πàË∞ÉÊï¥Ê∞îÂéãÔºå‰ªäÂ§©‰∏ãÈõ®ÁöÑÂèØËÉΩÊÄßÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºà‰∏éÊ∞îÂéãËÆ°Êó†ÂÖ≥Ôºâ„ÄÇÊàë‰ª¨ÂèØ‰ª•‰∫ÜËß£Âà∞ÔºåSeeingÂèØ‰ª•ËßÇÊµãRelevanceÔºåDoingÂèØ‰ª•ÁúãÂá∫ÊòØÂê¶Êã•ÊúâCausalityÔºàÂ¶ÇÊûúË∞É‰ΩéÊ∞îÂéãËÆ°‰ΩøÂæóÂÆπÊòì‰∏ãÈõ®ÔºåÈÇ£‰πàÊúâcausalityÔºâ„ÄÇ Âè¶‰∏Ä‰∏™ÊòØABÂåªÈô¢ÁöÑ‰æãÂ≠êÔºå‰∏§‰∏™ÂåªÈô¢ÁöÑÊ≤ªÊÑàÁéá‰∏çÂêåÔºàObservationÔºâ„ÄÇÊúâÂèØËÉΩÈáçÁóáÈÉΩÈÄÅÂæÄÂ•ΩÁöÑÂåªÈô¢Ôºå‰ΩÜÂèçËÄåÂØºËá¥ÂÖ∂Ê≤ªÊÑàÁéá‰∏ãÈôç‰∫Ü„ÄÇÁé∞Âú®‰∏Ä‰∏™‰∫∫ÁîüÁóÖÔºåÈÄÅÂéªÂ•ΩÁöÑÂåªÈô¢Ê≤ªÊÑàÁöÑÂèØËÉΩÊÄßÈ´òÔºàCausalityÔºâ„ÄÇ Just do itÂú®Statistics learning‰∏≠ÔºåÊàë‰ª¨Áü•ÈÅìÂæóÂà∞$f(x)$ÁöÑËøáÁ®ãÔºå$f(x)$ÊòØÁ†îÁ©∂relevanceÁöÑ‰∫ßÁâ©„ÄÇÂ¶ÇÊûúÊàë‰ª¨ÊûÑÈÄ†‰∏Ä‰∏™Êï∞ÊçÆÈõÜÔºåÈÉΩÁî±doËøô‰∏ÄË°åÂä®ÂæóÂà∞ÔºåËøôÊ†∑ÔºåÊï∞ÊçÆÊú¨Ë∫´ÁöÑrelevanceÊòØÁî±causalityËÄåÊù•„ÄÇÂç≥ÔºåËøôÊó∂ÂÄôÊàë‰ª¨ÁöÑ$f(x)$Â∞±ÊòØcausality„ÄÇÊâÄ‰ª•Á†îÁ©∂causalityÁöÑÊñπÊ≥ïÔºåÂ∞±ÊòØÁõ¥Êé•ËøõË°ådo operationÔºåÂ∞ΩÈáèÊûÑÈÄ†‰∏Ä‰∏™RCT„ÄÇ Causality DiagramCommon causeÂíåcommon effectÊòØ‰∏§‰∏™ÂÖ∏‰æã„ÄÇ ÂØπ‰∫écommon causeÔºåÊàë‰ª¨ÁöÑ‰æãÂ≠êÊòØÔºåÂÅáËÆæÊäΩÁÉü‰ºöÂØºËá¥Â∏¶ÊâìÁÅ´Êú∫(A)ÂíåÁôåÁóá(Y)Ôºå‰∏îÂè™ËÄÉËôëËøô‰∏â‰∏™Âõ†Á¥†„ÄÇÈÇ£‰πàÔºåÊàë‰ª¨Ëã•Á†îÁ©∂Â∏¶ÊâìÁÅ´Êú∫ÈÄ†ÊàêÁôåÁóáÁöÑcausalityÔºåÂ∞±‰ºöÂõ†‰∏∫common causeËÄå‰∫ßÁîü‰∏ÄÂÆöÁöÑËÅîÁ≥ª„ÄÇÂõ†‰∏∫Êàë‰ª¨ËÆ©‰∏Ä‰∏™‰∫∫$do(A=1)$Ôºå‰ªñÂæóÁóÖËøòÁî±ÊòØÂê¶Âê∏ÁÉüÂØºËá¥ÔºåÂç≥$E(Y|A)\\neq E(Y|do(A=1))$„ÄÇÊ≠§Êó∂ÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÂõ∫ÂÆöÊµãËØï‰∫∫‰∏∫Âê∏ÁÉüÊàñ‰∏çÂê∏ÁÉü„ÄÇÂ∞±ÂèØ‰ª•ÂæóÂà∞ÊØîËæÉÁ∫ØÁ≤πÁöÑcausality„ÄÇ \\begin{aligned} E[y|do(x)] &= \\sum E[y|do(x),z]p(z)\\\\ &=\\sum E[y|x,z]p(z) \\end{aligned}ËøôÈáåÔºåÊàë‰ª¨ÁöÑ$E$ Â∞±ÂèØ‰ª•apply‰∏Ä‰∫õÊ®°ÂûãÔºåÂ∞±ÂÉèÊàë‰ª¨Âú®statisticsÈáå‰∏ÄÊ†∑„ÄÇ","link":"/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/"},{"title":"Report - VC Dimension","text":"VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a large VC dimension implies a powerful and complex model. However, it may not always be a good signal. If the VC dimension is far away large, then there will be problems in out-sample error, which may present as the overfitting problem. This report aims at discussing why the VC dimension is required and how it comes into being based on the process of machine learning. Notice that the thinking track of the report is based on the lecture Jiaming Mao presents. Figures are from the slides and the Internet. Let‚Äôs talk about the feasibility of machine learning first. Feasibility ConditionsProcess of Machine LearningMachine learning is predicting the value of $y$ based on the value of $x$. And the picture below shows the process of a learning. $f(x)$ is the underlying mechanism, so called target function. And $\\mathcal{H}$ is a set of candidate formula (also called hypothesis set or model), from which, through the algorithm $\\mathcal{A}$ (calculating the loss), we can choose a ‚Äúbest‚Äù hypothesis $g$ that approximately equals to $f(x)$. The error emerged within the training data is called in-sample error and the one generated in testing data is called out-sample error. In such a process, we‚Äôd like to get $E_{out}=0$ if possible. Nevertheless, we take a compromise and make it as $E_{out}\\approx0$ for there always are some disturbing terms. Now, a problem is that we can only observe $E_{in}$, which is the experience from the training data. Thus, we wonder whether $E_{out}\\approx E_{in}\\approx0$ such that $E_{in}$ can be a measurement. And it remains a problem here. Hoeffding‚Äôs InequalityHere we have an example, which is drawing marbles. The bin contains many many differently-colored marbles, and in this case we‚Äôd like to figure out the probability of red marbles in the bin, denoted by $\\mu$. It is impossible for us to count every marbles and what we can do is to draw samples. Then, we denote the probability of red marbles in the samples as $\\nu$. According to the law of large numbers, intuitively, the larger the sample is, the closer the distance between $\\mu$ and $\\nu$. One of the pattern is the Hoeffding‚Äôs inequality, (if the proof is wanted, click here.) Pr(|\\mu-\\nu|>\\epsilon)\\epsilon)\\epsilon)\\cup\\cdots] &\\le Pr[(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon)\\ \\text{or}\\ (|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon)\\cdots]\\\\ &\\le Pr(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon) + Pr(|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon) + \\cdots\\\\ &\\le \\sum_{i=1}^M 2\\exp(-2\\epsilon^2N) = 2M\\exp(-2\\epsilon^2N) \\end{aligned}, $M$ denotes the number of hypothesis in $\\mathcal{H}$. Clearly, if $M$ is large, then the model is complex. And let‚Äôs summarize the inequality as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2M\\exp(-2\\epsilon^2N)To sum up, there are two conditions if machine learning is feasible. The Algorithm $\\mathcal{A}$ can find a $g$ such that $E_{in}\\approx0$, and it is consistent with the process ‚Äútrain‚Äù. Actually, the more complex the model is, the smaller the $E_{in}$ is. $M$ is finite and a $N$ is sufficiently large to make $E_{out}\\approx E_{in}$, and it is consistent with the process ‚Äútest‚Äù. Next, we‚Äôd like to focus on the second condition, and mainly talk about the $M$. VC DimensionGenerally, $\\mathcal{H}$ contains infinite $h$. For example, a simple linear model $y=ax+b$ generates infinite $h$, because the parameters are changeable. How do we shrink the upper bound by substitute $M$? Effective Number of HypothesisIn the process that expanding Hoeffding‚Äôs inequality, the inequality below is used, Pr(A_1\\ \\cup\\ A_2\\ \\cup\\ A_3\\ \\cdots) \\le \\sum_{i=1}^M Pr(A_i)If and only if $A_i$s are independent, we choose the equal sign. Actually, there are many intersection part in $\\mathcal{H}$. A simple example is shown in the graph. In the two-dimensional space, we can simply divide $\\mathcal{H}$ into two parts. One judges that $x_1$ is positive, like $h_1$. Another one marks $x_1$ as negative, like $h_2$. Here, the effective number of hypothesis is two. Under a specific sample, the effective number is fixed. Thus, the inequality can be written as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2*\\text{effective}(M)*\\exp(-2\\epsilon^2N)Growth FunctionDichotomyFirstly, we‚Äôd like to introduce a binary target function and hypothesis set that contains $h:\\mathcal{X}\\to\\{-1,+1\\}$. And, h(X_1,X_2,\\cdots,X_n) = (h(X_1), h(X_2), \\cdots, h(X_n))is called one dichotomy. Generally speaking, dichotomy represents a result which marks all points in the sample. $\\mathcal{H}(X_1,X_2,\\cdots,X_n)$ denotes the number of all dichotomies generated by hypothesis set $\\mathcal{H}$, given $n$ points. For instance, if three points are in the training set, One dichotomy is $(h_1(X_1), h_1(X_2), \\cdots) = (+1,+1,+1)$. Another dichotomy can be $(h_2(X_1), h_2(X_2), \\cdots) = (-1,+1,+1)$. ‚Ä¶ $2^3=8$ is the maximum dichotomies that $\\mathcal{H}(X_1,X_2,\\cdots)$ can value. Say, the number of dichotomies a $\\mathcal{H}$ can generate at most is $2^n$. Shatter Given n points, (and their locations are fixed,) if a hypothesis set $\\mathcal{H}$ can generate exactly $2^n$ dichotomies, we call $\\mathcal{H}$ shatters these points. An example is under two-dimensional dataset and the hypothesis set contains all linear model. There are three points, no matter what points are black, there always be a line that can separate black points from white points. (See the figure below.) As we mentioned above, three points have eight dichotomies. And black and white points are like the $+1,-1$ marks. If the model can tell the marks apart in every scenario, then we can say that the model, or $\\mathcal{H}$, shatters the points. However, if there are four points, then the linear model cannot shatter them. We call the smallest number of points as Break Point. Say, 4 is the break point for the two-dimensional linear regressor. Growth FunctionWe define the growth function as, m_\\mathcal{H}(n)=\\max_{X_1,X_2,\\cdots,X_n \\in \\mathcal{X}} |\\mathcal{H}(X_1,X_2,\\cdots,X_n)| That is, $m_\\mathcal{H}(n)$ is the maximum possible number of dichotomies $\\mathcal{H}$ can generate on a dataset of n points. Obviously, $m_\\mathcal{H}(n)\\le2^n$. The larger the $m_\\mathcal{H}(n)$ is, the more powerful the model is. And $m_\\mathcal{H}(n)$ can tell us whether the dataset of size $s$ can be shattered by $\\mathcal{H}$. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) Now, can we simply exchange $\\text{effective}(M)$ to $m_\\mathcal{H}(n)$ to shrink the upper bound? VC boundNo, because the upper bound for $m_\\mathcal{H}(n)$ is still an explosively increasing function. It is hard to tell whether the result can converge to zero. Sauer‚Äôs Lemma solves the problem. If the break point $k$ exists, then $m_\\mathcal{H}(n)$ is a polynomial. According to Sauer‚Äôs Lemma, m_\\mathcal{H}(n) \\le \\sum_{i=0}^{k-1} \\begin{pmatrix} N\\\\ i \\end{pmatrix}\\le\\begin{cases} N^{k-1}+1,\\\\ (\\frac{eN}{k-1})^{k-1}, \\text{if } N\\ge k-1 \\end{cases} And the polynomial is much easier to converge and it releases a signal that it is feasible to make the upper bound to be zero! Through some complicated transformations, the inequality can be written as, (the proof is here) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*m_\\mathcal{H}(2N)*\\exp(-\\frac{1}{8}\\epsilon^2N) This inequality is the VC bound and the exponential part has a higher rate of convergence than the polynomial part. Thus, the inequality tells that if there is a break point, when $N$ is sufficiently large, for any hypothesis $h$, $E_{out}\\to E_{in}$ is very possible. Then the machine learning in the space is feasible, which meets the second condition. If the significance level is $\\delta$, and the upper bound should be $\\delta$. Then, \\Rightarrow Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le\\delta\\\\ \\epsilon = \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\\\ E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}Ôºå\\text{probability 1} -\\deltaThe bound is called VC generalization bound, which is usually applied in training model because we are required to select a best hypothesis. But, we will not apply it onto the testing. Instead, we use the original Hoeffding‚Äôs Inequality for there is only one hypothesis and the VC bound is much looser. Why is VC bound so loose? The basic Hoeffding‚Äôs inequality used in the proof already has a slack. Using $m_\\mathcal{H}(N)$ to quantify the number of dichotomies on N points, regardless of which $N$ points are in the dataset, gives us a worst-case estimate (because we expanded the inequality to all $h$). Bound $m_\\mathcal{H}(N)$ by a simple polynomial. VC DimensionNow, we can talk about the definition of VC dimension. The Vapnik-Chervonenkis (VC) dimension of $\\mathcal{H}$, denoted $d_{vc}(\\mathcal{H})$ is the size of the largest data set that $\\mathcal{H}$ can shatter. Remember that we have already talked about this problem in part Growth Function. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) And here, $d_{vc}(\\mathcal{H})=3$. If $m_\\mathcal{H}(n)=\\infty$, then $d_{vc}(\\mathcal{H})=\\infty$. In general, $d_{vc}(\\mathcal{H})=k-1$, where $k$ is the break point. If the VC dimension is large, the model $\\mathcal{H}$ is complex because a large sample can be shattered by it. Thus, VC dimension tells the ability of learning for the model. The definition of VC dimension has nothing to do with the locations of points in dataset, which leads to a problem. For example, again, under two-dimensional plane, there are three points. However, this time, they are on the same line (See the graph). A linear model cannot shatter the dataset now. Therefore, VC dimension tells you the largest dataset $\\mathcal{H}$ can shatter, but not every same-sized dataset can be shattered. The VC bound can also be written as, (the first inequality in Sauer‚Äôs Lemma is usually used.) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*((2N)^{d_{vc}(\\mathcal{H})}+1)*\\exp(-\\frac{1}{8}\\epsilon^2N)Approximation-Generalization TradeoffRecall the two conditions if machine learning is feasible, and now, the two conditions should be, The algorithm $\\mathcal{A}$ can find the $g$ that makes $E_{in}\\approx0$. Actually, the larger the VC dimension is (the more complex the model is), the smaller the $E_{in}$ is. The VC dimension are finite to make the polynomial part easy to converge and $N$ should be sufficiently large so that $E_{out}\\approx E_{in}$. Also, recall the relationship that we talked in VC bound, E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}Ôºå\\text{with probability 1} -\\deltaLet $R = \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}} $. If $N$ holds constant, then if $d_{vc}(\\mathcal{H})$ increases, then $R$ will increases, which means the bound will become loose. As a result, $E_{out}$ may get away from $E_{in}$. Nevertheless, $\\mathcal{H}$ will be more complicated. Say, $E_{in}$ decreases, even reaches the scenario that $E_{in}=0$. We call it approximation-generalization tradeoff. When one condition gets too tight, another one will be hard to meet. Therefore, we‚Äôd better find a point where we can balance these two well. And based on the inequality, it is how the VC dimension functions to make machine learning feasible. SummaryStarting from the two feasibility conditions, this report mainly discusses how the VC dimension comes into being and how should we understand VC dimension. VC dimension is important because it regulates the two conditions for machine to learn, which is the approximation-generalization tradeoff. Actually, mathematics is just a tool for us to shrink the bound and VC dimension is the aggregation of them, which can help us study further in machine learning. References Jiaming Mao, Data Analysis for Economics Liubai01‚Äôs blog, Êú∫Âô®Â≠¶‰π†Êé®ÂØºÂêàÈõÜ01 ECE 901 Lecture 19 Yaser &amp; Malik &amp; Hsuan-tien Lin, Learn from Data [M], California Institute of Technology Hsuan-tien Lin, Machine Learning Foundation Picture References&nbsp;&nbsp; Vector Landscape Vectors by Vecteezy &nbsp;&nbsp; Data Analysis by Jiaming Mao &nbsp;&nbsp; Machine Learning by Hsuan-tien Lin","link":"/2021/03/29/VC%20Dimension/"},{"title":"Report - Regression Splines","text":"Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline belongs to non-parametric family and it is the feature that endows the regression high flexibility. This report aims to tell you a story about Why we need Regression Splines, How we make the regression ‚ÄúSpline‚Äù. However, before the story, it‚Äôs inevitable to talk about some basic regression. Let‚Äôs briefly review them. Intuitively, we simulate dots based on $f(x)=\\sin(x), x\\in[-\\pi,2\\pi]$ with the Gaussian-distributed error. For the graph, a code will be attached. If you ‚Äòd like to see the code, please just unfold it. Click to Unfold12345678910111213141516import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinenp.random.seed(15)## Generate 150 dotsx = np.linspace(-np.pi,2*np.pi,150)y0 = np.sin(x)e = np.random.normal(0,1,150)y = y0+e # Add error term# plotfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x,y,color='lightgrey')ax.plot(x,y0,color='r') # oringinal traceplt.show() From Linear to PolynomialLinear RegressionAt the beginning, we always apply the linear regression. Say, the hypothesis set $\\mathcal{H}=\\{h(x)\\} $ consisting of linear functions, assuming there are $N$ samples and $p$ dimensions per sample. h(x) = x'\\beta,\\ \\ \\text{where } x=\\begin{pmatrix} 1\\\\ x_1\\\\ \\vdots\\\\ x_p \\end{pmatrix} ,\\ \\beta=\\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p \\end{pmatrix}And the $\\beta$ determines whether the function is the best one. Of course, L2 loss is always chosen to measure best. The calculation of $E_{in}$ can be written below, \\begin{aligned} E_{in} &= \\frac{1}{N}\\sum_{i=1}^N (x_i'\\beta-y_i)^2\\\\ &= \\frac{1}{N}\\left|\\begin{array}{c} x_1'\\beta - y_1 \\\\ \\vdots \\\\ x_N'\\beta - y_N \\end{array}\\right|^2\\\\ &=\\frac{1}{N}\\left|X\\beta-\\vec{y}\\right|^2,\\ \\text{where } X = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 1 & x_{N1} & \\cdots & x_{Np} \\end{bmatrix} \\end{aligned}To minimize it, the gradient should be zero, \\frac{\\partial}{\\partial \\beta}\\frac{1}{N}(\\beta'X'X\\beta-2\\beta'X'\\vec{y}+\\vec{y}'\\vec{y})=\\vec{0}\\\\ \\frac{2}{N}(X'X\\beta-X'\\vec{y})=\\vec{0}\\\\ \\beta = (X'X)^{-1}X'\\vec{y}Be sure to remember the equation that generates best $\\beta$, it will be the foundation for the following regressions. And the estimator is BLUE (best linear unbiased estimator), proved by Gaussian-Markov Theorem. Click to Unfold1234567891011121314X = np.ones((150,2))X[:,1] = x # Matrix X is generatedY = np.zeros((150,1))Y[:,0] = y # Matrix Y is generatedpseudo = np.dot(np.linalg.inv(np.dot(X.T,X)), X.T) # Calculate the pseudo-inversebeta = np.dot(pseudo,Y)y_lin = beta[0]+beta[1]*xfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x, y, color='lightgrey')ax.plot(x, y0, color='r', label='Original') # oringinal traceax.plot(x, y_lin, color='y', label='Linear')ax.legend()plt.show() PolynomialBasis ExpansionActually, there are many underlying functions that are not typically linear. Then, the question is how to deal with the non-linear terms. Our solution is simply to regard each non-linear term as a whole ‚Äúdegree-one‚Äù term. For example, the model we‚Äôd like to apply is $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2$. The only non-linear part is $x^2$, and we use $\\phi(x)=x^2$ to replace it. The result is $h(x)=\\beta_0+\\beta_1x+\\beta_2\\phi(x)$. Practically, we this method can be applied on linear terms to build a kind of consistency. Say, $\\phi$ can be any function including dummies and constants. $\\phi(x)$ is called basis function. A linear basis function model is defined as, y=\\sum_{i=1}^M \\beta_i\\phi_iJiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Linear basis function model ensures that non-linear regression can be calculated by OLS because it shows the consistent linear form. And, $\\hat{\\beta}=(\\Phi‚Äô\\Phi)^{-1}\\Phi‚ÄôY$, where $\\Phi=(\\phi_1,\\cdots,\\phi_M)$‚Äô. Polynomial RegressionWe assume that the dots we simulated is underlying a cubic polynomial function, which is, specifically, $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3 x^3$. Follow the procedure, we can get a regression curve shown in the first graph. However, when we increases the polynomial degree, the regression curve tends to be more and more abnormal. Actually, it‚Äôs a problem called overfitting, where $E_{out}$ is large while $E_{in}$ is very small. (In order to roughly compare the regression models, $R^2$ is also attached on figures.) In statistics, overfitting is ‚Äúthe production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably‚Äù. LeineberD.J (2007). Stupid data miner tricks Click to Unfold1234567891011121314151617181920212223242526272829303132333435363738394041def polynomial_regression(x,y,degree=3): #define a polynomial regression function Y = np.zeros((len(y),1)) Y[:,0] = y # Matrix Y is generated polynomial_X = np.ones((len(x),degree+1)) for i in range(degree): polynomial_X[:,i+1] = np.power(x,i+1) polynomial_pseudo = np.dot(np.linalg.inv(np.dot(polynomial_X.T,polynomial_X)), polynomial_X.T) # Calculate the pseudo-inverse polynomial_beta = np.dot(polynomial_pseudo,Y) return np.dot(polynomial_X, polynomial_beta)def R2(y,predict_y): # R-square is attached my = np.mean(y) denominator = np.sum((y-my)**2) numerator = np.sum((predict_y-my)**2) return numerator/denominatord3y = polynomial_regression(x,y,3)d5y = polynomial_regression(x,y,5)d7y = polynomial_regression(x,y,7)d9y = polynomial_regression(x,y,9)fig, axes = plt.subplots(2,2,figsize=(10,10))for i in [0,1]: for j in [0,1]: axes[i][j].scatter(x,y,color='lightgrey') axes[i][j].plot(x,y0,color='r',label='Original trace') axes[i][j].grid()axes[0][0].plot(x, d3y, color='g', label='3 Degree Polynomial')axes[0][0].legend()axes[0][0].annotate('$R^2={:.5f}$'.format(R2(y,d3y)), xy=(-2,2))axes[0][1].plot(x, d5y, color='g', label='5 Degree Polynomial')axes[0][1].legend()axes[0][1].annotate('$R^2={:.5f}$'.format(R2(y,d5y)), xy=(-2,2))axes[1][0].plot(x, d7y, color='g', label='7 Degree Polynomial')axes[1][0].legend()axes[1][0].annotate('$R^2={:.5f}$'.format(R2(y,d7y)), xy=(-2,2))axes[1][1].plot(x, d9y, color='g', label='9 Degree Polynomial')axes[1][1].legend()axes[1][1].annotate('$R^2={:.5f}$'.format(R2(y,d9y)), xy=(-2,2))plt.show() A high-degree polynomial does fit samples well, but will the underlying mechanism really perform in that way? What if it meets some extrapolating data? Actually, we are always trapped in a thought that all samples follow a global mechanism. The global regression with high orders induces Runge‚Äôs Phenomenon (overfitting problem). How about break the domain into pieces? Say, piecewise function. Regression SplinePiecewise RegressionPiecewise regression breaks the input space into distinct regions and fit a different relationship in each region. Jiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Piecewise PolynomialsAs mentioned above, a piecewise polynomial function is obtained by dividing the domain of ùëã into contiguous intervals and representing the function by a separate degree-d polynomial in each interval. In mathematics, if there are $n$ knots, h(x) = \\begin{cases} \\beta_0\\phi_0(x), & x","link":"/2021/03/31/Report-Regression-Spline/"},{"title":"Ridge and Lasso Regression","text":"Ridge and Lasso Regressions are expansions of OLS. The key difference is that they restrict the coefficients in loss function. You may wonder why we should implement constraints and this report tries to explain the reason. Of course, there must be some problems in OLS and let‚Äôs have a look. Problems of Ordinary Least SquaresIn this report, I plan to construct a view towards it based on linear algebra. Basically, we have $Ax=b$ for $m$ equations and $n$ features and they are contained into $A$ and $b$. OLS is born to tackle the problem where we tend to give the appropriate solution but we ‚Äòcannot solve it‚Äô. Said another way, what can we do if $A$ is singular ($b$ is out of the column space $C(A)$)? And this situation is quite often when $m\\ne n$. We can show it in the figure 1 where the line is the column space consisting of all the linear combinations of $a$ (basis). However, if we were forced to get the solution, we can only get the closest one. And that‚Äôs what regression typically does. We should select $v_1$ instead of any others, like $v_2$, because $e_1$ is the shortest segment, which puts $v_1$ where it is. Based on the perpendicular condition (projection on $a$), we get, v_1 = ax, \\text{for }v_1\\parallel a\\\\ a^T(b-ax) = 0, \\text{for }a\\perp e_1\\\\ \\Rightarrow a^Tax=a^Tb\\\\ \\Rightarrow x = (a^Ta)^{-1}a^Tb, where $x$ is the coefficient of $a$ and it is the solution. In practice, if you calculate, \\min ||b-xa||^2, then $a^Tax=a^Tb$ is the F.O.C of it. Of course, the matrix operating process can be applied in the higher-dimensional cases. It is the perpendicular property (uncorrelated) that makes the OLS estimator unbiased. However, the crucial part is that we require $a^Ta$ is invertible! Note that $Rank(a^Ta)=Rank(a)$. Therefore, we will easily proceed further, which is $a$ should have a full column rank. When the requirement is not satisfied (a ill-conditioned $A$), there are two probabilities, $m\\ge n$, but linearly dependent vectors (Or highly correlated ones) ‚Äòdecrease‚Äô the rank. $m&lt;n$. Some vectors must be linearly dependent. Thus, we need Ridge regression and Lasso regression to solve the multi-collinearity problem. And let‚Äôs see how. Ridge RegressionBiased EstimatorRidge regression is widely used when most features are important, which is the problem one. Ridge regression solve the method by building a ‚Äòridge‚Äô, which is adding $\\lambda I$ ($\\lambda&gt;0$) onto the $A^TA$. Say, if a full rank matrix is plus in, then the matrix $A^TA$ will also be nonsingular. However, one condition is $A$ should be standardized. Now, the solution to $x$ is, x = (A^TA+\\lambda I)^{-1}A^TbLike real numbers, an adding term makes the inverse a smaller multiplier. As a result, $x$ will become smaller. For simplicity, suppose $A$ is an orthogonal matrix. Say, $A^TA=I$. Then, $x_{OLS}=(A^TA)^{-1}A^Tb=A^Tb$, x_{Ridge} = (I+\\lambda I)^{-1}x_{OLS}=\\frac{x_{OLS}}{1+\\lambda} Thus, you can clearly notice that ridge regression is a shrink on OLS estimator but how to imagine it intuitively and what about a common case? After simple transposition in dimension one, we get, a^T(b-ax)=\\lambda x,so the perpendicular condition is not satisfied. As a not surprising result, the answer is not closest any more, which leads it as a biased estimator. We can draw the Fig2, Since that we will have $ax_r+pa=x_{OLS}$ (where $x_r$ is the ridge estimator) in the figure, if we go deeper for the shrinkage details, \\begin{aligned} a^T(b-ax_r) &= \\lambda x_r\\\\ ||a||*||b-ax_r||*\\cos \\theta &=\\lambda x_r\\\\ p*||a||^2 &= \\lambda x_r\\\\ p &= \\frac{\\lambda x_r}{||a||^2}\\\\ \\Rightarrow ax_r+ap &= ax_{OLS}\\\\ \\Rightarrow x_r + \\frac{\\lambda x_r}{||a||^2}&=x_{OLS}\\\\ \\Rightarrow x_r &= \\frac{||a||^2}{||a||^2+\\lambda}x_{OLS} \\end{aligned}Note that the brown dashed segment is $b-ax$ and $x_r$ denotes the ridge estimator. As you can see, orthogonal cases is special one because $a$ is in a unit length. The result shows that it is because the shrinkage that makes $x_r$ biased. If $\\lambda=0$, ridge regression is actually applied the OLS. If $\\lambda\\to\\infty$, $x_r$ will be really close to zero but not real zero, which makes it suit for problem one (maintaining all features). Say, the larger the $\\lambda$ is, the more biased the estimator is. In mathematics, the amount of bias is, ($E(x_{OLS})=\\beta$, $\\beta$ is the true solution) E(x_r-x_{OLS}) = [(A^TA+\\lambda I)^{-1}-(A^TA)^{-1}]A^TbRelationship with SVDSVD DecompositionIn linear algebra, we have an extraordinary magic called SVD (singular value decomposition). In mathematics, A = UDV^T , where $U$ and $V$ are orthogonal matrices. If we apply the magic to $\\hat{y}=Ax_r$, \\begin{aligned} \\hat{y} &= A(A^TA+\\lambda I)^{-1}A^Tb\\\\ &= UDV^T(VD^2V^T+\\lambda VV^T)^{-1} VDU^Tb,\\text{ if orthogonal, } VV^T=I\\\\ &= UDV^T[V(D^2+\\lambda)V^T]^{-1}VDU^Tb\\\\ &= UD(D^2+\\lambda)^{-1}DU^Tb\\\\ &=\\sum_{i=1}u_i\\frac{d_i^2}{d_i^2+\\lambda}u_i^Tb \\end{aligned}, where $D$ is diagonal. Compare with $\\hat{y}=Ax_{OLS}$, \\begin{aligned} \\hat{y}&=A(A^TA)^{-1}A^Tb\\\\ &=UDV^T(VD^2V^T)^{-1}VDU^Tb\\\\ &=UU^Tb\\\\ &=\\sum_{i=1}u_iu_i^Tb \\end{aligned}We can rewrite $\\hat{y} = Ax_{OLS}$ to be $\\hat{y}=Ux^U_{OLS}$, since $U$ is orthogonal. (Notice that $U^TUx=U^Tb\\to x=U^Tb$). We can treat it as if projecting on $U$. So does the $\\hat{y}$ of ridge. \\hat{y} = \\underbrace{(D^2+\\lambda)^{-1}}_{\\text{coefficient}}\\ \\ UD(UD)^Tb = UDx_r^USay, we can treat it as if the input for the projection was $UD$ instead of $A$, and we should multiply a coefficient which is included in $x^U_r$. Actually, we are using a new variable, which is more concentrated, to replace original variables. You might realize that it is the PCA (Principal Component Analysis). Linkage to PCAFor data $X$, we have covariance matrix $C=X^TX/(n-1)$. In eigen-decomposition, we have $C=Q\\Lambda Q^T1$, for $C$ is symmetric. $\\Lambda$ is diagonal of its all eigenvalues with decreasing order, and $Q$ is an orthogonal eigenvector matrix. Eigenvectors are Principal Axes. Projections on the axes are the Principal Components. The $j$-th principal component is given by the $j$-th column of $XQ$. ($XQ=\\lambda Q$) In SVD, $C=VD^2V^T$, where $Q=V$. The covariance matrix is $D$, and Principal Component is $XV=UDV^TV=UD$. Stackexchangestats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca Therefore, OLS and ridge can both be regarded as projecting on the Principal Component Direction $U$ because $D$ is just coefficient. Fig4 may help you to understand it (make regression on the second one). And if we back to the Fig3 or Fig1, we can regard $a$ as the principal component. The length of it is $d_i$ from the $AV=UD$, where $U$ is in a unit length. So it is not a surprise to find that $\\frac{d_i^2}{d_i^2+\\lambda}$ in $\\hat{y}=Ax_r$ is consistent with what we talked at dimension-one illstration. For high dimensions, we can illustrate like this, As you can see, the high dimensional case can be decomposed to be many Fig3, which is one dimension. From this aspect by the thinking of PCA, we should standardize input $A$ because ridge regression is sensitive to it. Variance DecreaseExcept for solving the problem of ill-conditioned $A$, the ridge regression also decreases the variance of estimator $x_r$. Intuitively, estimators are shrink, so the variance of estimators are smaller. Since that we have $x_r = (A^TA+\\lambda I)^{-1}A^Tb$, then the variance should be below, provided $A$ is full-rank, \\begin{aligned} x_r &= (A^TA+\\lambda I)^{-1}A^T(A\\beta+\\epsilon)\\\\ x_r-\\beta &= (A^TA+\\lambda I)^{-1}A^T\\epsilon\\\\ Var(x_r-\\beta) &= (A^TA+\\lambda I)^{-1}A^T\\epsilon \\epsilon^TA[(A^TA+\\lambda I)^{-1}]^T\\\\ Var(x_r)&=\\sigma^2(A^TA+\\lambda I)^{-1}A^TA (A^TA+\\lambda I)^{-1} \\end{aligned} And we know that, \\begin{aligned} x_{OLS}-\\beta&= (A^TA)^{-1}A^T\\epsilon\\\\ Var(x_{OLS}) &= (A^TA)^{-1}A^T\\epsilon \\epsilon^TA(A^TA)^{-1}\\\\ &= \\sigma^2 (A^TA)^{-1} A^TA(A^TA)^{-1} \\end{aligned}As we mentioned above, $(A^TA+\\lambda I)^{-1}$ can be regarded as a smaller multiplier than $(A^TA)^{-1}$, so $Var(x_{OLS})&gt;Var(x_r)$. If a detailed proof is wanted, click here. Let‚Äôs back to why we want to apply ridge. In the case that vectors are highly correlated (multicollinearity) or linearly dependent, the extra ‚Äòridge‚Äô $\\lambda I$ can differ them from each other. If not differed, then $x$ for two multicollinear vectors can vary a lot, which increases the variance much. And the MSE difference can be calculated as, MSE_{OLS}-MSE_{r} = Var(x_{OLS})-Var(x_{r}) - E^2(x_r-x_{OLS})It‚Äôs clear that if $E^2&lt;Var(x_{OLS})-Var(x_r)$, which depends on $\\lambda$, the ridge regression has the smaller MSE. We may also wonder the concrete effects on the variance. The covariance matrix is $A^TA/(n-1)$, as we mentioned above, we can rewrite it to be $VD^2V^T$. Thus, $D^2$ is the variance within the principal component direction ($V$-s). And for direction $i$, $Var=\\frac{d_i^2}{n-1}$. Combine the equation $\\sum_{i=1}u_i\\frac{d_i^2}{d_i^2+\\lambda}u_i^Tb$, we can know that if the smaller the variance $d_i^2/(n-1)$ is, the more shrink will be applied by ridge regression. Lasso RegressionLASSO is the shorthand of Least Absolute Shrinkage and Selection Operator inspired by Tibshirani. Lasso regression focuses on the problem two, where there are many useless features. Lasso solves this problem by adding a L1-norm in the OLS form. \\min ||y-x\\beta||^2 \\ \\ \\text{s.t } \\sum_{i=1}||\\beta_i||\\le C\\\\ \\Rightarrow \\min L = ||y-x\\beta||^2+\\lambda||\\beta||Biased EstimatorSubgradientHowever, if we take derivative on $\\beta_j$, we will face a problem that $||\\beta||$ may not have the gradient. Therefore, we introduce the concept, which is subgradient, to substitute gradient when it doesn‚Äôt exist. Let $f:R^n\\to R$ be a convex function with domain $R^n$. A vector $v$ is called subgradient at $x_0\\in R^n$ if, f(x)-f(x_0)\\ge v*(x-x_0)All subgradient at $x_0$ forms a set called subdifferential, denoted by $\\partial f(x_0)$. And $x_0$ is the minimum point of a convex function if and only if $0\\in\\partial f(x_0)$. Wikipediaen.wikipedia.org/wiki/Subgradient_method EstimatorNow, we can analyze the best point of $\\beta$. In order to get the explicit solution, we suppose $x$ is orthogonal. If we take differential on $\\beta_i$, there may be two scenarios, Gradient exists and $\\beta_i\\ne 0$. Gradient doesn‚Äôt exist so according to the subgradient, $\\beta_i=0\\in\\partial f$. For situation 1, \\begin{aligned} \\frac{\\partial L}{\\partial \\beta_i} = (-2x^Ty+2x^Tx\\beta_i)+\\lambda*\\text{sign}(\\beta_i)&=0\\\\ \\Rightarrow x^Ty-x^Tx\\beta_i&=-\\frac{\\lambda}{2}\\text{sign}(\\beta_i)\\\\ \\beta_i^{OLS}-\\beta_i&=-\\frac{\\lambda}{2}\\text{sign}(\\beta_i)\\\\ \\Rightarrow \\beta_i = \\begin{cases} \\beta^{OLS}_i-\\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i>\\frac{\\lambda}{2}\\\\ \\beta^{OLS}_i + \\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i\\frac{\\lambda}{2}\\\\ \\beta^{OLS}_i + \\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i 0 \\end{aligned}It gives the quadratic form and $(\\beta-\\hat{\\beta})$ is positive definite for $x^Tx$. The function should be drawn like below, If we let $z=(\\beta-\\hat{\\beta})^Tx^Tx(\\beta-\\hat{\\beta})$, where $z$ is any constant, we will get a ellipsoid graph on the $\\beta1.\\beta2$ plane. Either ridge or lasso has a constraint, we draw them on the same plane. From this viewpoint, it tells why Lasso generates sparsity solutions (many $\\beta$-s are 0). This is because L1-norm is not smooth and the $(0,\\beta_2)$ or $(\\beta_1,0)$ are easy to touch the ellipsoid. Someone plug Lasso into the Ridge regression and call it as Elastic Net. It can be written as, \\min L= ||y-x\\beta||^2+\\lambda_1||\\beta||^2+\\lambda_2||\\beta||,\\ \\ \\lambda_1+\\lambda_2=1The elastic net combines all the advantages of ridge and lasso so that you can choose it when you know nothing about the dataset. Of course, you can choose the potential type of regression by adjusting $\\lambda_1$ and $\\lambda_2$. The figure of its constraint is, Thus, it is able to get sparse solution and smooth solution by an implemented weighting mechanism. It is the improvement. Thanks For Reading! ReferenceList of Works MIT Course 18.06, Linear Algebra, Gilbert Strang Jiaming Mao, Data Analysis for Economics, Regularization Upenn, Ridge Regression Stackexchange, Relationship between PCA and SVD Robert Tibshirani, Regression Shrinkage and Selection via the Lasso Wikipedia, Subgradient Ryan Tibshirani, Convex Optimization - Coordinate Descent Pictures Cover: &nbsp;&nbsp; Pixiv by Katann Some Figures: &nbsp;&nbsp; Ridge Regression, Upenn &nbsp;&nbsp; Regularization, Jiaming Mao","link":"/2021/05/07/Report-Ridge-Lasso/"}],"tags":[{"name":"ÂæÆËßÇËÆ°Èáè","slug":"ÂæÆËßÇËÆ°Èáè","link":"/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"}],"categories":[]}