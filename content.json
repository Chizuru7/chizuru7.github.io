{"pages":[],"posts":[{"title":"Data Analysis for Economics Intro","text":"终于有一个相对轻松的学期了，就在这里记录一下茅家铭老师的微观计量课程。这是本学期观感最好的一门课程了，它在泰式英语，连环逼问的门门课程中太过鹤立鸡群，老师也是WISE一顶一的男神，教学资源也是相当好的。（教学网站：点击这里。）于是想在这里用通俗一些的话来过一下老师上课的一些内容，然后推导一些公式，也防止后面摸鱼摸得什么都忘了。这也是我的第一篇博文，是有很多不足之处的，写下来也很生疏，是我脑子里意识流的Intro part，详细还是参见茅神的课程吧。 正文1&lt;span style=&quot;text-align:center&quot;&gt;Machine learning $\\to$ Statistics $\\to$ Econometrics&lt;/span&gt; 从左到右是一个从结果推断(Prediction)向因果推断(Causality)加强的链。基于统计学知识，前者主要观测特征，后者想找出作用机理。而机器学习模型的准确度实际上是大大超过传统的一些计量方法的，比如Panel Data Regression和Logit Model。但是这里的缺陷在于很难表示出经济学的一些insights，类如随机森林等算法，很难有一个经济学标准的interpretation，而这些却是计量中很需要的。这个问题的出现主要是因为Relevance不能说明Causality，两者的终极目标也因此分开。 Statistics$x$ 和 $y$ 是两个变量，寻找这两个变量的关系应该是寻找他们的联合概率分布 $p(x,y)$。而实际上我们更多需要的是从 $x$ 得到 $y$ 的过程，即建立一个方程 $f(x)$ 来测定 $y$。 损失函数选择范数选择是由 $f(x)$ 引出的问题，是一个如何衡量”best“的问题。因为当得到预测方程时，我们总有$e = y-f(x)$, 误差 $e$ 越大是对预测模型越不好的。几种损失函数所得到的$f(x)$是完全不同的。 L1与MAEL1范数是$|x_1|+|x_2|+\\cdots+|x_n|$, 这里的损失函数是MAE，即：$\\frac{1}{n}\\sum |e_i|$，MAE是L1正则化的代表 (Lasso回归就是利用了L1的正则化)。最小化MAE产生的将是中位数。具体可以参见知乎-子元的回答。 MAE求解的下降速度统一。不同与MSE，会对误差小的项有大的包容性，MAE一视同仁。对于一些噪声异常值，MAE没有很大的反应，这是它相较于MSE优秀的地方。 L2与MSEL2 范数是 $\\sqrt{(x_1^2+\\cdots+x_n^2)}$ ,这里用于估算损失的函数为MSE，即：$\\frac{1}{n}\\sum_{i}^ne_i^2\\to E[(y-f(x))^2]$ 。L2范数和MSE的最小化过程实际上是一样的，MSE是L2正则化的一个代表 (Ridge回归就是用了L2的正则化)。如果选择这一种方法，得到 $f(x)$ 的过程就是最小化MSE的过程。这一个过程即是对目标函数$f(x)$求导的过程，最后的结果即是$f(x)$ 。要注意的是，这里 $y_i$ 是给定 $x$ 之后的数据点，因此$f(x)=f$,是一个定值。 \\partial \\sum (y_i-f)^2 = \\sum(y_i-f)=0\\\\ \\to f=\\frac{1}{n}\\sum{y_i}=E(y|x)用MSE的一大优点就是好算。相比于其他的方法，可能还会遇到稀疏等问题，而这个只需要求导就可以得到了，求导的具体过程是需要有对$f(x)$的预先估计的。例如，一元线性回归。 另一个优势是MSE对于异常值是很敏感的。因为次方的关系，一旦 $y-f(x)&gt;1$ ，那么MSE对与这类异常值的惩罚是要比MAE高很多的，这样的性质让MSE有防止过拟合的功效。因为这个代价函数会让$f(x)$尽可能贴近异常值，这会给异常值更大的权重。直觉上来讲，只有平均数可以让每个 $y-f(x)$ 都尽可能地小，从而让MSE最小。 下图里，左边是MAE，右边是MSE。 0-1 Loss这里需要建立一个Indicator Function, $\\hat{y}$ 是预测值。 I(y) = \\begin{cases} 0, y = \\hat{y}\\\\ 1, y\\neq \\hat{y} \\end{cases}我们将损失函数记为$\\sum_i I(y_i)$，自然，能达到最小损失函数的 $\\hat{y}$ 是我们想要的。能达到这一要求的是众数(mode)。这一直觉是，找到最有可能是正确的 $y$ 值。 Learning f对于$f(x)$的具体寻找方法，有两个方法。 Parametric Method它需要对数据的分布有基本的经验判断，这一判断是用于给定model/hypothesis的。这可能会导致有Wrong Functional Form这一个问题。对于复杂的数据将要面对approximation-generalization tradeoff。但这一好处是他们的经济学意义是有据可循的。在样本比较少，维度较多的情况下，参数估计是行之有效的。后面主要会讨论的是参数估计法。 Nonparametric Method这是需要很多数据支撑才能够使用的一种方法。在MSE下，根据$E(y|x)$，非参数估计都是以取平均值为基础来延伸的。 当某处的数据点不够充足的情况下，就采集这一位置附近的点，即Nearest Neighborhood method。 但是这一方法不一定非常实用。这一方法受到高纬度的“诅咒”：即，每多一个维度，neighborhood的空间会被压榨地更小，这样neighborhood里的点数量就会更少，这样取到的平均值（甚至可能取不出平均值）是不够可信的。 Goodness of Fit拟合优度是在预测中的一个重要属性。它和系数的显著性没有关系，是一个衡量预测出的模型和给定的数据点分布长得多像的一个指标。在计量中，我们有调整R方这一指标：$\\bar{R}^2 = 1- \\frac{n-1}{n-k-1}\\frac{SSR}{TSS}$. 即残差越小，拟合越好。 通常地，因为只有一个数据集，如果全部用于训练模型，那么其实很难有知道模型的普适性。所以一个数据集会被划分成训练集和测试集。训练集产生模型$g$，其中的误差为in-sample error。而测试集中，使用$g$来算出误差，为out-sample error。测试集中的误差（代表generalization）是我们重点要关注的。公式为$e_{TE} =\\frac{1}{M}\\sum (y_i-\\hat{f})^2 \\to E[(y-g)^2]$，实际上也是MSE。 当然，如果运用复杂的模型，那么in-sample error是要减小的，而模型的精度越大越可能导致out-sample error异常的大，从而失去普适性。这主要看$f(x)$的准确分布。这就是过拟合与欠拟合问题，也是即approximation-generalization tradeoff。在Hoeffding’s inequality里，机器学习的关键在于让$E_{in} = E_{out}$，且$E_{out}=0$。换而言之，这就要求$E_{in}=0$，$E_{out}\\to E_{in}$。前者是approximation，后者是generalization。 如果从out-sample error来看，我们可以把他分解成Bias和Variance： \\begin{aligned} E_{out}(g) &= E[(g(x)-f(x))^2]\\\\ &=Var(g-f)+E^2(g-f)\\\\ &=Var(g)+bias^2 \\end{aligned}注意这里的$g$是根据训练集得到的（先观测，后挑选出最适合的函数$g$），他同时condition on x和D(训练集)。 所以，如果从预测误差来看拟合好坏，这就是著名的Bias-Variance Tradeoff. Causality在Statistics里，我们更关注的是Relevance。然而在计量中，我们更要关注Causality。 Seeing vs. DoingSeeing 是直接观测历史数据，而doing是physically进行动作，得到结果。 我们用气压计作为例子。Seeing：气压低的时候比气压高时容易下雨。Doing：手动调整气压计，不论怎么调整气压，今天下雨的可能性都是一样的（与气压计无关）。我们可以了解到，Seeing可以观测Relevance，Doing可以看出是否拥有Causality（如果调低气压计使得容易下雨，那么有causality）。 另一个是AB医院的例子，两个医院的治愈率不同（Observation）。有可能重症都送往好的医院，但反而导致其治愈率下降了。现在一个人生病，送去好的医院治愈的可能性高（Causality）。 Just do it在Statistics learning中，我们知道得到$f(x)$的过程，$f(x)$是研究relevance的产物。如果我们构造一个数据集，都由do这一行动得到，这样，数据本身的relevance是由causality而来。即，这时候我们的$f(x)$就是causality。所以研究causality的方法，就是直接进行do operation，尽量构造一个RCT。 Causality DiagramCommon cause和common effect是两个典例。 对于common cause，我们的例子是，假设抽烟会导致带打火机(A)和癌症(Y)，且只考虑这三个因素。那么，我们若研究带打火机造成癌症的causality，就会因为common cause而产生一定的联系。因为我们让一个人$do(A=1)$，他得病还由是否吸烟导致，即$E(Y|A)\\neq E(Y|do(A=1))$。此时，我们只需要固定测试人为吸烟或不吸烟。就可以得到比较纯粹的causality。 \\begin{aligned} E[y|do(x)] &= \\sum E[y|do(x),z]p(z)\\\\ &=\\sum E[y|x,z]p(z) \\end{aligned}这里，我们的$E$ 就可以apply一些模型，就像我们在statistics里一样。","link":"/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/"},{"title":"VC Dimension","text":"VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a large VC dimension implies a powerful and complex model. However, it may not always be a good signal. If the VC dimension is far away large, then there will be problems in out-sample error, which may present as the overfitting problem. This report aims at discussing why the VC dimension is required and how it comes into being based on the process of machine learning. Notice that the thinking track of the report is based on the lecture Jiaming Mao presents. Figures are from the slides and the Internet. Let’s talk about the feasibility of machine learning first. Feasibility ConditionsProcess of Machine LearningMachine learning is predicting the value of $y$ based on the value of $x$. And the picture below shows the process of a learning. $f(x)$ is the underlying mechanism, so called target function. And $\\mathcal{H}$ is a set of candidate formula (also called hypothesis set or model), from which, through the algorithm $\\mathcal{A}$ (calculating the loss), we can choose a “best” hypothesis $g$ that approximately equals to $f(x)$. The error emerged within the training data is called in-sample error and the one generated in testing data is called out-sample error. In such a process, we’d like to get $E_{out}=0$ if possible. Nevertheless, we take a compromise and make it as $E_{out}\\approx0$ for there always are some disturbing terms. Now, a problem is that we can only observe $E_{in}$, which is the experience from the training data. Thus, we wonder whether $E_{out}\\approx E_{in}\\approx0$ such that $E_{in}$ can be a measurement. And it remains a problem here. Hoeffding’s InequalityHere we have an example, which is drawing marbles. The bin contains many many differently-colored marbles, and in this case we’d like to figure out the probability of red marbles in the bin, denoted by $\\mu$. It is impossible for us to count every marbles and what we can do is to draw samples. Then, we denote the probability of red marbles in the samples as $\\nu$. According to the law of large numbers, intuitively, the larger the sample is, the closer the distance between $\\mu$ and $\\nu$. One of the pattern is the Hoeffding’s inequality, (if the proof is wanted, click here.) Pr(|\\mu-\\nu|>\\epsilon)\\epsilon)\\epsilon)\\cup\\cdots] &\\le Pr[(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon)\\ \\text{or}\\ (|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon)\\cdots]\\\\ &\\le Pr(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon) + Pr(|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon) + \\cdots\\\\ &\\le \\sum_{i=1}^M 2\\exp(-2\\epsilon^2N) = 2M\\exp(-2\\epsilon^2N) \\end{aligned}, $M$ denotes the number of hypothesis in $\\mathcal{H}$. Clearly, if $M$ is large, then the model is complex. And let’s summarize the inequality as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2M\\exp(-2\\epsilon^2N)To sum up, there are two conditions if machine learning is feasible. The Algorithm $\\mathcal{A}$ can find a $g$ such that $E_{in}\\approx0$, and it is consistent with the process “train”. Actually, the more complex the model is, the smaller the $E_{in}$ is. $M$ is finite and a $N$ is sufficiently large to make $E_{out}\\approx E_{in}$, and it is consistent with the process “test”. Next, we’d like to focus on the second condition, and mainly talk about the $M$. VC DimensionGenerally, $\\mathcal{H}$ contains infinite $h$. For example, a simple linear model $y=ax+b$ generates infinite $h$, because the parameters are changeable. How do we shrink the upper bound by substitute $M$? Effective Number of HypothesisIn the process that expanding Hoeffding’s inequality, the inequality below is used, Pr(A_1\\ \\cup\\ A_2\\ \\cup\\ A_3\\ \\cdots) \\le \\sum_{i=1}^M Pr(A_i)If and only if $A_i$s are independent, we choose the equal sign. Actually, there are many intersection part in $\\mathcal{H}$. A simple example is shown in the graph. In the two-dimensional space, we can simply divide $\\mathcal{H}$ into two parts. One judges that $x_1$ is positive, like $h_1$. Another one marks $x_1$ as negative, like $h_2$. Here, the effective number of hypothesis is two. Under a specific sample, the effective number is fixed. Thus, the inequality can be written as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2*\\text{effective}(M)*\\exp(-2\\epsilon^2N)Growth FunctionDichotomyFirstly, we’d like to introduce a binary target function and hypothesis set that contains $h:\\mathcal{X}\\to\\{-1,+1\\}$. And, h(X_1,X_2,\\cdots,X_n) = (h(X_1), h(X_2), \\cdots, h(X_n))is called one dichotomy. Generally speaking, dichotomy represents a result which marks all points in the sample. $\\mathcal{H}(X_1,X_2,\\cdots,X_n)$ denotes the number of all dichotomies generated by hypothesis set $\\mathcal{H}$, ssgiven $n$ points. For instance, if three points are in the training set, One dichotomy is $(h_1(X_1), h_1(X_2), \\cdots) = (+1,+1,+1)$. Another dichotomy can be $(h_2(X_1), h_2(X_2), \\cdots) = (-1,+1,+1)$. … $2^3=8$ is the maximum dichotomies that $\\mathcal{H}(X_1,X_2,\\cdots)$ can value. Say, the number of dichotomies a $\\mathcal{H}$ can generate at most is $2^n$. Shatter Given n points, (and their locations are fixed,) if a hypothesis set $\\mathcal{H}$ can generate exactly $2^n$ dichotomies, we call $\\mathcal{H}$ shatters these points. An example is under two-dimensional dataset and the hypothesis set contains all linear model. There are three points, no matter what points are black, there always be a line that can separate black points from white points. (See the figure below.) As we mentioned above, three points have eight dichotomies. And black and white points are like the $+1,-1$ marks. If the model can tell the marks apart in every scenario, then we can say that the model, or $\\mathcal{H}$, shatters the points. However, if there are four points, then the linear model cannot shatter them. We call the smallest number of points as Break Point. Say, 4 is the break point for the two-dimensional linear regressor. Growth FunctionWe define the growth function as, m_\\mathcal{H}(n)=\\max_{X_1,X_2,\\cdots,X_n \\in \\mathcal{X}} |\\mathcal{H}(X_1,X_2,\\cdots,X_n)| That is, $m_\\mathcal{H}(n)$ is the maximum possible number of dichotomies $\\mathcal{H}$ can generate on a dataset of n points. Obviously, $m_\\mathcal{H}(n)\\le2^n$. The larger the $m_\\mathcal{H}(n)$ is, the more powerful the model is. And $m_\\mathcal{H}(n)$ can tell us whether the dataset of size $s$ can be shattered by $\\mathcal{H}$. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) Now, can we simply exchange $\\text{effective}(M)$ to $m_\\mathcal{H}(n)$ to shrink the upper bound? VC boundNo, because the upper bound for $m_\\mathcal{H}(n)$ is still an explosively increasing function. It is hard to tell whether the result can converge to zero. Sauer’s Lemma solves the problem. If the break point $k$ exists, then $m_\\mathcal{H}(n)$ is a polynomial. According to Sauer’s Lemma, m_\\mathcal{H}(n) \\le \\sum_{i=0}^{k-1} \\begin{pmatrix} N\\\\ i \\end{pmatrix}\\le\\begin{cases} N^{k-1}+1,\\\\ (\\frac{eN}{k-1})^{k-1}, \\text{if } N\\ge k-1 \\end{cases} And the polynomial is much easier to converge and it releases a signal that it is feasible to make the upper bound to be zero! Through some complicated transformations, the inequality can be written as, (the proof is here) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*m_\\mathcal{H}(2N)*\\exp(-\\frac{1}{8}\\epsilon^2N) This inequality is the VC bound and the exponential part has a higher rate of convergence than the polynomial part. Thus, the inequality tells that if there is a break point, when $N$ is sufficiently large, for any hypothesis $h$, $E_{out}\\to E_{in}$ is very possible. Then the machine learning in the space is feasible, which meets the second condition. If the significance level is $\\delta$, and the upper bound should be $\\delta$. Then, \\Rightarrow Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le\\delta\\\\ \\epsilon = \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\\\ E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}，\\text{probability 1} -\\deltaThe bound is called VC generalization bound, which is usually applied in training model because we are required to select a best hypothesis. But, we will not apply it onto the testing. Instead, we use the original Hoeffding’s Inequality for there is only one hypothesis and the VC bound is much looser. Why is VC bound so loose? The basic Hoeffding’s inequality used in the proof already has a slack. Using $m_\\mathcal{H}(N)$ to quantify the number of dichotomies on N points, regardless of which $N$ points are in the dataset, gives us a worst-case estimate (because we expanded the inequality to all $h$). Bound $m_\\mathcal{H}(N)$ by a simple polynomial. VC DimensionNow, we can talk about the definition of VC dimension. The Vapnik-Chervonenkis (VC) dimension of $\\mathcal{H}$, denoted $d_{vc}(\\mathcal{H})$ is the size of the largest data set that $\\mathcal{H}$ can shatter. Remember that we have already talked about this problem in part Growth Function. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) And here, $d_{vc}(\\mathcal{H})=3$. If $m_\\mathcal{H}(n)=\\infty$, then $d_{vc}(\\mathcal{H})=\\infty$. In general, $d_{vc}(\\mathcal{H})=k-1$, where $k$ is the break point. If the VC dimension is large, the model $\\mathcal{H}$ is complex because a large sample can be shattered by it. Thus, VC dimension tells the ability of learning for the model. The definition of VC dimension has nothing to do with the locations of points in dataset, which leads to a problem. For example, again, under two-dimensional plane, there are three points. However, this time, they are on the same line (See the graph). A linear model cannot shatter the dataset now. Therefore, VC dimension tells you the largest dataset $\\mathcal{H}$ can shatter, but not every same-sized dataset can be shattered. The VC bound can also be written as, (the first inequality in Sauer’s Lemma is usually used.) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*((2N)^{d_{vc}(\\mathcal{H})}+1)*\\exp(-\\frac{1}{8}\\epsilon^2N)Approximation-Generalization TradeoffRecall the two conditions if machine learning is feasible, and now, the two conditions should be, The algorithm $\\mathcal{A}$ can find the $g$ that makes $E_{in}\\approx0$. Actually, the larger the VC dimension is (the more complex the model is), the smaller the $E_{in}$ is. The VC dimension are finite to make the polynomial part easy to converge and $N$ should be sufficiently large so that $E_{out}\\approx E_{in}$. Also, recall the relationship that we talked in VC bound, E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}，\\text{with probability 1} -\\deltaLet $R = \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}} $. If $N$ holds constant, then if $d_{vc}(\\mathcal{H})$ increases, then $R$ will increases, which means the bound will become loose. As a result, $E_{out}$ may get away from $E_{in}$. Nevertheless, $\\mathcal{H}$ will be more complicated. Say, $E_{in}$ decreases, even reaches the scenario that $E_{in}=0$. We call it approximation-generalization tradeoff. When one condition gets too tight, another one will be hard to meet. Therefore, we’d better find a point where we can balance these two well. And based on the inequality, it is how the VC dimension functions to make machine learning feasible. SummaryStarting from the two feasibility conditions, this report mainly discusses how the VC dimension comes into being and how should we understand VC dimension. VC dimension is important because it regulates the two conditions for machine to learn, which is the approximation-generalization tradeoff. Actually, mathematics is just a tool for us to shrink the bound and VC dimension is the aggregation of them, which can help us study further in machine learning. References Jiaming Mao, Data Analysis for Economics Liubai01’s blog, 机器学习推导合集01 ECE 901 Lecture 19 Yaser &amp; Malik &amp; Hsuan-tien Lin, Learn from Data [M], California Institute of Technology Hsuan-tien Lin, Machine Learning Foundation","link":"/2021/03/29/VC%20Dimension/"}],"tags":[{"name":"微观计量","slug":"微观计量","link":"/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"}],"categories":[]}