{"pages":[],"posts":[{"title":"Data Analysis for Economics Intro","text":"Áªà‰∫éÊúâ‰∏Ä‰∏™Áõ∏ÂØπËΩªÊùæÁöÑÂ≠¶Êúü‰∫ÜÔºåÂ∞±Âú®ËøôÈáåËÆ∞ÂΩï‰∏Ä‰∏ãËåÖÂÆ∂Èì≠ËÄÅÂ∏àÁöÑÂæÆËßÇËÆ°ÈáèËØæÁ®ã„ÄÇËøôÊòØÊú¨Â≠¶ÊúüËßÇÊÑüÊúÄÂ•ΩÁöÑ‰∏ÄÈó®ËØæÁ®ã‰∫ÜÔºåÂÆÉÂú®Ê≥∞ÂºèËã±ËØ≠ÔºåËøûÁéØÈÄºÈóÆÁöÑÈó®Èó®ËØæÁ®ã‰∏≠Â§™ËøáÈπ§Á´ãÈ∏°Áæ§ÔºåËÄÅÂ∏à‰πüÊòØWISE‰∏ÄÈ°∂‰∏ÄÁöÑÁî∑Á•ûÔºåÊïôÂ≠¶ËµÑÊ∫ê‰πüÊòØÁõ∏ÂΩìÂ•ΩÁöÑ„ÄÇÔºàÊïôÂ≠¶ÁΩëÁ´ôÔºöÁÇπÂáªËøôÈáå„ÄÇÔºâ‰∫éÊòØÊÉ≥Âú®ËøôÈáåÁî®ÈÄö‰øó‰∏Ä‰∫õÁöÑËØùÊù•Ëøá‰∏Ä‰∏ãËÄÅÂ∏à‰∏äËØæÁöÑ‰∏Ä‰∫õÂÜÖÂÆπÔºåÁÑ∂ÂêéÊé®ÂØº‰∏Ä‰∫õÂÖ¨ÂºèÔºå‰πüÈò≤Ê≠¢ÂêéÈù¢Êë∏È±ºÊë∏Âæó‰ªÄ‰πàÈÉΩÂøò‰∫Ü„ÄÇËøô‰πüÊòØÊàëÁöÑÁ¨¨‰∏ÄÁØáÂçöÊñáÔºåÊòØÊúâÂæàÂ§ö‰∏çË∂≥‰πãÂ§ÑÁöÑÔºåÂÜô‰∏ãÊù•‰πüÂæàÁîüÁñèÔºåÊòØÊàëËÑëÂ≠êÈáåÊÑèËØÜÊµÅÁöÑIntro partÔºåËØ¶ÁªÜËøòÊòØÂèÇËßÅËåÖÁ•ûÁöÑËØæÁ®ãÂêß„ÄÇ Ê≠£Êñá1&lt;span style=&quot;text-align:center&quot;&gt;Machine learning $\\to$ Statistics $\\to$ Econometrics&lt;/span&gt; ‰ªéÂ∑¶Âà∞Âè≥ÊòØ‰∏Ä‰∏™‰ªéÁªìÊûúÊé®Êñ≠(Prediction)ÂêëÂõ†ÊûúÊé®Êñ≠(Causality)Âä†Âº∫ÁöÑÈìæ„ÄÇÂü∫‰∫éÁªüËÆ°Â≠¶Áü•ËØÜÔºåÂâçËÄÖ‰∏ªË¶ÅËßÇÊµãÁâπÂæÅÔºåÂêéËÄÖÊÉ≥ÊâæÂá∫‰ΩúÁî®Êú∫ÁêÜ„ÄÇËÄåÊú∫Âô®Â≠¶‰π†Ê®°ÂûãÁöÑÂáÜÁ°ÆÂ∫¶ÂÆûÈôÖ‰∏äÊòØÂ§ßÂ§ßË∂ÖËøá‰º†ÁªüÁöÑ‰∏Ä‰∫õËÆ°ÈáèÊñπÊ≥ïÁöÑÔºåÊØîÂ¶ÇPanel Data RegressionÂíåLogit Model„ÄÇ‰ΩÜÊòØËøôÈáåÁöÑÁº∫Èô∑Âú®‰∫éÂæàÈöæË°®Á§∫Âá∫ÁªèÊµéÂ≠¶ÁöÑ‰∏Ä‰∫õinsightsÔºåÁ±ªÂ¶ÇÈöèÊú∫Ê£ÆÊûóÁ≠âÁÆóÊ≥ïÔºåÂæàÈöæÊúâ‰∏Ä‰∏™ÁªèÊµéÂ≠¶Ê†áÂáÜÁöÑinterpretationÔºåËÄåËøô‰∫õÂç¥ÊòØËÆ°Èáè‰∏≠ÂæàÈúÄË¶ÅÁöÑ„ÄÇËøô‰∏™ÈóÆÈ¢òÁöÑÂá∫Áé∞‰∏ªË¶ÅÊòØÂõ†‰∏∫Relevance‰∏çËÉΩËØ¥ÊòéCausalityÔºå‰∏§ËÄÖÁöÑÁªàÊûÅÁõÆÊ†á‰πüÂõ†Ê≠§ÂàÜÂºÄ„ÄÇ Statistics$x$ Âíå $y$ ÊòØ‰∏§‰∏™ÂèòÈáèÔºåÂØªÊâæËøô‰∏§‰∏™ÂèòÈáèÁöÑÂÖ≥Á≥ªÂ∫îËØ•ÊòØÂØªÊâæ‰ªñ‰ª¨ÁöÑËÅîÂêàÊ¶ÇÁéáÂàÜÂ∏É $p(x,y)$„ÄÇËÄåÂÆûÈôÖ‰∏äÊàë‰ª¨Êõ¥Â§öÈúÄË¶ÅÁöÑÊòØ‰ªé $x$ ÂæóÂà∞ $y$ ÁöÑËøáÁ®ãÔºåÂç≥Âª∫Á´ã‰∏Ä‰∏™ÊñπÁ®ã $f(x)$ Êù•ÊµãÂÆö $y$„ÄÇ ÊçüÂ§±ÂáΩÊï∞ÈÄâÊã©ËåÉÊï∞ÈÄâÊã©ÊòØÁî± $f(x)$ ÂºïÂá∫ÁöÑÈóÆÈ¢òÔºåÊòØ‰∏Ä‰∏™Â¶Ç‰ΩïË°°Èáè‚Äùbest‚ÄúÁöÑÈóÆÈ¢ò„ÄÇÂõ†‰∏∫ÂΩìÂæóÂà∞È¢ÑÊµãÊñπÁ®ãÊó∂ÔºåÊàë‰ª¨ÊÄªÊúâ$e = y-f(x)$, ËØØÂ∑Æ $e$ Ë∂äÂ§ßÊòØÂØπÈ¢ÑÊµãÊ®°ÂûãË∂ä‰∏çÂ•ΩÁöÑ„ÄÇÂá†ÁßçÊçüÂ§±ÂáΩÊï∞ÊâÄÂæóÂà∞ÁöÑ$f(x)$ÊòØÂÆåÂÖ®‰∏çÂêåÁöÑ„ÄÇ L1‰∏éMAEL1ËåÉÊï∞ÊòØ$|x_1|+|x_2|+\\cdots+|x_n|$, ËøôÈáåÁöÑÊçüÂ§±ÂáΩÊï∞ÊòØMAEÔºåÂç≥Ôºö$\\frac{1}{n}\\sum |e_i|$ÔºåMAEÊòØL1Ê≠£ÂàôÂåñÁöÑ‰ª£Ë°® (LassoÂõûÂΩíÂ∞±ÊòØÂà©Áî®‰∫ÜL1ÁöÑÊ≠£ÂàôÂåñ)„ÄÇÊúÄÂ∞èÂåñMAE‰∫ßÁîüÁöÑÂ∞ÜÊòØ‰∏≠‰ΩçÊï∞„ÄÇÂÖ∑‰ΩìÂèØ‰ª•ÂèÇËßÅÁü•‰πé-Â≠êÂÖÉÁöÑÂõûÁ≠î„ÄÇ MAEÊ±ÇËß£ÁöÑ‰∏ãÈôçÈÄüÂ∫¶Áªü‰∏Ä„ÄÇ‰∏çÂêå‰∏éMSEÔºå‰ºöÂØπËØØÂ∑ÆÂ∞èÁöÑÈ°πÊúâÂ§ßÁöÑÂåÖÂÆπÊÄßÔºåMAE‰∏ÄËßÜÂêå‰ªÅ„ÄÇÂØπ‰∫é‰∏Ä‰∫õÂô™Â£∞ÂºÇÂ∏∏ÂÄºÔºåMAEÊ≤°ÊúâÂæàÂ§ßÁöÑÂèçÂ∫îÔºåËøôÊòØÂÆÉÁõ∏ËæÉ‰∫éMSE‰ºòÁßÄÁöÑÂú∞Êñπ„ÄÇ L2‰∏éMSEL2 ËåÉÊï∞ÊòØ $\\sqrt{(x_1^2+\\cdots+x_n^2)}$ ,ËøôÈáåÁî®‰∫é‰º∞ÁÆóÊçüÂ§±ÁöÑÂáΩÊï∞‰∏∫MSEÔºåÂç≥Ôºö$\\frac{1}{n}\\sum_{i}^ne_i^2\\to E[(y-f(x))^2]$ „ÄÇL2ËåÉÊï∞ÂíåMSEÁöÑÊúÄÂ∞èÂåñËøáÁ®ãÂÆûÈôÖ‰∏äÊòØ‰∏ÄÊ†∑ÁöÑÔºåMSEÊòØL2Ê≠£ÂàôÂåñÁöÑ‰∏Ä‰∏™‰ª£Ë°® (RidgeÂõûÂΩíÂ∞±ÊòØÁî®‰∫ÜL2ÁöÑÊ≠£ÂàôÂåñ)„ÄÇÂ¶ÇÊûúÈÄâÊã©Ëøô‰∏ÄÁßçÊñπÊ≥ïÔºåÂæóÂà∞ $f(x)$ ÁöÑËøáÁ®ãÂ∞±ÊòØÊúÄÂ∞èÂåñMSEÁöÑËøáÁ®ã„ÄÇËøô‰∏Ä‰∏™ËøáÁ®ãÂç≥ÊòØÂØπÁõÆÊ†áÂáΩÊï∞$f(x)$Ê±ÇÂØºÁöÑËøáÁ®ãÔºåÊúÄÂêéÁöÑÁªìÊûúÂç≥ÊòØ$f(x)$ „ÄÇË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåËøôÈáå $y_i$ ÊòØÁªôÂÆö $x$ ‰πãÂêéÁöÑÊï∞ÊçÆÁÇπÔºåÂõ†Ê≠§$f(x)=f$,ÊòØ‰∏Ä‰∏™ÂÆöÂÄº„ÄÇ \\partial \\sum (y_i-f)^2 = \\sum(y_i-f)=0\\\\ \\to f=\\frac{1}{n}\\sum{y_i}=E(y|x)Áî®MSEÁöÑ‰∏ÄÂ§ß‰ºòÁÇπÂ∞±ÊòØÂ•ΩÁÆó„ÄÇÁõ∏ÊØî‰∫éÂÖ∂‰ªñÁöÑÊñπÊ≥ïÔºåÂèØËÉΩËøò‰ºöÈÅáÂà∞Á®ÄÁñèÁ≠âÈóÆÈ¢òÔºåËÄåËøô‰∏™Âè™ÈúÄË¶ÅÊ±ÇÂØºÂ∞±ÂèØ‰ª•ÂæóÂà∞‰∫ÜÔºåÊ±ÇÂØºÁöÑÂÖ∑‰ΩìËøáÁ®ãÊòØÈúÄË¶ÅÊúâÂØπ$f(x)$ÁöÑÈ¢ÑÂÖà‰º∞ËÆ°ÁöÑ„ÄÇ‰æãÂ¶ÇÔºå‰∏ÄÂÖÉÁ∫øÊÄßÂõûÂΩí„ÄÇ Âè¶‰∏Ä‰∏™‰ºòÂäøÊòØMSEÂØπ‰∫éÂºÇÂ∏∏ÂÄºÊòØÂæàÊïèÊÑüÁöÑ„ÄÇÂõ†‰∏∫Ê¨°ÊñπÁöÑÂÖ≥Á≥ªÔºå‰∏ÄÊó¶ $y-f(x)&gt;1$ ÔºåÈÇ£‰πàMSEÂØπ‰∏éËøôÁ±ªÂºÇÂ∏∏ÂÄºÁöÑÊÉ©ÁΩöÊòØË¶ÅÊØîMAEÈ´òÂæàÂ§öÁöÑÔºåËøôÊ†∑ÁöÑÊÄßË¥®ËÆ©MSEÊúâÈò≤Ê≠¢ËøáÊãüÂêàÁöÑÂäüÊïà„ÄÇÂõ†‰∏∫Ëøô‰∏™‰ª£‰ª∑ÂáΩÊï∞‰ºöËÆ©$f(x)$Â∞ΩÂèØËÉΩË¥¥ËøëÂºÇÂ∏∏ÂÄºÔºåËøô‰ºöÁªôÂºÇÂ∏∏ÂÄºÊõ¥Â§ßÁöÑÊùÉÈáç„ÄÇÁõ¥Ëßâ‰∏äÊù•ËÆ≤ÔºåÂè™ÊúâÂπ≥ÂùáÊï∞ÂèØ‰ª•ËÆ©ÊØè‰∏™ $y-f(x)$ ÈÉΩÂ∞ΩÂèØËÉΩÂú∞Â∞èÔºå‰ªéËÄåËÆ©MSEÊúÄÂ∞è„ÄÇ ‰∏ãÂõæÈáåÔºåÂ∑¶ËæπÊòØMAEÔºåÂè≥ËæπÊòØMSE„ÄÇ 0-1 LossËøôÈáåÈúÄË¶ÅÂª∫Á´ã‰∏Ä‰∏™Indicator Function, $\\hat{y}$ ÊòØÈ¢ÑÊµãÂÄº„ÄÇ I(y) = \\begin{cases} 0, y = \\hat{y}\\\\ 1, y\\neq \\hat{y} \\end{cases}Êàë‰ª¨Â∞ÜÊçüÂ§±ÂáΩÊï∞ËÆ∞‰∏∫$\\sum_i I(y_i)$ÔºåËá™ÁÑ∂ÔºåËÉΩËææÂà∞ÊúÄÂ∞èÊçüÂ§±ÂáΩÊï∞ÁöÑ $\\hat{y}$ ÊòØÊàë‰ª¨ÊÉ≥Ë¶ÅÁöÑ„ÄÇËÉΩËææÂà∞Ëøô‰∏ÄË¶ÅÊ±ÇÁöÑÊòØ‰ºóÊï∞(mode)„ÄÇËøô‰∏ÄÁõ¥ËßâÊòØÔºåÊâæÂà∞ÊúÄÊúâÂèØËÉΩÊòØÊ≠£Á°ÆÁöÑ $y$ ÂÄº„ÄÇ Learning fÂØπ‰∫é$f(x)$ÁöÑÂÖ∑‰ΩìÂØªÊâæÊñπÊ≥ïÔºåÊúâ‰∏§‰∏™ÊñπÊ≥ï„ÄÇ Parametric MethodÂÆÉÈúÄË¶ÅÂØπÊï∞ÊçÆÁöÑÂàÜÂ∏ÉÊúâÂü∫Êú¨ÁöÑÁªèÈ™åÂà§Êñ≠ÔºåËøô‰∏ÄÂà§Êñ≠ÊòØÁî®‰∫éÁªôÂÆömodel/hypothesisÁöÑ„ÄÇËøôÂèØËÉΩ‰ºöÂØºËá¥ÊúâWrong Functional FormËøô‰∏Ä‰∏™ÈóÆÈ¢ò„ÄÇÂØπ‰∫éÂ§çÊùÇÁöÑÊï∞ÊçÆÂ∞ÜË¶ÅÈù¢ÂØπapproximation-generalization tradeoff„ÄÇ‰ΩÜËøô‰∏ÄÂ•ΩÂ§ÑÊòØ‰ªñ‰ª¨ÁöÑÁªèÊµéÂ≠¶ÊÑè‰πâÊòØÊúâÊçÆÂèØÂæ™ÁöÑ„ÄÇÂú®Ê†∑Êú¨ÊØîËæÉÂ∞ëÔºåÁª¥Â∫¶ËæÉÂ§öÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèÇÊï∞‰º∞ËÆ°ÊòØË°å‰πãÊúâÊïàÁöÑ„ÄÇÂêéÈù¢‰∏ªË¶Å‰ºöËÆ®ËÆ∫ÁöÑÊòØÂèÇÊï∞‰º∞ËÆ°Ê≥ï„ÄÇ Nonparametric MethodËøôÊòØÈúÄË¶ÅÂæàÂ§öÊï∞ÊçÆÊîØÊíëÊâçËÉΩÂ§ü‰ΩøÁî®ÁöÑ‰∏ÄÁßçÊñπÊ≥ï„ÄÇÂú®MSE‰∏ãÔºåÊ†πÊçÆ$E(y|x)$ÔºåÈùûÂèÇÊï∞‰º∞ËÆ°ÈÉΩÊòØ‰ª•ÂèñÂπ≥ÂùáÂÄº‰∏∫Âü∫Á°ÄÊù•Âª∂‰º∏ÁöÑ„ÄÇ ÂΩìÊüêÂ§ÑÁöÑÊï∞ÊçÆÁÇπ‰∏çÂ§üÂÖÖË∂≥ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂ∞±ÈááÈõÜËøô‰∏Ä‰ΩçÁΩÆÈôÑËøëÁöÑÁÇπÔºåÂç≥Nearest Neighborhood method„ÄÇ ‰ΩÜÊòØËøô‰∏ÄÊñπÊ≥ï‰∏ç‰∏ÄÂÆöÈùûÂ∏∏ÂÆûÁî®„ÄÇËøô‰∏ÄÊñπÊ≥ïÂèóÂà∞È´òÁ∫¨Â∫¶ÁöÑ‚ÄúËØÖÂíí‚ÄùÔºöÂç≥ÔºåÊØèÂ§ö‰∏Ä‰∏™Áª¥Â∫¶ÔºåneighborhoodÁöÑÁ©∫Èó¥‰ºöË¢´ÂéãÊ¶®Âú∞Êõ¥Â∞èÔºåËøôÊ†∑neighborhoodÈáåÁöÑÁÇπÊï∞ÈáèÂ∞±‰ºöÊõ¥Â∞ëÔºåËøôÊ†∑ÂèñÂà∞ÁöÑÂπ≥ÂùáÂÄºÔºàÁîöËá≥ÂèØËÉΩÂèñ‰∏çÂá∫Âπ≥ÂùáÂÄºÔºâÊòØ‰∏çÂ§üÂèØ‰ø°ÁöÑ„ÄÇ Goodness of FitÊãüÂêà‰ºòÂ∫¶ÊòØÂú®È¢ÑÊµã‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÂ±ûÊÄß„ÄÇÂÆÉÂíåÁ≥ªÊï∞ÁöÑÊòæËëóÊÄßÊ≤°ÊúâÂÖ≥Á≥ªÔºåÊòØ‰∏Ä‰∏™Ë°°ÈáèÈ¢ÑÊµãÂá∫ÁöÑÊ®°ÂûãÂíåÁªôÂÆöÁöÑÊï∞ÊçÆÁÇπÂàÜÂ∏ÉÈïøÂæóÂ§öÂÉèÁöÑ‰∏Ä‰∏™ÊåáÊ†á„ÄÇÂú®ËÆ°Èáè‰∏≠ÔºåÊàë‰ª¨ÊúâË∞ÉÊï¥RÊñπËøô‰∏ÄÊåáÊ†áÔºö$\\bar{R}^2 = 1- \\frac{n-1}{n-k-1}\\frac{SSR}{TSS}$. Âç≥ÊÆãÂ∑ÆË∂äÂ∞èÔºåÊãüÂêàË∂äÂ•Ω„ÄÇ ÈÄöÂ∏∏Âú∞ÔºåÂõ†‰∏∫Âè™Êúâ‰∏Ä‰∏™Êï∞ÊçÆÈõÜÔºåÂ¶ÇÊûúÂÖ®ÈÉ®Áî®‰∫éËÆ≠ÁªÉÊ®°ÂûãÔºåÈÇ£‰πàÂÖ∂ÂÆûÂæàÈöæÊúâÁü•ÈÅìÊ®°ÂûãÁöÑÊôÆÈÄÇÊÄß„ÄÇÊâÄ‰ª•‰∏Ä‰∏™Êï∞ÊçÆÈõÜ‰ºöË¢´ÂàíÂàÜÊàêËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ„ÄÇËÆ≠ÁªÉÈõÜ‰∫ßÁîüÊ®°Âûã$g$ÔºåÂÖ∂‰∏≠ÁöÑËØØÂ∑Æ‰∏∫in-sample error„ÄÇËÄåÊµãËØïÈõÜ‰∏≠Ôºå‰ΩøÁî®$g$Êù•ÁÆóÂá∫ËØØÂ∑ÆÔºå‰∏∫out-sample error„ÄÇÊµãËØïÈõÜ‰∏≠ÁöÑËØØÂ∑ÆÔºà‰ª£Ë°®generalizationÔºâÊòØÊàë‰ª¨ÈáçÁÇπË¶ÅÂÖ≥Ê≥®ÁöÑ„ÄÇÂÖ¨Âºè‰∏∫$e_{TE} =\\frac{1}{M}\\sum (y_i-\\hat{f})^2 \\to E[(y-g)^2]$ÔºåÂÆûÈôÖ‰∏ä‰πüÊòØMSE„ÄÇ ÂΩìÁÑ∂ÔºåÂ¶ÇÊûúËøêÁî®Â§çÊùÇÁöÑÊ®°ÂûãÔºåÈÇ£‰πàin-sample errorÊòØË¶ÅÂáèÂ∞èÁöÑÔºåËÄåÊ®°ÂûãÁöÑÁ≤æÂ∫¶Ë∂äÂ§ßË∂äÂèØËÉΩÂØºËá¥out-sample errorÂºÇÂ∏∏ÁöÑÂ§ßÔºå‰ªéËÄåÂ§±ÂéªÊôÆÈÄÇÊÄß„ÄÇËøô‰∏ªË¶ÅÁúã$f(x)$ÁöÑÂáÜÁ°ÆÂàÜÂ∏É„ÄÇËøôÂ∞±ÊòØËøáÊãüÂêà‰∏éÊ¨†ÊãüÂêàÈóÆÈ¢òÔºå‰πüÊòØÂç≥approximation-generalization tradeoff„ÄÇÂú®Hoeffding‚Äôs inequalityÈáåÔºåÊú∫Âô®Â≠¶‰π†ÁöÑÂÖ≥ÈîÆÂú®‰∫éËÆ©$E_{in} = E_{out}$Ôºå‰∏î$E_{out}=0$„ÄÇÊç¢ËÄåË®Ä‰πãÔºåËøôÂ∞±Ë¶ÅÊ±Ç$E_{in}=0$Ôºå$E_{out}\\to E_{in}$„ÄÇÂâçËÄÖÊòØapproximationÔºåÂêéËÄÖÊòØgeneralization„ÄÇ Â¶ÇÊûú‰ªéout-sample errorÊù•ÁúãÔºåÊàë‰ª¨ÂèØ‰ª•Êää‰ªñÂàÜËß£ÊàêBiasÂíåVarianceÔºö \\begin{aligned} E_{out}(g) &= E[(g(x)-f(x))^2]\\\\ &=Var(g-f)+E^2(g-f)\\\\ &=Var(g)+bias^2 \\end{aligned}Ê≥®ÊÑèËøôÈáåÁöÑ$g$ÊòØÊ†πÊçÆËÆ≠ÁªÉÈõÜÂæóÂà∞ÁöÑÔºàÂÖàËßÇÊµãÔºåÂêéÊåëÈÄâÂá∫ÊúÄÈÄÇÂêàÁöÑÂáΩÊï∞$g$ÔºâÔºå‰ªñÂêåÊó∂condition on xÂíåD(ËÆ≠ÁªÉÈõÜ)„ÄÇ ÊâÄ‰ª•ÔºåÂ¶ÇÊûú‰ªéÈ¢ÑÊµãËØØÂ∑ÆÊù•ÁúãÊãüÂêàÂ•ΩÂùèÔºåËøôÂ∞±ÊòØËëóÂêçÁöÑBias-Variance Tradeoff. CausalityÂú®StatisticsÈáåÔºåÊàë‰ª¨Êõ¥ÂÖ≥Ê≥®ÁöÑÊòØRelevance„ÄÇÁÑ∂ËÄåÂú®ËÆ°Èáè‰∏≠ÔºåÊàë‰ª¨Êõ¥Ë¶ÅÂÖ≥Ê≥®Causality„ÄÇ Seeing vs. DoingSeeing ÊòØÁõ¥Êé•ËßÇÊµãÂéÜÂè≤Êï∞ÊçÆÔºåËÄådoingÊòØphysicallyËøõË°åÂä®‰ΩúÔºåÂæóÂà∞ÁªìÊûú„ÄÇ Êàë‰ª¨Áî®Ê∞îÂéãËÆ°‰Ωú‰∏∫‰æãÂ≠ê„ÄÇSeeingÔºöÊ∞îÂéã‰ΩéÁöÑÊó∂ÂÄôÊØîÊ∞îÂéãÈ´òÊó∂ÂÆπÊòì‰∏ãÈõ®„ÄÇDoingÔºöÊâãÂä®Ë∞ÉÊï¥Ê∞îÂéãËÆ°Ôºå‰∏çËÆ∫ÊÄé‰πàË∞ÉÊï¥Ê∞îÂéãÔºå‰ªäÂ§©‰∏ãÈõ®ÁöÑÂèØËÉΩÊÄßÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑÔºà‰∏éÊ∞îÂéãËÆ°Êó†ÂÖ≥Ôºâ„ÄÇÊàë‰ª¨ÂèØ‰ª•‰∫ÜËß£Âà∞ÔºåSeeingÂèØ‰ª•ËßÇÊµãRelevanceÔºåDoingÂèØ‰ª•ÁúãÂá∫ÊòØÂê¶Êã•ÊúâCausalityÔºàÂ¶ÇÊûúË∞É‰ΩéÊ∞îÂéãËÆ°‰ΩøÂæóÂÆπÊòì‰∏ãÈõ®ÔºåÈÇ£‰πàÊúâcausalityÔºâ„ÄÇ Âè¶‰∏Ä‰∏™ÊòØABÂåªÈô¢ÁöÑ‰æãÂ≠êÔºå‰∏§‰∏™ÂåªÈô¢ÁöÑÊ≤ªÊÑàÁéá‰∏çÂêåÔºàObservationÔºâ„ÄÇÊúâÂèØËÉΩÈáçÁóáÈÉΩÈÄÅÂæÄÂ•ΩÁöÑÂåªÈô¢Ôºå‰ΩÜÂèçËÄåÂØºËá¥ÂÖ∂Ê≤ªÊÑàÁéá‰∏ãÈôç‰∫Ü„ÄÇÁé∞Âú®‰∏Ä‰∏™‰∫∫ÁîüÁóÖÔºåÈÄÅÂéªÂ•ΩÁöÑÂåªÈô¢Ê≤ªÊÑàÁöÑÂèØËÉΩÊÄßÈ´òÔºàCausalityÔºâ„ÄÇ Just do itÂú®Statistics learning‰∏≠ÔºåÊàë‰ª¨Áü•ÈÅìÂæóÂà∞$f(x)$ÁöÑËøáÁ®ãÔºå$f(x)$ÊòØÁ†îÁ©∂relevanceÁöÑ‰∫ßÁâ©„ÄÇÂ¶ÇÊûúÊàë‰ª¨ÊûÑÈÄ†‰∏Ä‰∏™Êï∞ÊçÆÈõÜÔºåÈÉΩÁî±doËøô‰∏ÄË°åÂä®ÂæóÂà∞ÔºåËøôÊ†∑ÔºåÊï∞ÊçÆÊú¨Ë∫´ÁöÑrelevanceÊòØÁî±causalityËÄåÊù•„ÄÇÂç≥ÔºåËøôÊó∂ÂÄôÊàë‰ª¨ÁöÑ$f(x)$Â∞±ÊòØcausality„ÄÇÊâÄ‰ª•Á†îÁ©∂causalityÁöÑÊñπÊ≥ïÔºåÂ∞±ÊòØÁõ¥Êé•ËøõË°ådo operationÔºåÂ∞ΩÈáèÊûÑÈÄ†‰∏Ä‰∏™RCT„ÄÇ Causality DiagramCommon causeÂíåcommon effectÊòØ‰∏§‰∏™ÂÖ∏‰æã„ÄÇ ÂØπ‰∫écommon causeÔºåÊàë‰ª¨ÁöÑ‰æãÂ≠êÊòØÔºåÂÅáËÆæÊäΩÁÉü‰ºöÂØºËá¥Â∏¶ÊâìÁÅ´Êú∫(A)ÂíåÁôåÁóá(Y)Ôºå‰∏îÂè™ËÄÉËôëËøô‰∏â‰∏™Âõ†Á¥†„ÄÇÈÇ£‰πàÔºåÊàë‰ª¨Ëã•Á†îÁ©∂Â∏¶ÊâìÁÅ´Êú∫ÈÄ†ÊàêÁôåÁóáÁöÑcausalityÔºåÂ∞±‰ºöÂõ†‰∏∫common causeËÄå‰∫ßÁîü‰∏ÄÂÆöÁöÑËÅîÁ≥ª„ÄÇÂõ†‰∏∫Êàë‰ª¨ËÆ©‰∏Ä‰∏™‰∫∫$do(A=1)$Ôºå‰ªñÂæóÁóÖËøòÁî±ÊòØÂê¶Âê∏ÁÉüÂØºËá¥ÔºåÂç≥$E(Y|A)\\neq E(Y|do(A=1))$„ÄÇÊ≠§Êó∂ÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÂõ∫ÂÆöÊµãËØï‰∫∫‰∏∫Âê∏ÁÉüÊàñ‰∏çÂê∏ÁÉü„ÄÇÂ∞±ÂèØ‰ª•ÂæóÂà∞ÊØîËæÉÁ∫ØÁ≤πÁöÑcausality„ÄÇ \\begin{aligned} E[y|do(x)] &= \\sum E[y|do(x),z]p(z)\\\\ &=\\sum E[y|x,z]p(z) \\end{aligned}ËøôÈáåÔºåÊàë‰ª¨ÁöÑ$E$ Â∞±ÂèØ‰ª•apply‰∏Ä‰∫õÊ®°ÂûãÔºåÂ∞±ÂÉèÊàë‰ª¨Âú®statisticsÈáå‰∏ÄÊ†∑„ÄÇ","link":"/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/"},{"title":"Report - VC Dimension","text":"VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a large VC dimension implies a powerful and complex model. However, it may not always be a good signal. If the VC dimension is far away large, then there will be problems in out-sample error, which may present as the overfitting problem. This report aims at discussing why the VC dimension is required and how it comes into being based on the process of machine learning. Notice that the thinking track of the report is based on the lecture Jiaming Mao presents. Figures are from the slides and the Internet. Let‚Äôs talk about the feasibility of machine learning first. Feasibility ConditionsProcess of Machine LearningMachine learning is predicting the value of $y$ based on the value of $x$. And the picture below shows the process of a learning. $f(x)$ is the underlying mechanism, so called target function. And $\\mathcal{H}$ is a set of candidate formula (also called hypothesis set or model), from which, through the algorithm $\\mathcal{A}$ (calculating the loss), we can choose a ‚Äúbest‚Äù hypothesis $g$ that approximately equals to $f(x)$. The error emerged within the training data is called in-sample error and the one generated in testing data is called out-sample error. In such a process, we‚Äôd like to get $E_{out}=0$ if possible. Nevertheless, we take a compromise and make it as $E_{out}\\approx0$ for there always are some disturbing terms. Now, a problem is that we can only observe $E_{in}$, which is the experience from the training data. Thus, we wonder whether $E_{out}\\approx E_{in}\\approx0$ such that $E_{in}$ can be a measurement. And it remains a problem here. Hoeffding‚Äôs InequalityHere we have an example, which is drawing marbles. The bin contains many many differently-colored marbles, and in this case we‚Äôd like to figure out the probability of red marbles in the bin, denoted by $\\mu$. It is impossible for us to count every marbles and what we can do is to draw samples. Then, we denote the probability of red marbles in the samples as $\\nu$. According to the law of large numbers, intuitively, the larger the sample is, the closer the distance between $\\mu$ and $\\nu$. One of the pattern is the Hoeffding‚Äôs inequality, (if the proof is wanted, click here.) Pr(|\\mu-\\nu|>\\epsilon)\\epsilon)\\epsilon)\\cup\\cdots] &\\le Pr[(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon)\\ \\text{or}\\ (|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon)\\cdots]\\\\ &\\le Pr(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon) + Pr(|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon) + \\cdots\\\\ &\\le \\sum_{i=1}^M 2\\exp(-2\\epsilon^2N) = 2M\\exp(-2\\epsilon^2N) \\end{aligned}, $M$ denotes the number of hypothesis in $\\mathcal{H}$. Clearly, if $M$ is large, then the model is complex. And let‚Äôs summarize the inequality as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2M\\exp(-2\\epsilon^2N)To sum up, there are two conditions if machine learning is feasible. The Algorithm $\\mathcal{A}$ can find a $g$ such that $E_{in}\\approx0$, and it is consistent with the process ‚Äútrain‚Äù. Actually, the more complex the model is, the smaller the $E_{in}$ is. $M$ is finite and a $N$ is sufficiently large to make $E_{out}\\approx E_{in}$, and it is consistent with the process ‚Äútest‚Äù. Next, we‚Äôd like to focus on the second condition, and mainly talk about the $M$. VC DimensionGenerally, $\\mathcal{H}$ contains infinite $h$. For example, a simple linear model $y=ax+b$ generates infinite $h$, because the parameters are changeable. How do we shrink the upper bound by substitute $M$? Effective Number of HypothesisIn the process that expanding Hoeffding‚Äôs inequality, the inequality below is used, Pr(A_1\\ \\cup\\ A_2\\ \\cup\\ A_3\\ \\cdots) \\le \\sum_{i=1}^M Pr(A_i)If and only if $A_i$s are independent, we choose the equal sign. Actually, there are many intersection part in $\\mathcal{H}$. A simple example is shown in the graph. In the two-dimensional space, we can simply divide $\\mathcal{H}$ into two parts. One judges that $x_1$ is positive, like $h_1$. Another one marks $x_1$ as negative, like $h_2$. Here, the effective number of hypothesis is two. Under a specific sample, the effective number is fixed. Thus, the inequality can be written as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2*\\text{effective}(M)*\\exp(-2\\epsilon^2N)Growth FunctionDichotomyFirstly, we‚Äôd like to introduce a binary target function and hypothesis set that contains $h:\\mathcal{X}\\to\\{-1,+1\\}$. And, h(X_1,X_2,\\cdots,X_n) = (h(X_1), h(X_2), \\cdots, h(X_n))is called one dichotomy. Generally speaking, dichotomy represents a result which marks all points in the sample. $\\mathcal{H}(X_1,X_2,\\cdots,X_n)$ denotes the number of all dichotomies generated by hypothesis set $\\mathcal{H}$, given $n$ points. For instance, if three points are in the training set, One dichotomy is $(h_1(X_1), h_1(X_2), \\cdots) = (+1,+1,+1)$. Another dichotomy can be $(h_2(X_1), h_2(X_2), \\cdots) = (-1,+1,+1)$. ‚Ä¶ $2^3=8$ is the maximum dichotomies that $\\mathcal{H}(X_1,X_2,\\cdots)$ can value. Say, the number of dichotomies a $\\mathcal{H}$ can generate at most is $2^n$. Shatter Given n points, (and their locations are fixed,) if a hypothesis set $\\mathcal{H}$ can generate exactly $2^n$ dichotomies, we call $\\mathcal{H}$ shatters these points. An example is under two-dimensional dataset and the hypothesis set contains all linear model. There are three points, no matter what points are black, there always be a line that can separate black points from white points. (See the figure below.) As we mentioned above, three points have eight dichotomies. And black and white points are like the $+1,-1$ marks. If the model can tell the marks apart in every scenario, then we can say that the model, or $\\mathcal{H}$, shatters the points. However, if there are four points, then the linear model cannot shatter them. We call the smallest number of points as Break Point. Say, 4 is the break point for the two-dimensional linear regressor. Growth FunctionWe define the growth function as, m_\\mathcal{H}(n)=\\max_{X_1,X_2,\\cdots,X_n \\in \\mathcal{X}} |\\mathcal{H}(X_1,X_2,\\cdots,X_n)| That is, $m_\\mathcal{H}(n)$ is the maximum possible number of dichotomies $\\mathcal{H}$ can generate on a dataset of n points. Obviously, $m_\\mathcal{H}(n)\\le2^n$. The larger the $m_\\mathcal{H}(n)$ is, the more powerful the model is. And $m_\\mathcal{H}(n)$ can tell us whether the dataset of size $s$ can be shattered by $\\mathcal{H}$. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) Now, can we simply exchange $\\text{effective}(M)$ to $m_\\mathcal{H}(n)$ to shrink the upper bound? VC boundNo, because the upper bound for $m_\\mathcal{H}(n)$ is still an explosively increasing function. It is hard to tell whether the result can converge to zero. Sauer‚Äôs Lemma solves the problem. If the break point $k$ exists, then $m_\\mathcal{H}(n)$ is a polynomial. According to Sauer‚Äôs Lemma, m_\\mathcal{H}(n) \\le \\sum_{i=0}^{k-1} \\begin{pmatrix} N\\\\ i \\end{pmatrix}\\le\\begin{cases} N^{k-1}+1,\\\\ (\\frac{eN}{k-1})^{k-1}, \\text{if } N\\ge k-1 \\end{cases} And the polynomial is much easier to converge and it releases a signal that it is feasible to make the upper bound to be zero! Through some complicated transformations, the inequality can be written as, (the proof is here) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*m_\\mathcal{H}(2N)*\\exp(-\\frac{1}{8}\\epsilon^2N) This inequality is the VC bound and the exponential part has a higher rate of convergence than the polynomial part. Thus, the inequality tells that if there is a break point, when $N$ is sufficiently large, for any hypothesis $h$, $E_{out}\\to E_{in}$ is very possible. Then the machine learning in the space is feasible, which meets the second condition. If the significance level is $\\delta$, and the upper bound should be $\\delta$. Then, \\Rightarrow Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le\\delta\\\\ \\epsilon = \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\\\ E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}Ôºå\\text{probability 1} -\\deltaThe bound is called VC generalization bound, which is usually applied in training model because we are required to select a best hypothesis. But, we will not apply it onto the testing. Instead, we use the original Hoeffding‚Äôs Inequality for there is only one hypothesis and the VC bound is much looser. Why is VC bound so loose? The basic Hoeffding‚Äôs inequality used in the proof already has a slack. Using $m_\\mathcal{H}(N)$ to quantify the number of dichotomies on N points, regardless of which $N$ points are in the dataset, gives us a worst-case estimate (because we expanded the inequality to all $h$). Bound $m_\\mathcal{H}(N)$ by a simple polynomial. VC DimensionNow, we can talk about the definition of VC dimension. The Vapnik-Chervonenkis (VC) dimension of $\\mathcal{H}$, denoted $d_{vc}(\\mathcal{H})$ is the size of the largest data set that $\\mathcal{H}$ can shatter. Remember that we have already talked about this problem in part Growth Function. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) And here, $d_{vc}(\\mathcal{H})=3$. If $m_\\mathcal{H}(n)=\\infty$, then $d_{vc}(\\mathcal{H})=\\infty$. In general, $d_{vc}(\\mathcal{H})=k-1$, where $k$ is the break point. If the VC dimension is large, the model $\\mathcal{H}$ is complex because a large sample can be shattered by it. Thus, VC dimension tells the ability of learning for the model. The definition of VC dimension has nothing to do with the locations of points in dataset, which leads to a problem. For example, again, under two-dimensional plane, there are three points. However, this time, they are on the same line (See the graph). A linear model cannot shatter the dataset now. Therefore, VC dimension tells you the largest dataset $\\mathcal{H}$ can shatter, but not every same-sized dataset can be shattered. The VC bound can also be written as, (the first inequality in Sauer‚Äôs Lemma is usually used.) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*((2N)^{d_{vc}(\\mathcal{H})}+1)*\\exp(-\\frac{1}{8}\\epsilon^2N)Approximation-Generalization TradeoffRecall the two conditions if machine learning is feasible, and now, the two conditions should be, The algorithm $\\mathcal{A}$ can find the $g$ that makes $E_{in}\\approx0$. Actually, the larger the VC dimension is (the more complex the model is), the smaller the $E_{in}$ is. The VC dimension are finite to make the polynomial part easy to converge and $N$ should be sufficiently large so that $E_{out}\\approx E_{in}$. Also, recall the relationship that we talked in VC bound, E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}Ôºå\\text{with probability 1} -\\deltaLet $R = \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}} $. If $N$ holds constant, then if $d_{vc}(\\mathcal{H})$ increases, then $R$ will increases, which means the bound will become loose. As a result, $E_{out}$ may get away from $E_{in}$. Nevertheless, $\\mathcal{H}$ will be more complicated. Say, $E_{in}$ decreases, even reaches the scenario that $E_{in}=0$. We call it approximation-generalization tradeoff. When one condition gets too tight, another one will be hard to meet. Therefore, we‚Äôd better find a point where we can balance these two well. And based on the inequality, it is how the VC dimension functions to make machine learning feasible. SummaryStarting from the two feasibility conditions, this report mainly discusses how the VC dimension comes into being and how should we understand VC dimension. VC dimension is important because it regulates the two conditions for machine to learn, which is the approximation-generalization tradeoff. Actually, mathematics is just a tool for us to shrink the bound and VC dimension is the aggregation of them, which can help us study further in machine learning. References Jiaming Mao, Data Analysis for Economics Liubai01‚Äôs blog, Êú∫Âô®Â≠¶‰π†Êé®ÂØºÂêàÈõÜ01 ECE 901 Lecture 19 Yaser &amp; Malik &amp; Hsuan-tien Lin, Learn from Data [M], California Institute of Technology Hsuan-tien Lin, Machine Learning Foundation Picture References&nbsp;&nbsp; Vector Landscape Vectors by Vecteezy &nbsp;&nbsp; Data Analysis by Jiaming Mao &nbsp;&nbsp; Machine Learning by Hsuan-tien Lin","link":"/2021/03/29/VC%20Dimension/"},{"title":"Report - Regression Splines","text":"Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline belongs to non-parametric family and it is the feature that endows the regression high flexibility. This report aims to tell you a story about Why we need Regression Splines, How we make the regression ‚ÄúSpline‚Äù. However, before the story, it‚Äôs inevitable to talk about some basic regression. Let‚Äôs briefly review them. Intuitively, we simulate dots based on $f(x)=\\sin(x), x\\in[-\\pi,2\\pi]$ with the Gaussian-distributed error. For the graph, a code will be attached. If you ‚Äòd like to see the code, please just unfold it. Click to Unfold12345678910111213141516import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinenp.random.seed(15)## Generate 150 dotsx = np.linspace(-np.pi,2*np.pi,150)y0 = np.sin(x)e = np.random.normal(0,1,150)y = y0+e # Add error term# plotfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x,y,color='lightgrey')ax.plot(x,y0,color='r') # oringinal traceplt.show() From Linear to PolynomialLinear RegressionAt the beginning, we always apply the linear regression. Say, the hypothesis set $\\mathcal{H}=\\{h(x)\\} $ consisting of linear functions, assuming there are $N$ samples and $p$ dimensions per sample. h(x) = x'\\beta,\\ \\ \\text{where } x=\\begin{pmatrix} 1\\\\ x_1\\\\ \\vdots\\\\ x_p \\end{pmatrix} ,\\ \\beta=\\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p \\end{pmatrix}And the $\\beta$ determines whether the function is the best one. Of course, L2 loss is always chosen to measure best. The calculation of $E_{in}$ can be written below, \\begin{aligned} E_{in} &= \\frac{1}{N}\\sum_{i=1}^N (x_i'\\beta-y_i)^2\\\\ &= \\frac{1}{N}\\left|\\begin{array}{c} x_1'\\beta - y_1 \\\\ \\vdots \\\\ x_N'\\beta - y_N \\end{array}\\right|^2\\\\ &=\\frac{1}{N}\\left|X\\beta-\\vec{y}\\right|^2,\\ \\text{where } X = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 1 & x_{N1} & \\cdots & x_{Np} \\end{bmatrix} \\end{aligned}To minimize it, the gradient should be zero, \\frac{\\partial}{\\partial \\beta}\\frac{1}{N}(\\beta'X'X\\beta-2\\beta'X'\\vec{y}+\\vec{y}'\\vec{y})=\\vec{0}\\\\ \\frac{2}{N}(X'X\\beta-X'\\vec{y})=\\vec{0}\\\\ \\beta = (X'X)^{-1}X'\\vec{y}Be sure to remember the equation that generates best $\\beta$, it will be the foundation for the following regressions. And the estimator is BLUE (best linear unbiased estimator), proved by Gaussian-Markov Theorem. Click to Unfold1234567891011121314X = np.ones((150,2))X[:,1] = x # Matrix X is generatedY = np.zeros((150,1))Y[:,0] = y # Matrix Y is generatedpseudo = np.dot(np.linalg.inv(np.dot(X.T,X)), X.T) # Calculate the pseudo-inversebeta = np.dot(pseudo,Y)y_lin = beta[0]+beta[1]*xfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x, y, color='lightgrey')ax.plot(x, y0, color='r', label='Original') # oringinal traceax.plot(x, y_lin, color='y', label='Linear')ax.legend()plt.show() --- ### Polynomial #### Basis Expansion Actually, there are many underlying functions that are not typically linear. Then, the question is how to deal with the non-linear terms. Our solution is simply to regard each non-linear term as **a whole \"degree-one\" term**. For example, the model we'd like to apply is $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2$. The only non-linear part is $x^2$, and we use $\\phi(x)=x^2$ to replace it. The result is $h(x)=\\beta_0+\\beta_1x+\\beta_2\\phi(x)$. Practically, we this method can be applied on linear terms to build a kind of consistency. Say, $\\phi$ can be any function including dummies and constants. $\\phi(x)$ is called basis function. A linear basis function model is defined as, y=\\sum_{i=1}^M \\beta_i\\phi_iJiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Linear basis function model ensures that non-linear regression can be calculated by OLS because it shows the consistent linear form. And, $\\hat{\\beta}=(\\Phi'\\Phi)^{-1}\\Phi'Y$, where $\\Phi=(\\phi_1,\\cdots,\\phi_M)$'. #### Polynomial Regression We assume that the dots we simulated is underlying a cubic polynomial function, which is, specifically, $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3 x^3$. Follow the procedure, we can get a regression curve shown in the first graph. However, when we increases the polynomial degree, the regression curve tends to be more and more abnormal. Actually, it's a problem called **overfitting**, where $E_{out}$ is large while $E_{in}$ is very small. (In order to roughly compare the regression models, $R^2$ is also attached on figures.) In statistics, overfitting is ‚Äúthe production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably‚Äù. LeineberD.J (2007). Stupid data miner tricks Click to Unfold1234567891011121314151617181920212223242526272829303132333435363738394041def polynomial_regression(x,y,degree=3): #define a polynomial regression function Y = np.zeros((len(y),1)) Y[:,0] = y # Matrix Y is generated polynomial_X = np.ones((len(x),degree+1)) for i in range(degree): polynomial_X[:,i+1] = np.power(x,i+1) polynomial_pseudo = np.dot(np.linalg.inv(np.dot(polynomial_X.T,polynomial_X)), polynomial_X.T) # Calculate the pseudo-inverse polynomial_beta = np.dot(polynomial_pseudo,Y) return np.dot(polynomial_X, polynomial_beta)def R2(y,predict_y): # R-square is attached my = np.mean(y) denominator = np.sum((y-my)**2) numerator = np.sum((predict_y-my)**2) return numerator/denominatord3y = polynomial_regression(x,y,3)d5y = polynomial_regression(x,y,5)d7y = polynomial_regression(x,y,7)d9y = polynomial_regression(x,y,9)fig, axes = plt.subplots(2,2,figsize=(10,10))for i in [0,1]: for j in [0,1]: axes[i][j].scatter(x,y,color='lightgrey') axes[i][j].plot(x,y0,color='r',label='Original trace') axes[i][j].grid()axes[0][0].plot(x, d3y, color='g', label='3 Degree Polynomial')axes[0][0].legend()axes[0][0].annotate('$R^2={:.5f}$'.format(R2(y,d3y)), xy=(-2,2))axes[0][1].plot(x, d5y, color='g', label='5 Degree Polynomial')axes[0][1].legend()axes[0][1].annotate('$R^2={:.5f}$'.format(R2(y,d5y)), xy=(-2,2))axes[1][0].plot(x, d7y, color='g', label='7 Degree Polynomial')axes[1][0].legend()axes[1][0].annotate('$R^2={:.5f}$'.format(R2(y,d7y)), xy=(-2,2))axes[1][1].plot(x, d9y, color='g', label='9 Degree Polynomial')axes[1][1].legend()axes[1][1].annotate('$R^2={:.5f}$'.format(R2(y,d9y)), xy=(-2,2))plt.show() A high-degree polynomial does fit samples well, but will the underlying mechanism really perform in that way? What if it meets some extrapolating data? Actually, we are always trapped in a thought that all samples follow a global mechanism. The global regression with high orders induces Runge‚Äôs Phenomenon (overfitting problem). How about break the domain into pieces? Say, piecewise function. Regression SplinePiecewise RegressionPiecewise regression breaks the input space into distinct regions and fit a different relationship in each region. Jiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Piecewise PolynomialsAs mentioned above, a piecewise polynomial function is obtained by dividing the domain of ùëã into contiguous intervals and representing the function by a separate degree-d polynomial in each interval. In mathematics, if there are $n$ knots, h(x) = \\begin{cases} \\beta_0\\phi_0(x), & x","link":"/2021/03/31/Report-Regression-Spline/"}],"tags":[{"name":"ÂæÆËßÇËÆ°Èáè","slug":"ÂæÆËßÇËÆ°Èáè","link":"/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"}],"categories":[]}