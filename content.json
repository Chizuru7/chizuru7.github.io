{"pages":[],"posts":[{"title":"Challenge: Ordered Logit","text":"Ordered Logit is in the family of ordinal regression. Itâ€™s widely used when facing ordered categorical outcomes. For example, in bond ratings, there are $AAA, AA, A, B$ etc. Of course, we can simply apply multinomial logistics model to fit it. However, we may lose the information of the ranking in the dependent variables. As a result, ordered logit (or ordered probit) is the â€˜evolutionâ€™ version. Mechanism of Ordered LogitReminder of Multinomial LogisticsIf we consider the binary logit model, the function should be, Pr(y=1|x) = \\frac{\\exp(x'\\beta)}{1+\\exp(x'\\beta)}, which implies that, \\ln \\frac{Pr(y=1|x)}{Pr(y=0|x)} = x'\\betaExpanding the kind of thoughts to the multinomial part, there is the soft-max function for n choices, \\sigma_j(x'\\beta_j) = \\frac{\\exp(x'\\beta_j)}{\\sum_{i=1}^n\\exp(x'\\beta_i)}The core thinking is that we can pick any two categories to do a binary logistic regression. Even if each pair has different underlying classification criteria (which generates different $\\beta$), we can still merge these regressions together to form the soft-max function. Therefore, the fraction of probabilities is, \\ln \\frac{Pr(y=j|x)}{Pr(y=k|x)} = x'(\\beta_j-\\beta_k)And we use the MLE (Maximum Likelihood Estimation) method to estimate it. Parallel Regression AssumptionNevertheless, if ordered logit is desired, then we cannot belike soft-max function to have many different $\\beta$-s. Simply speaking, the dependent variables of ordinal regression always contain the rank information. Rank always implies comparisons. For example, is it reasonable or fair for a bond credit rating firm to have different grading criteria onto the very bond that is not evaluated yet? The answer is certainly no. We call the assumption as Parallel Assumption or Proportional Odds Assumption. TestsIt is a strong assumption so that we unify the coefficients among categories. Practically, we have many methods to test whether the dataset can pass the assumption. Brant test, Wald test and LR test are the most popular ways to verify it. If not all variables pass the examinations, then a significant result will be thrown out. Generally, Wald test is looser than Brant test. Here, Wald test will be introduced briefly. Wald test is a kind of likelihood ratio test but lowers much more computation burdens than that within LR. WikiPediaen.wikipedia.org/wiki/Wald_test Wald test performs one-by-one in finding the bad variables in ordered logit model within Stata. H_0: \\hat{\\beta} = \\beta_0\\\\ H_1: \\hat{\\beta} \\ne \\beta_0\\\\ W = \\frac{(\\hat{\\beta}-\\beta_0)^2}{Var(\\hat{\\beta})}\\to \\chi^2, where $\\hat{\\beta}$ was found as the maximizing argument of the unconstrained likelihood function and it is compared with a hypothesized one $\\beta_0$. We hope that null hypothesis passes so an insignificant p-value is desired. Details of both tests can tell what variables violate the assumption. Itâ€™s also called Partial Proportional Odds and we will talk about the situation later. Distinguish CategoriesThresholdsYou must have questions about how to distinguish categories under the circumstance of $\\beta$-s being the same. Notice that it indicates that every category shares the same Logit function. And a idea comes out. One variable increases, it would result in a shift toward either end of the spectrum of ordinal responses. Said another way, the probability of responding toward either end of the spectrum would increase as the predictor variables change in a given direction. Therefore, we assume some thresholds $\\alpha$ aiming at dividing the rank for categories. For example, if an index model for a single latent variable $y_i^*=x_iâ€™\\beta+u_i$ and $u_i$ follows the Gumbel, then, y_i = j \\hspace{1em} \\text{if} \\hspace{1em} \\alpha_{j-1}\\le y_i^*\\le \\alpha_jSuppose it is a health survey with three ($J$) levels, then there should be two ($J-1$) intercepts. What should be kept in mind is that we donâ€™t know the thresholds beforehand and thresholds are unlikely to be averagely divided. A good example is that good health might have little differences with normal health, but normal health can be much better than unhealthy. Thus, the probability of individual $i$ choosing $j$ (just like the normal health case in the graph) is, Pr(y_i=j) = Pr(\\alpha_{j-1}-x_i'\\beta\\le u_i\\le \\alpha_j-x'_i\\beta) = F(\\alpha_j-x'_i\\beta)-F(\\alpha_{j-1}-x_i'\\beta) , where $F = \\frac{\\exp(z)}{1+\\exp(z)}$, for $u_i$ follows standard $\\text{Gumbel}$ distribution. (Of course, if $u_i$ is distributed under the standard normal, then the $F$ can be the $cdf$ of it. And that is the ordered probit.) Particularly, $Pr(y_i=1)=F(\\alpha_1-xâ€™\\beta)$ and $Pr(y_i=J) = 1-F(\\alpha_{J-1}-xâ€™\\beta)$. And the fraction of probability is, \\ln \\frac{Pr(y_i=j|x_i)}{Pr(y_i=k|x_i)} = \\alpha_j-\\alpha_{j-1} - (\\alpha_k - \\alpha_{k-1})As you can see that the equation is $\\beta$-free because it satisfies the parallel assumption. EstimationOnce we have the probability function, we can estimate it by MLE. The log-likelihood function is, \\ln \\mathcal{L}(\\beta, \\vec{\\alpha}|x_i,y_i) = \\sum_{j=1}^J [y_i=j]\\ln [F(\\alpha_{j-1}-x'\\beta)-F(\\alpha_j-x'\\beta)], where $[y_i=j]$ is an Iverson bracket. If we continue using the core thinking in the multinomial logistics here, we can separate each $\\alpha$ independently and the binary (smaller than $j$ or larger than $j$ are the two groups) logistics function is easy to get, \\ln\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)} = \\alpha_j -x'\\beta\\\\ \\Rightarrow Pr(y_i\\le j) = \\frac{\\exp(\\alpha_j-x'\\beta)}{1+\\exp(\\alpha_j-x'\\beta)}And here, we can operate it as if we were doing a binary logistics regression but for many times. InterpretationCoefficientsOne way to interpret the coefficients is via a proportional odds ratio. Say, you can interpret it mainly on the latent variable then interpret it in the exponential way. For example, one ordered logit regression outcome is, \\ln (\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)}) = \\alpha_j + 0.2*\\text{Gender}, where $\\text{Gender}=\\begin{cases}1, \\text{if Male}\\ 0, \\text{if Female} \\end{cases}$. The odds ratio for gender is $\\exp(-\\beta)=\\exp(0.2)=1.22$. Say, the latent variable will increase $0.2$ so will the likelihood of a higher rank. Then the odds of rating for a man is 1.22 times higher than that for a woman. Marginal EffectsAn advantage of the ordered logit regression is that we can estimate the marginal effect of each regressor on the probability that individual $i$ chooses alternative $j$ ($p_{ij}$), \\frac{\\partial p_{ij}}{\\partial x_r} = \\beta_r[f(\\alpha_{j-1}-x'\\beta)-f(\\alpha_j-x'\\beta)]=\\beta_r\\Delta f, where $f$ is the $pdf$ of logit function and $x_r$ is the $r^{th}$ regressor. And the marginal effects across all alternatives for individual $i$ sums up to zero. This interesting property inherits from logit-like functions (like logit function and soft-max function). \\begin{aligned} \\sum_{j=1}^J \\frac{\\partial p_{ij}}{\\partial x_r} &= \\beta_r [f(\\alpha_1-x'\\beta)+f(\\alpha_2-x'\\beta)-f(\\alpha_1-x'\\beta)+\\cdots\\\\ &+f(\\alpha_{J-1}-x'\\beta)-f(\\alpha_{J-2}-x'\\beta)-f(\\alpha_{J-1}-x'\\beta)]\\\\ &=\\beta_r*0=0 \\end{aligned}The interpretation of marginal effect is that each unit increase in independent variable increase/decrease the probability of selecting alternative $j$ by the amount expressed as a percent (for probability is percentage). We can see from the graph that $\\Delta f$ is actually the difference of boundary, if selecting â€˜Normal Healthâ€™. Generalized Ordered LogitRemember that we remained a problem called Parital Proprtional Odds? And weâ€™d like to talk about it now. If some $\\beta$-s differ across different categories, what should we do? Here comes the Generalized Ordered Logit, which looses the strong parallel assumption while somehow remaining the rank information in regression formula. In Parallel Assumption, we talked about some test like Brant test, Wald test and you can immediately find the variables that violate the assumption. Then, we can reconstruct our formula based on ordered logit model, \\ln\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)} = \\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j}\\\\ \\Rightarrow Pr(y_i\\le j) = \\frac{\\exp(\\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j})}{1 + \\exp(\\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j})}, where the undermark $\\text{not}$ represents not violate. The $\\alpha$-s here inherits from the idea. It is still the threshold to represent the ranks of categories even if every binary logistics doesnâ€™t share the same $pdf$. Whatâ€™s more, the estimation or interpretations are still the same. We can say that ordered logit is a particular of generalized ordered logit. NOTE! In some software packages, Stata as the example, if you use the command gologit to do a generalized ordered logit, there will be a totally different result with ologit even if no $\\beta$ varies. This is because gologit is a user-written package, and the model is, \\ln\\frac{Pr(y_i>j)}{1-Pr(y_i>j)} = \\alpha_j + x'_{\\text{not},i}\\beta_{\\text{not}} + x'_{\\text{vio},i}\\beta_{\\text{vio},j}, where the greater-than sign matters and change all the results. So plz be very careful! Empirical ExampleAbout the DatasetLong and Freese (2006) present data from the 1977/1989 General Social Survey. Respondents are asked to evaluate the following statement: â€œA working mother can establish just as warm and secure a relationship with her child as a mother who does not work.â€ Responses were coded as 1 = Strongly Disagree (1SD), 2 = Disagree (2D), 3 = Agree (3A), and 4 = Strongly Agree (4SA). Practically, the number should impart the rank information. Explanatory variables are yr89 (survey year; 0 = 1977, 1 = 1989), male (0 = female, 1 = male), white (0 = nonwhite, 1 = white), age (measured in years), ed (years of education), and prst (occupational prestige scale). In this example, we will use 3A as the base outcome. Assumption TestAs we mentioned in tests, there are many can figure out whether the dataset satisfies the parallel assumption. 1oparallel And the result is shown below, Test Method Chi2 df P&gt;Chi2 Wolfe Gould 48.91 12 0.000 Brant 49.18 12 0.000 score 48.42 12 0.000 likelihood ratio 49.2 12 0.000 Wald 48.55 12 0.000 For the reason why the degree of freedom is 12, it is because we have 6 variables and each of them provides a $\\chi^2$. A significant test statistic provides evidence that the parallel regression assumption has been violated. 1auto lrf store(gologit2) Itâ€™s an optional command following after the gologit2 regression and we show it here in advance. In the detail, you will find that only yr89, male violate the assumption. variables p-value white 0.7136 ed 0.1589 prst 0.2046 age 0.0743 yr89 0.00093 male 0.00002 Ordered LogitHowever, in order to perform the ordered logit, we kick those two bad variables out. 1ologit warm white age ed prst Here is the result, Variables Coef. white -.4478496* age -.0186559* ed .078407* prst .0053391 /cut1 -2.056959 /cut2 -.2866278 /cut3 1.525569 Generalized Ordered LogitIf weâ€™d like to take all variables into account, we should use the generalized ordered logit since it doesnâ€™t require a $\\beta$ that across all categories. And before we operate it in Stata, we must consider the true model that gologit give, which we talked at the note. 1gologit2 warm yr89 male white age ed prst, auto lrf store(gologit2) Note that gologit2 is used when some but not all variables are parallel in $\\beta$. And the result is, Variables 1SD 2D 3A yr89 0.984* 0.534* 0.326* male -0.333* -0.693* -1.098* white -0.383* -0.383* -0.383* age -0.022* -0.022* -0.022* ed 0.067* 0.067* 0.067* prst 0.006* 0.006* 0.006* _cons 2.122* 0.602* -1.048* The model parameterization dictates the interpretation of the odds ratio. Take the odds ratio for gender as example, using Statâ€™s estimates, the odds ratio for gender is $exp(-\\beta_{male})$, which is 0.7168, 0.5, 0.3353 when calculating $P(Y&gt;1)$, $P(Y&gt;2)$, $P(Y&gt;3)$. The impact of gender shows heterogeneity. The odds of rating score higher than 1, 2, 3 is respectively 0.7168, 0.5, 0.3353 times lower for man than it is for women. We calculate the accuracy and find it is $43.26\\%$. Comparison to Multinomial LogisticsWe also implement a multinomial logistics regression on the dataset for you to show the difference. 1mlogit warm yr89 male white age ed prst The accuracy here is $42.4\\%$, which is lower than the generalized ordered logit. Thus, generalized ordered logit is more efficient when facing a category with rank. ReferencesList of Works Richard Williams, Generalized Ordered Logit Models Agresti, Alan. â€œCategorical Data Analysis.â€ New York: Wiley, 2002. Jiaming Mao, Data Analysis for Economics Cornell Consulting Unit #91 Econometrics Academy, Ordered Logit and Ordered Probit Picture References&nbsp;&nbsp; Pixiv by Alcxome","link":"/2021/05/04/Challenge-Ordered-Logit/"},{"title":"Data Analysis for Economics Intro","text":"ç»ˆäºŽæœ‰ä¸€ä¸ªç›¸å¯¹è½»æ¾çš„å­¦æœŸäº†ï¼Œå°±åœ¨è¿™é‡Œè®°å½•ä¸€ä¸‹èŒ…å®¶é“­è€å¸ˆçš„å¾®è§‚è®¡é‡è¯¾ç¨‹ã€‚è¿™æ˜¯æœ¬å­¦æœŸè§‚æ„Ÿæœ€å¥½çš„ä¸€é—¨è¯¾ç¨‹äº†ï¼Œå®ƒåœ¨æ³°å¼è‹±è¯­ï¼Œè¿žçŽ¯é€¼é—®çš„é—¨é—¨è¯¾ç¨‹ä¸­å¤ªè¿‡é¹¤ç«‹é¸¡ç¾¤ï¼Œè€å¸ˆä¹Ÿæ˜¯WISEä¸€é¡¶ä¸€çš„ç”·ç¥žï¼Œæ•™å­¦èµ„æºä¹Ÿæ˜¯ç›¸å½“å¥½çš„ã€‚ï¼ˆæ•™å­¦ç½‘ç«™ï¼šç‚¹å‡»è¿™é‡Œã€‚ï¼‰äºŽæ˜¯æƒ³åœ¨è¿™é‡Œç”¨é€šä¿—ä¸€äº›çš„è¯æ¥è¿‡ä¸€ä¸‹è€å¸ˆä¸Šè¯¾çš„ä¸€äº›å†…å®¹ï¼Œç„¶åŽæŽ¨å¯¼ä¸€äº›å…¬å¼ï¼Œä¹Ÿé˜²æ­¢åŽé¢æ‘¸é±¼æ‘¸å¾—ä»€ä¹ˆéƒ½å¿˜äº†ã€‚è¿™ä¹Ÿæ˜¯æˆ‘çš„ç¬¬ä¸€ç¯‡åšæ–‡ï¼Œæ˜¯æœ‰å¾ˆå¤šä¸è¶³ä¹‹å¤„çš„ï¼Œå†™ä¸‹æ¥ä¹Ÿå¾ˆç”Ÿç–ï¼Œæ˜¯æˆ‘è„‘å­é‡Œæ„è¯†æµçš„Intro partï¼Œè¯¦ç»†è¿˜æ˜¯å‚è§èŒ…ç¥žçš„è¯¾ç¨‹å§ã€‚ æ­£æ–‡Machine learning $\\to$ Statistics $\\to$ Econometrics ä»Žå·¦åˆ°å³æ˜¯ä¸€ä¸ªä»Žç»“æžœæŽ¨æ–­(Prediction)å‘å› æžœæŽ¨æ–­(Causality)åŠ å¼ºçš„é“¾ã€‚åŸºäºŽç»Ÿè®¡å­¦çŸ¥è¯†ï¼Œå‰è€…ä¸»è¦è§‚æµ‹ç‰¹å¾ï¼ŒåŽè€…æƒ³æ‰¾å‡ºä½œç”¨æœºç†ã€‚è€Œæœºå™¨å­¦ä¹ æ¨¡åž‹çš„å‡†ç¡®åº¦å®žé™…ä¸Šæ˜¯å¤§å¤§è¶…è¿‡ä¼ ç»Ÿçš„ä¸€äº›è®¡é‡æ–¹æ³•çš„ï¼Œæ¯”å¦‚Panel Data Regressionå’ŒLogit Modelã€‚ä½†æ˜¯è¿™é‡Œçš„ç¼ºé™·åœ¨äºŽå¾ˆéš¾è¡¨ç¤ºå‡ºç»æµŽå­¦çš„ä¸€äº›insightsï¼Œç±»å¦‚éšæœºæ£®æž—ç­‰ç®—æ³•ï¼Œå¾ˆéš¾æœ‰ä¸€ä¸ªç»æµŽå­¦æ ‡å‡†çš„interpretationï¼Œè€Œè¿™äº›å´æ˜¯è®¡é‡ä¸­å¾ˆéœ€è¦çš„ã€‚è¿™ä¸ªé—®é¢˜çš„å‡ºçŽ°ä¸»è¦æ˜¯å› ä¸ºRelevanceä¸èƒ½è¯´æ˜ŽCausalityï¼Œä¸¤è€…çš„ç»ˆæžç›®æ ‡ä¹Ÿå› æ­¤åˆ†å¼€ã€‚ Statistics$x$ å’Œ $y$ æ˜¯ä¸¤ä¸ªå˜é‡ï¼Œå¯»æ‰¾è¿™ä¸¤ä¸ªå˜é‡çš„å…³ç³»åº”è¯¥æ˜¯å¯»æ‰¾ä»–ä»¬çš„è”åˆæ¦‚çŽ‡åˆ†å¸ƒ $p(x,y)$ã€‚è€Œå®žé™…ä¸Šæˆ‘ä»¬æ›´å¤šéœ€è¦çš„æ˜¯ä»Ž $x$ å¾—åˆ° $y$ çš„è¿‡ç¨‹ï¼Œå³å»ºç«‹ä¸€ä¸ªæ–¹ç¨‹ $f(x)$ æ¥æµ‹å®š $y$ã€‚ æŸå¤±å‡½æ•°é€‰æ‹©èŒƒæ•°é€‰æ‹©æ˜¯ç”± $f(x)$ å¼•å‡ºçš„é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªå¦‚ä½•è¡¡é‡â€bestâ€œçš„é—®é¢˜ã€‚å› ä¸ºå½“å¾—åˆ°é¢„æµ‹æ–¹ç¨‹æ—¶ï¼Œæˆ‘ä»¬æ€»æœ‰$e = y-f(x)$, è¯¯å·® $e$ è¶Šå¤§æ˜¯å¯¹é¢„æµ‹æ¨¡åž‹è¶Šä¸å¥½çš„ã€‚å‡ ç§æŸå¤±å‡½æ•°æ‰€å¾—åˆ°çš„$f(x)$æ˜¯å®Œå…¨ä¸åŒçš„ã€‚ L1ä¸ŽMAEL1èŒƒæ•°æ˜¯$|x_1|+|x_2|+\\cdots+|x_n|$, è¿™é‡Œçš„æŸå¤±å‡½æ•°æ˜¯MAEï¼Œå³ï¼š$\\frac{1}{n}\\sum |e_i|$ï¼ŒMAEæ˜¯L1æ­£åˆ™åŒ–çš„ä»£è¡¨ (Lassoå›žå½’å°±æ˜¯åˆ©ç”¨äº†L1çš„æ­£åˆ™åŒ–)ã€‚æœ€å°åŒ–MAEäº§ç”Ÿçš„å°†æ˜¯ä¸­ä½æ•°ã€‚å…·ä½“å¯ä»¥å‚è§çŸ¥ä¹Ž-å­å…ƒçš„å›žç­”ã€‚ MAEæ±‚è§£çš„ä¸‹é™é€Ÿåº¦ç»Ÿä¸€ã€‚ä¸åŒä¸ŽMSEï¼Œä¼šå¯¹è¯¯å·®å°çš„é¡¹æœ‰å¤§çš„åŒ…å®¹æ€§ï¼ŒMAEä¸€è§†åŒä»ã€‚å¯¹äºŽä¸€äº›å™ªå£°å¼‚å¸¸å€¼ï¼ŒMAEæ²¡æœ‰å¾ˆå¤§çš„ååº”ï¼Œè¿™æ˜¯å®ƒç›¸è¾ƒäºŽMSEä¼˜ç§€çš„åœ°æ–¹ã€‚ L2ä¸ŽMSEL2 èŒƒæ•°æ˜¯ $\\sqrt{(x_1^2+\\cdots+x_n^2)}$ ,è¿™é‡Œç”¨äºŽä¼°ç®—æŸå¤±çš„å‡½æ•°ä¸ºMSEï¼Œå³ï¼š$\\frac{1}{n}\\sum_{i}^ne_i^2\\to E[(y-f(x))^2]$ ã€‚L2èŒƒæ•°å’ŒMSEçš„æœ€å°åŒ–è¿‡ç¨‹å®žé™…ä¸Šæ˜¯ä¸€æ ·çš„ï¼ŒMSEæ˜¯L2æ­£åˆ™åŒ–çš„ä¸€ä¸ªä»£è¡¨ (Ridgeå›žå½’å°±æ˜¯ç”¨äº†L2çš„æ­£åˆ™åŒ–)ã€‚å¦‚æžœé€‰æ‹©è¿™ä¸€ç§æ–¹æ³•ï¼Œå¾—åˆ° $f(x)$ çš„è¿‡ç¨‹å°±æ˜¯æœ€å°åŒ–MSEçš„è¿‡ç¨‹ã€‚è¿™ä¸€ä¸ªè¿‡ç¨‹å³æ˜¯å¯¹ç›®æ ‡å‡½æ•°$f(x)$æ±‚å¯¼çš„è¿‡ç¨‹ï¼Œæœ€åŽçš„ç»“æžœå³æ˜¯$f(x)$ ã€‚è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œ $y_i$ æ˜¯ç»™å®š $x$ ä¹‹åŽçš„æ•°æ®ç‚¹ï¼Œå› æ­¤$f(x)=f$,æ˜¯ä¸€ä¸ªå®šå€¼ã€‚ \\partial \\sum (y_i-f)^2 = \\sum(y_i-f)=0\\\\ \\to f=\\frac{1}{n}\\sum{y_i}=E(y|x)ç”¨MSEçš„ä¸€å¤§ä¼˜ç‚¹å°±æ˜¯å¥½ç®—ã€‚ç›¸æ¯”äºŽå…¶ä»–çš„æ–¹æ³•ï¼Œå¯èƒ½è¿˜ä¼šé‡åˆ°ç¨€ç–ç­‰é—®é¢˜ï¼Œè€Œè¿™ä¸ªåªéœ€è¦æ±‚å¯¼å°±å¯ä»¥å¾—åˆ°äº†ï¼Œæ±‚å¯¼çš„å…·ä½“è¿‡ç¨‹æ˜¯éœ€è¦æœ‰å¯¹$f(x)$çš„é¢„å…ˆä¼°è®¡çš„ã€‚ä¾‹å¦‚ï¼Œä¸€å…ƒçº¿æ€§å›žå½’ã€‚ å¦ä¸€ä¸ªä¼˜åŠ¿æ˜¯MSEå¯¹äºŽå¼‚å¸¸å€¼æ˜¯å¾ˆæ•æ„Ÿçš„ã€‚å› ä¸ºæ¬¡æ–¹çš„å…³ç³»ï¼Œä¸€æ—¦ $y-f(x)&gt;1$ ï¼Œé‚£ä¹ˆMSEå¯¹ä¸Žè¿™ç±»å¼‚å¸¸å€¼çš„æƒ©ç½šæ˜¯è¦æ¯”MAEé«˜å¾ˆå¤šçš„ï¼Œè¿™æ ·çš„æ€§è´¨è®©MSEæœ‰é˜²æ­¢è¿‡æ‹Ÿåˆçš„åŠŸæ•ˆã€‚å› ä¸ºè¿™ä¸ªä»£ä»·å‡½æ•°ä¼šè®©$f(x)$å°½å¯èƒ½è´´è¿‘å¼‚å¸¸å€¼ï¼Œè¿™ä¼šç»™å¼‚å¸¸å€¼æ›´å¤§çš„æƒé‡ã€‚ç›´è§‰ä¸Šæ¥è®²ï¼Œåªæœ‰å¹³å‡æ•°å¯ä»¥è®©æ¯ä¸ª $y-f(x)$ éƒ½å°½å¯èƒ½åœ°å°ï¼Œä»Žè€Œè®©MSEæœ€å°ã€‚ ä¸‹å›¾é‡Œï¼Œå·¦è¾¹æ˜¯MAEï¼Œå³è¾¹æ˜¯MSEã€‚ 0-1 Lossè¿™é‡Œéœ€è¦å»ºç«‹ä¸€ä¸ªIndicator Function, $\\hat{y}$ æ˜¯é¢„æµ‹å€¼ã€‚ I(y) = \\begin{cases} 0, y = \\hat{y}\\\\ 1, y\\neq \\hat{y} \\end{cases}æˆ‘ä»¬å°†æŸå¤±å‡½æ•°è®°ä¸º$\\sum_i I(y_i)$ï¼Œè‡ªç„¶ï¼Œèƒ½è¾¾åˆ°æœ€å°æŸå¤±å‡½æ•°çš„ $\\hat{y}$ æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚èƒ½è¾¾åˆ°è¿™ä¸€è¦æ±‚çš„æ˜¯ä¼—æ•°(mode)ã€‚è¿™ä¸€ç›´è§‰æ˜¯ï¼Œæ‰¾åˆ°æœ€æœ‰å¯èƒ½æ˜¯æ­£ç¡®çš„ $y$ å€¼ã€‚ Learning få¯¹äºŽ$f(x)$çš„å…·ä½“å¯»æ‰¾æ–¹æ³•ï¼Œæœ‰ä¸¤ä¸ªæ–¹æ³•ã€‚ Parametric Methodå®ƒéœ€è¦å¯¹æ•°æ®çš„åˆ†å¸ƒæœ‰åŸºæœ¬çš„ç»éªŒåˆ¤æ–­ï¼Œè¿™ä¸€åˆ¤æ–­æ˜¯ç”¨äºŽç»™å®šmodel/hypothesisçš„ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æœ‰Wrong Functional Formè¿™ä¸€ä¸ªé—®é¢˜ã€‚å¯¹äºŽå¤æ‚çš„æ•°æ®å°†è¦é¢å¯¹approximation-generalization tradeoffã€‚ä½†è¿™ä¸€å¥½å¤„æ˜¯ä»–ä»¬çš„ç»æµŽå­¦æ„ä¹‰æ˜¯æœ‰æ®å¯å¾ªçš„ã€‚åœ¨æ ·æœ¬æ¯”è¾ƒå°‘ï¼Œç»´åº¦è¾ƒå¤šçš„æƒ…å†µä¸‹ï¼Œå‚æ•°ä¼°è®¡æ˜¯è¡Œä¹‹æœ‰æ•ˆçš„ã€‚åŽé¢ä¸»è¦ä¼šè®¨è®ºçš„æ˜¯å‚æ•°ä¼°è®¡æ³•ã€‚ Nonparametric Methodè¿™æ˜¯éœ€è¦å¾ˆå¤šæ•°æ®æ”¯æ’‘æ‰èƒ½å¤Ÿä½¿ç”¨çš„ä¸€ç§æ–¹æ³•ã€‚åœ¨MSEä¸‹ï¼Œæ ¹æ®$E(y|x)$ï¼Œéžå‚æ•°ä¼°è®¡éƒ½æ˜¯ä»¥å–å¹³å‡å€¼ä¸ºåŸºç¡€æ¥å»¶ä¼¸çš„ã€‚ å½“æŸå¤„çš„æ•°æ®ç‚¹ä¸å¤Ÿå……è¶³çš„æƒ…å†µä¸‹ï¼Œå°±é‡‡é›†è¿™ä¸€ä½ç½®é™„è¿‘çš„ç‚¹ï¼Œå³Nearest Neighborhood methodã€‚ ä½†æ˜¯è¿™ä¸€æ–¹æ³•ä¸ä¸€å®šéžå¸¸å®žç”¨ã€‚è¿™ä¸€æ–¹æ³•å—åˆ°é«˜çº¬åº¦çš„â€œè¯…å’’â€ï¼šå³ï¼Œæ¯å¤šä¸€ä¸ªç»´åº¦ï¼Œneighborhoodçš„ç©ºé—´ä¼šè¢«åŽ‹æ¦¨åœ°æ›´å°ï¼Œè¿™æ ·neighborhoodé‡Œçš„ç‚¹æ•°é‡å°±ä¼šæ›´å°‘ï¼Œè¿™æ ·å–åˆ°çš„å¹³å‡å€¼ï¼ˆç”šè‡³å¯èƒ½å–ä¸å‡ºå¹³å‡å€¼ï¼‰æ˜¯ä¸å¤Ÿå¯ä¿¡çš„ã€‚ Goodness of Fitæ‹Ÿåˆä¼˜åº¦æ˜¯åœ¨é¢„æµ‹ä¸­çš„ä¸€ä¸ªé‡è¦å±žæ€§ã€‚å®ƒå’Œç³»æ•°çš„æ˜¾è‘—æ€§æ²¡æœ‰å…³ç³»ï¼Œæ˜¯ä¸€ä¸ªè¡¡é‡é¢„æµ‹å‡ºçš„æ¨¡åž‹å’Œç»™å®šçš„æ•°æ®ç‚¹åˆ†å¸ƒé•¿å¾—å¤šåƒçš„ä¸€ä¸ªæŒ‡æ ‡ã€‚åœ¨è®¡é‡ä¸­ï¼Œæˆ‘ä»¬æœ‰è°ƒæ•´Ræ–¹è¿™ä¸€æŒ‡æ ‡ï¼š$\\bar{R}^2 = 1- \\frac{n-1}{n-k-1}\\frac{SSR}{TSS}$. å³æ®‹å·®è¶Šå°ï¼Œæ‹Ÿåˆè¶Šå¥½ã€‚ é€šå¸¸åœ°ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå¦‚æžœå…¨éƒ¨ç”¨äºŽè®­ç»ƒæ¨¡åž‹ï¼Œé‚£ä¹ˆå…¶å®žå¾ˆéš¾æœ‰çŸ¥é“æ¨¡åž‹çš„æ™®é€‚æ€§ã€‚æ‰€ä»¥ä¸€ä¸ªæ•°æ®é›†ä¼šè¢«åˆ’åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚è®­ç»ƒé›†äº§ç”Ÿæ¨¡åž‹$g$ï¼Œå…¶ä¸­çš„è¯¯å·®ä¸ºin-sample errorã€‚è€Œæµ‹è¯•é›†ä¸­ï¼Œä½¿ç”¨$g$æ¥ç®—å‡ºè¯¯å·®ï¼Œä¸ºout-sample errorã€‚æµ‹è¯•é›†ä¸­çš„è¯¯å·®ï¼ˆä»£è¡¨generalizationï¼‰æ˜¯æˆ‘ä»¬é‡ç‚¹è¦å…³æ³¨çš„ã€‚å…¬å¼ä¸º$e_{TE} =\\frac{1}{M}\\sum (y_i-\\hat{f})^2 \\to E[(y-g)^2]$ï¼Œå®žé™…ä¸Šä¹Ÿæ˜¯MSEã€‚ å½“ç„¶ï¼Œå¦‚æžœè¿ç”¨å¤æ‚çš„æ¨¡åž‹ï¼Œé‚£ä¹ˆin-sample erroræ˜¯è¦å‡å°çš„ï¼Œè€Œæ¨¡åž‹çš„ç²¾åº¦è¶Šå¤§è¶Šå¯èƒ½å¯¼è‡´out-sample errorå¼‚å¸¸çš„å¤§ï¼Œä»Žè€Œå¤±åŽ»æ™®é€‚æ€§ã€‚è¿™ä¸»è¦çœ‹$f(x)$çš„å‡†ç¡®åˆ†å¸ƒã€‚è¿™å°±æ˜¯è¿‡æ‹Ÿåˆä¸Žæ¬ æ‹Ÿåˆé—®é¢˜ï¼Œä¹Ÿæ˜¯å³approximation-generalization tradeoffã€‚åœ¨Hoeffdingâ€™s inequalityé‡Œï¼Œæœºå™¨å­¦ä¹ çš„å…³é”®åœ¨äºŽè®©$E_{in} = E_{out}$ï¼Œä¸”$E_{out}=0$ã€‚æ¢è€Œè¨€ä¹‹ï¼Œè¿™å°±è¦æ±‚$E_{in}=0$ï¼Œ$E_{out}\\to E_{in}$ã€‚å‰è€…æ˜¯approximationï¼ŒåŽè€…æ˜¯generalizationã€‚ å¦‚æžœä»Žout-sample erroræ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠä»–åˆ†è§£æˆBiaså’ŒVarianceï¼š \\begin{aligned} E_{out}(g) &= E[(g(x)-f(x))^2]\\\\ &=Var(g-f)+E^2(g-f)\\\\ &=Var(g)+bias^2 \\end{aligned}æ³¨æ„è¿™é‡Œçš„$g$æ˜¯æ ¹æ®è®­ç»ƒé›†å¾—åˆ°çš„ï¼ˆå…ˆè§‚æµ‹ï¼ŒåŽæŒ‘é€‰å‡ºæœ€é€‚åˆçš„å‡½æ•°$g$ï¼‰ï¼Œä»–åŒæ—¶condition on xå’ŒD(è®­ç»ƒé›†)ã€‚ æ‰€ä»¥ï¼Œå¦‚æžœä»Žé¢„æµ‹è¯¯å·®æ¥çœ‹æ‹Ÿåˆå¥½åï¼Œè¿™å°±æ˜¯è‘—åçš„Bias-Variance Tradeoff. Causalityåœ¨Statisticsé‡Œï¼Œæˆ‘ä»¬æ›´å…³æ³¨çš„æ˜¯Relevanceã€‚ç„¶è€Œåœ¨è®¡é‡ä¸­ï¼Œæˆ‘ä»¬æ›´è¦å…³æ³¨Causalityã€‚ Seeing vs. DoingSeeing æ˜¯ç›´æŽ¥è§‚æµ‹åŽ†å²æ•°æ®ï¼Œè€Œdoingæ˜¯physicallyè¿›è¡ŒåŠ¨ä½œï¼Œå¾—åˆ°ç»“æžœã€‚ æˆ‘ä»¬ç”¨æ°”åŽ‹è®¡ä½œä¸ºä¾‹å­ã€‚Seeingï¼šæ°”åŽ‹ä½Žçš„æ—¶å€™æ¯”æ°”åŽ‹é«˜æ—¶å®¹æ˜“ä¸‹é›¨ã€‚Doingï¼šæ‰‹åŠ¨è°ƒæ•´æ°”åŽ‹è®¡ï¼Œä¸è®ºæ€Žä¹ˆè°ƒæ•´æ°”åŽ‹ï¼Œä»Šå¤©ä¸‹é›¨çš„å¯èƒ½æ€§éƒ½æ˜¯ä¸€æ ·çš„ï¼ˆä¸Žæ°”åŽ‹è®¡æ— å…³ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥äº†è§£åˆ°ï¼ŒSeeingå¯ä»¥è§‚æµ‹Relevanceï¼ŒDoingå¯ä»¥çœ‹å‡ºæ˜¯å¦æ‹¥æœ‰Causalityï¼ˆå¦‚æžœè°ƒä½Žæ°”åŽ‹è®¡ä½¿å¾—å®¹æ˜“ä¸‹é›¨ï¼Œé‚£ä¹ˆæœ‰causalityï¼‰ã€‚ å¦ä¸€ä¸ªæ˜¯ABåŒ»é™¢çš„ä¾‹å­ï¼Œä¸¤ä¸ªåŒ»é™¢çš„æ²»æ„ˆçŽ‡ä¸åŒï¼ˆObservationï¼‰ã€‚æœ‰å¯èƒ½é‡ç—‡éƒ½é€å¾€å¥½çš„åŒ»é™¢ï¼Œä½†åè€Œå¯¼è‡´å…¶æ²»æ„ˆçŽ‡ä¸‹é™äº†ã€‚çŽ°åœ¨ä¸€ä¸ªäººç”Ÿç—…ï¼Œé€åŽ»å¥½çš„åŒ»é™¢æ²»æ„ˆçš„å¯èƒ½æ€§é«˜ï¼ˆCausalityï¼‰ã€‚ Just do itåœ¨Statistics learningä¸­ï¼Œæˆ‘ä»¬çŸ¥é“å¾—åˆ°$f(x)$çš„è¿‡ç¨‹ï¼Œ$f(x)$æ˜¯ç ”ç©¶relevanceçš„äº§ç‰©ã€‚å¦‚æžœæˆ‘ä»¬æž„é€ ä¸€ä¸ªæ•°æ®é›†ï¼Œéƒ½ç”±doè¿™ä¸€è¡ŒåŠ¨å¾—åˆ°ï¼Œè¿™æ ·ï¼Œæ•°æ®æœ¬èº«çš„relevanceæ˜¯ç”±causalityè€Œæ¥ã€‚å³ï¼Œè¿™æ—¶å€™æˆ‘ä»¬çš„$f(x)$å°±æ˜¯causalityã€‚æ‰€ä»¥ç ”ç©¶causalityçš„æ–¹æ³•ï¼Œå°±æ˜¯ç›´æŽ¥è¿›è¡Œdo operationï¼Œå°½é‡æž„é€ ä¸€ä¸ªRCTã€‚ Causality DiagramCommon causeå’Œcommon effectæ˜¯ä¸¤ä¸ªå…¸ä¾‹ã€‚ å¯¹äºŽcommon causeï¼Œæˆ‘ä»¬çš„ä¾‹å­æ˜¯ï¼Œå‡è®¾æŠ½çƒŸä¼šå¯¼è‡´å¸¦æ‰“ç«æœº(A)å’Œç™Œç—‡(Y)ï¼Œä¸”åªè€ƒè™‘è¿™ä¸‰ä¸ªå› ç´ ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬è‹¥ç ”ç©¶å¸¦æ‰“ç«æœºé€ æˆç™Œç—‡çš„causalityï¼Œå°±ä¼šå› ä¸ºcommon causeè€Œäº§ç”Ÿä¸€å®šçš„è”ç³»ã€‚å› ä¸ºæˆ‘ä»¬è®©ä¸€ä¸ªäºº$do(A=1)$ï¼Œä»–å¾—ç—…è¿˜ç”±æ˜¯å¦å¸çƒŸå¯¼è‡´ï¼Œå³$E(Y|A)\\neq E(Y|do(A=1))$ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦å›ºå®šæµ‹è¯•äººä¸ºå¸çƒŸæˆ–ä¸å¸çƒŸã€‚å°±å¯ä»¥å¾—åˆ°æ¯”è¾ƒçº¯ç²¹çš„causalityã€‚ \\begin{aligned} E[y|do(x)] &= \\sum E[y|do(x),z]p(z)\\\\ &=\\sum E[y|x,z]p(z) \\end{aligned}è¿™é‡Œï¼Œæˆ‘ä»¬çš„$E$ å°±å¯ä»¥applyä¸€äº›æ¨¡åž‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨statisticsé‡Œä¸€æ ·ã€‚","link":"/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/"},{"title":"Casual Effect Estimation","text":"Causal effect is different from associative relationship. In reality, we are usually interested in how an action matters. For example, it is obvious that a high advertisement payment appeals to consumers, which is associative relationship. However, how to allocate your advertisement money, like focusing on young people, will get the best effect? It requires the causal effect. Shallow men believe in luck or in circumstance. Strong men believe in cause and effect. â€• Ralph Waldo Emerson PrerequisitesStructural Causal ModelLike statistical learning, weâ€™d better use a concise â€œlanguageâ€ to tell the causal effect. And here is the structural causal model (SCM). A SCM can always be drawn within a Directed Acyclic Graph (DAG). One reason is that the relationship doesnâ€™t have feedback effect, with which SCM and DAG are consistent. In Fig1, it shows a simple causal relationship that $X$ affects $Y$, and $X$ is the cause. The causal effect can be written as, Y \\leftarrow f_X(X)In general, $X$ is the treatment binary variable to be 0 or 1. Therefore, the causal effect is, \\tau = Y_{X=1}-Y_{X=0} \\tag{1}CounterfactualHere is a representative question that â€œhow much would I earn if I went to the university?â€. For an individual, he can only choose one option without foreseeing the future situation. Say, our minds cannot jump to the other parallel universe like Doctor Strange to observe his salary. Thus, we call it as counterfactual. It leads a severe problem that we cannot estimate the treatment effect (causal effect) for one specific individual because the equation 1 requires we to analyze some variations in different worlds. Potential Outcomes ModelPotential Outcomes Model combines the treatment, potential outcomes, observable outcome and treatment effect into one equation, which is, Y_i = X_iY_i^1+(1-X_i)Y_i^0 \\tag{2} ,where $X_i$ is the treatment. $Y_i^1$ denotes the outcome after treatment while $Y_i^0$ doesnâ€™t. Therefore, $Y_i$ is the final outcome observed. That is, if a treatment is taken, we will get $Y_i=Y_i^1$. Confounding VariablesSome variables may affect treatment and outcomes simultaneously. One DAG is, We call $Z$ blocks every path $X\\to Y$. So variable $Z$ makes variable $X$ and $Y$ dependent. $X$ denotes â€œgoing to universityâ€, $Y$ is salary and $Z$ is gender. Thus, we cannot observe the true treatment effect ($X\\to Y$) by only observing $X$, $Y$. This kind of variables called confounding variables. Average Treatment Effect No Confounding Variables here. Weâ€™d like to change our mind to estimate the average treatment effect (ATE). \\begin{aligned} \\tau = ATE &= E(Y_i^1-Y_i^0)\\\\ &= E(Y_i^1-Y_i^0|X_i=1)*p(X_i=1)+E(Y_i^1-Y_i^0|X_i=0)*p(X_i=0) \\end{aligned}\\tag{3}Suppose the sample here is created by the God, and individuals are homogeneous. $Y_i^1$ $Y_i^0$ Treatment 1 1 0 0 2 1 0 0 3 1 1 0 4 0 0 0 5 1 1 1 6 1 0 1 7 1 0 1 8 0 0 1 The ATE is, \\begin{aligned} \\tau = ATE &= E(Y_i^1-Y_i^0|X_i=1)*p(X_i=1)+E(Y_i^1-Y_i^0|X_i=0)*p(X_i=0)\\\\ &= \\frac{(1-0)+(1-0)+(1-1)+(0-0)}{4}*\\frac{1}{2} + \\frac{(1-1)+(1-0)+(1-0)+(0-0)}{4}*\\frac{1}{2}\\\\ &= \\frac{1}{2} \\end{aligned}\\tag{4}For this sample, if we only treat four individuals, the $ATE$ will not change because we know the outcomes before and after treatment. ATU &amp; ATT &amp; SDO ATU is the shorthand of average treatment effect of the untreated. ATU = E(Y_i^1-Y_i^0|X_i=0) ATT is the shorthand of average treatment effect of the treated. ATT = E(Y_i^1-Y_i^0|X_i=1) SDO is the shorthand of simple difference in mean outcomes. SDO = E(Y^1_i|X_i=1)-E(Y_i^0|X_i=0)In the sample, we can easily calculate, \\begin{aligned} ATU&=\\frac{3-1}{4}=\\frac{1}{2}\\\\ ATT&=\\frac{3-1}{4}=\\frac{1}{2}\\\\ SDO&=\\frac{3}{4}-\\frac{1}{4}=\\frac{1}{2} \\end{aligned}Since $SDO$ is not affected by counterfactual to be accurately computed, we wonder whether it can be used to estimate $ATE$. More importantly, we can do a simple transposition for ATE based on equation 3, (let $p=p(X_i=1)$) \\begin{aligned} ATE &= ATT*p+ATU*(1-p)\\\\ &= ATT + (ATU-ATT)*(1-p)\\\\ &= SDO + E(Y_i^0|X_i=0)-E(Y_i^0|X_i=1) +(1-p)(ATU-ATT)\\\\ \\Rightarrow SDO&= ATE + \\underbrace{E(Y_i^0|X_i=1)-E(Y_i^0|X_i=0)}_{\\text{Selection Bias}} + \\underbrace{(1-p)(ATT-ATU)}_{\\text{Heterogeneous Bias}} \\end{aligned}\\tag{5}So how the two terms influence the estimation? Selection BiasIf we intentionally assign our treatments to individual 1,3,5,7. The sample shows below, $Y_i^1$ $Y_i^0$ Treatment 1 1 0 1 2 1 0 0 3 1 1 1 4 0 0 0 5 1 1 1 6 1 0 0 7 1 0 1 8 0 0 0 If we calculate $SDO$, SDO = \\frac{1+1+1+1}{4}-\\frac{0+0+0+0}{4}=1Simply regrading it as $ATE$, then a huge effect reflects if a treatment is implemented. However, the true $ATE$ is $\\frac{1}{2}$. The selection bias is, \\text{Selection Bias}=\\frac{0+0+1+1}{4}-\\frac{0+0+0+0}{4}=\\frac{1}{2}What exactly contributes to the Selection Bias? The answer is $Y_i^0$. There is a probability that treated ones and untreated ones make different choices if no treatment assigned. Say, in order to ensure my estimated ATE is not distorted due to sampling bias, I must ensure treatment assignment strategy does not yield a significant difference in the potential outcome given no treatment ($\\color{red}{Y_i^0}$) of treated and untreated individuals. Confounders are generally the main cause of selection bias. Heterogeneous Treatment Effect BiasIf there are two economists, the God surprisingly finds that the treatment has no effect on them, and gives the sample below, $Y_i^1$ $Y_i^0$ Treatment 1 1 0 1 2 1 0 0 3 1 1 0 4ðŸ“ˆ 0 0 1 5 1 1 1 6 1 0 0 7 1 0 0 8ðŸ“ˆ 0 0 1 , where individuals 4, 8 are economists. Calculating the $SDO$, SDO = \\frac{1+1+0+0}{4}-\\frac{0+1+0+0}{4}=\\frac{1}{4}While with the selection bias is, \\text{Selection Bias}=\\frac{1+0+0+0}{4}-\\frac{0+1+0+0}{4}=0The heterogeneous treatment effect bias is, \\begin{aligned} HTE &= (1-\\frac{1}{2})*(ATT-ATU)\\\\ &= \\frac{1}{2}*(\\frac{1}{4}-\\frac{3}{4})=-\\frac{1}{4} \\end{aligned}What exactly causes Heterogeneous Treatment Effect (HTE)? The answer is $Y_i^1$. When there is no sampling bias, $Y_i^1$ is the main factor that influences $ATT$ and $ATU$ because the group of the treated and the untreated may not be the same. In this case, economists, who are not affected by the treatment, are all in the treated. In order to ensure my estimated ATE is not distorted due to sampling bias, I must ensure that my treatment assignment strategy does not yield a significant difference in the potential outcome given treatment ($\\color{red}{Y_i^1}$) of treated and untreated individuals. Randomized Controlled TrialsHow to eliminate those bias? Applying Randomized Controlled Trials is a good idea. Then, treatments are independent with potential outcomes $Y_i^1$ and $Y_i^0$. We will have, E(Y_i^1|X_i=1)=E(Y_i^1|X_i=0)\\\\ E(Y_i^0|X_i=1)=E(Y_i^0|X_i=0) \\tau = ATE = E(Y_i^1-Y_i^0) = E(Y_i^1-Y_i^0|X_i=0/1) = ATT = ATUATE under ConfoundednessIf there is observable confounding variables, how to estimate $ATE$? There are two criteria to figure out the $ATE$. Back-door CriterionBackdoor Criterion is simply controlling confounding variables constant (or nearly equal to the constant). By doing so, we will reach a magic place where treatment and outcome are independent. Remember the Fig3, the variable $Z$ is â€˜invisibleâ€™ in this case. Of course, we call it as conditional independent. An efficient method is matching). As we do above, we estimate conditional ATE (CATE), CATE = E(Y_i^1-Y_i^0|Z) \\tag{6}Then the $ATE$ is, ATE = \\sum CATE_{Z_i}p(Z_i)If there are many confounding variables, controlling all confounding variables are difficult, which is known as curse of dimensionality. Under this circumstance, we can apply propensity score matching, which compresses variables into one score and all we need to do is to control the score. Front-door CriterionAnother efficient way is Front-door Criterion. Based on Fig3, we add $M$ between $X$ and $Y$, $M$ is a mediator, then the causal effect $X\\to Y$ is the combination of $X\\to M$ and $M\\to Y$. Here, we can easily control $X$ to control $M$. As a result, the influence of $Z$ is eliminated. ATE_{X\\to Y}= ATE_{X\\to M}*ATE_{M\\to Y}Front-door criteria is a very useful way because I can even have no idea about $Z$! Causal RegressionBased on Potential Outcomes Model, we make a transposition on equation 2, \\begin{aligned} Y_i &= X_iY_i^1+(1-X_i)Y_i^0\\\\ &= X_i(Y_i^1-Y^0_i)+Y^0_i \\end{aligned}\\tag{7}It reminds a simple regression that contains a binary variable. And the coefficient is the $ATE$. AssumptionsUnconfoundedness X_i â«« (Y_i(0),Y_i(1))|Z_iThis assumption tells that if there are unobserved variables, those variables are not confounders. This assumption aims at eliminating selection bias. Overlap \\forall x \\in \\text{supp}(X), \\quad 0","link":"/2021/06/18/Casual-Regression/"},{"title":"Ridge and Lasso Regression","text":"Ridge and Lasso Regressions are expansions of OLS. The key difference is that they restrict the coefficients in loss function. You may wonder why we should implement constraints and this report tries to explain the reason. Of course, there must be some problems in OLS and letâ€™s have a look. Problems of Ordinary Least SquaresIn this report, I plan to construct a view towards it based on linear algebra. Basically, we have $Ax=b$ for $m$ equations and $n$ features and they are contained into $A$ and $b$. OLS is born to tackle the problem where we tend to give the appropriate solution but we â€˜cannot solve itâ€™. Said another way, what can we do if $A$ is singular ($b$ is out of the column space $C(A)$)? And this situation is quite often when $m\\ne n$. We can show it in the figure 1 where the line is the column space consisting of all the linear combinations of $a$ (basis). However, if we were forced to get the solution, we can only get the closest one. And thatâ€™s what regression typically does. We should select $v_1$ instead of any others, like $v_2$, because $e_1$ is the shortest segment, which puts $v_1$ where it is. Based on the perpendicular condition (projection on $a$), we get, v_1 = ax, \\text{for }v_1\\parallel a\\\\ a^T(b-ax) = 0, \\text{for }a\\perp e_1\\\\ \\Rightarrow a^Tax=a^Tb\\\\ \\Rightarrow x = (a^Ta)^{-1}a^Tb, where $x$ is the coefficient of $a$ and it is the solution. In practice, if you calculate, \\min ||b-xa||^2, then $a^Tax=a^Tb$ is the F.O.C of it. Of course, the matrix operating process can be applied in the higher-dimensional cases. It is the perpendicular property (uncorrelated) that makes the OLS estimator unbiased. However, the crucial part is that we require $a^Ta$ is invertible! Note that $Rank(a^Ta)=Rank(a)$. Therefore, we will easily proceed further, which is $a$ should have a full column rank. When the requirement is not satisfied (a ill-conditioned $A$), there are two probabilities, $m\\ge n$, but linearly dependent vectors (Or highly correlated ones) â€˜decreaseâ€™ the rank. $m&lt;n$. Some vectors must be linearly dependent. Thus, we need Ridge regression and Lasso regression to solve the multi-collinearity problem. And letâ€™s see how. Ridge RegressionBiased EstimatorRidge regression is widely used when most features are important, which is the problem one. Ridge regression solve the method by building a â€˜ridgeâ€™, which is adding $\\lambda I$ ($\\lambda&gt;0$) onto the $A^TA$. Say, if a full rank matrix is plus in, then the matrix $A^TA$ will also be nonsingular. However, one condition is $A$ should be standardized. Now, the solution to $x$ is, x = (A^TA+\\lambda I)^{-1}A^TbLike real numbers, an adding term makes the inverse a smaller multiplier. As a result, $x$ will become smaller. For simplicity, suppose $A$ is an orthogonal matrix. Say, $A^TA=I$. Then, $x_{OLS}=(A^TA)^{-1}A^Tb=A^Tb$, x_{Ridge} = (I+\\lambda I)^{-1}x_{OLS}=\\frac{x_{OLS}}{1+\\lambda} Thus, you can clearly notice that ridge regression is a shrink on OLS estimator but how to imagine it intuitively and what about a common case? After simple transposition in dimension one, we get, a^T(b-ax)=\\lambda x,so the perpendicular condition is not satisfied. As a not surprising result, the answer is not closest any more, which leads it as a biased estimator. We can draw the Fig2, Since that we will have $ax_r+pa=x_{OLS}$ (where $x_r$ is the ridge estimator) in the figure, if we go deeper for the shrinkage details, \\begin{aligned} a^T(b-ax_r) &= \\lambda x_r\\\\ ||a||*||b-ax_r||*\\cos \\theta &=\\lambda x_r\\\\ p*||a||^2 &= \\lambda x_r\\\\ p &= \\frac{\\lambda x_r}{||a||^2}\\\\ \\Rightarrow ax_r+ap &= ax_{OLS}\\\\ \\Rightarrow x_r + \\frac{\\lambda x_r}{||a||^2}&=x_{OLS}\\\\ \\Rightarrow x_r &= \\frac{||a||^2}{||a||^2+\\lambda}x_{OLS} \\end{aligned}Note that the brown dashed segment is $b-ax$ and $x_r$ denotes the ridge estimator. As you can see, orthogonal cases is special one because $a$ is in a unit length. The result shows that it is because the shrinkage that makes $x_r$ biased. If $\\lambda=0$, ridge regression is actually applied the OLS. If $\\lambda\\to\\infty$, $x_r$ will be really close to zero but not real zero, which makes it suit for problem one (maintaining all features). Say, the larger the $\\lambda$ is, the more biased the estimator is. In mathematics, the amount of bias is, ($E(x_{OLS})=\\beta$, $\\beta$ is the true solution) E(x_r-x_{OLS}) = [(A^TA+\\lambda I)^{-1}-(A^TA)^{-1}]A^TbRelationship with SVDSVD DecompositionIn linear algebra, we have an extraordinary magic called SVD (singular value decomposition). In mathematics, A = UDV^T , where $U$ and $V$ are orthogonal matrices. If we apply the magic to $\\hat{y}=Ax_r$, \\begin{aligned} \\hat{y} &= A(A^TA+\\lambda I)^{-1}A^Tb\\\\ &= UDV^T(VD^2V^T+\\lambda VV^T)^{-1} VDU^Tb,\\text{ if orthogonal, } VV^T=I\\\\ &= UDV^T[V(D^2+\\lambda)V^T]^{-1}VDU^Tb\\\\ &= UD(D^2+\\lambda)^{-1}DU^Tb\\\\ &=\\sum_{i=1}u_i\\frac{d_i^2}{d_i^2+\\lambda}u_i^Tb \\end{aligned}, where $D$ is diagonal. Compare with $\\hat{y}=Ax_{OLS}$, \\begin{aligned} \\hat{y}&=A(A^TA)^{-1}A^Tb\\\\ &=UDV^T(VD^2V^T)^{-1}VDU^Tb\\\\ &=UU^Tb\\\\ &=\\sum_{i=1}u_iu_i^Tb \\end{aligned}We can rewrite $\\hat{y} = Ax_{OLS}$ to be $\\hat{y}=Ux^U_{OLS}$, since $U$ is orthogonal. (Notice that $U^TUx=U^Tb\\to x=U^Tb$). We can treat it as if projecting on $U$. So does the $\\hat{y}$ of ridge. \\hat{y} = \\underbrace{(D^2+\\lambda)^{-1}}_{\\text{coefficient}}\\ \\ UD(UD)^Tb = UDx_r^USay, we can treat it as if the input for the projection was $UD$ instead of $A$, and we should multiply a coefficient which is included in $x^U_r$. Actually, we are using a new variable, which is more concentrated, to replace original variables. You might realize that it is the PCA (Principal Component Analysis). Linkage to PCAFor data $X$, we have covariance matrix $C=X^TX/(n-1)$. In eigen-decomposition, we have $C=Q\\Lambda Q^T1$, for $C$ is symmetric. $\\Lambda$ is diagonal of its all eigenvalues with decreasing order, and $Q$ is an orthogonal eigenvector matrix. Eigenvectors are Principal Axes. Projections on the axes are the Principal Components. The $j$-th principal component is given by the $j$-th column of $XQ$. ($XQ=\\lambda Q$) In SVD, $C=VD^2V^T$, where $Q=V$. The covariance matrix is $D$, and Principal Component is $XV=UDV^TV=UD$. Stackexchangestats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca Therefore, OLS and ridge can both be regarded as projecting on the Principal Component Direction $U$ because $D$ is just coefficient. Fig4 may help you to understand it (make regression on the second one). And if we back to the Fig3 or Fig1, we can regard $a$ as the principal component. The length of it is $d_i$ from the $AV=UD$, where $U$ is in a unit length. So it is not a surprise to find that $\\frac{d_i^2}{d_i^2+\\lambda}$ in $\\hat{y}=Ax_r$ is consistent with what we talked at dimension-one illstration. For high dimensions, we can illustrate like this, As you can see, the high dimensional case can be decomposed to be many Fig3, which is one dimension. From this aspect by the thinking of PCA, we should standardize input $A$ because ridge regression is sensitive to it. Variance DecreaseExcept for solving the problem of ill-conditioned $A$, the ridge regression also decreases the variance of estimator $x_r$. Intuitively, estimators are shrink, so the variance of estimators are smaller. Since that we have $x_r = (A^TA+\\lambda I)^{-1}A^Tb$, then the variance should be below, provided $A$ is full-rank, \\begin{aligned} x_r &= (A^TA+\\lambda I)^{-1}A^T(A\\beta+\\epsilon)\\\\ x_r-\\beta &= (A^TA+\\lambda I)^{-1}A^T\\epsilon\\\\ Var(x_r-\\beta) &= (A^TA+\\lambda I)^{-1}A^T\\epsilon \\epsilon^TA[(A^TA+\\lambda I)^{-1}]^T\\\\ Var(x_r)&=\\sigma^2(A^TA+\\lambda I)^{-1}A^TA (A^TA+\\lambda I)^{-1} \\end{aligned} And we know that, \\begin{aligned} x_{OLS}-\\beta&= (A^TA)^{-1}A^T\\epsilon\\\\ Var(x_{OLS}) &= (A^TA)^{-1}A^T\\epsilon \\epsilon^TA(A^TA)^{-1}\\\\ &= \\sigma^2 (A^TA)^{-1} A^TA(A^TA)^{-1} \\end{aligned}As we mentioned above, $(A^TA+\\lambda I)^{-1}$ can be regarded as a smaller multiplier than $(A^TA)^{-1}$, so $Var(x_{OLS})&gt;Var(x_r)$. If a detailed proof is wanted, click here. Letâ€™s back to why we want to apply ridge. In the case that vectors are highly correlated (multicollinearity) or linearly dependent, the extra â€˜ridgeâ€™ $\\lambda I$ can differ them from each other. If not differed, then $x$ for two multicollinear vectors can vary a lot, which increases the variance much. And the MSE difference can be calculated as, MSE_{OLS}-MSE_{r} = Var(x_{OLS})-Var(x_{r}) - E^2(x_r-x_{OLS})Itâ€™s clear that if $E^2&lt;Var(x_{OLS})-Var(x_r)$, which depends on $\\lambda$, the ridge regression has the smaller MSE. We may also wonder the concrete effects on the variance. The covariance matrix is $A^TA/(n-1)$, as we mentioned above, we can rewrite it to be $VD^2V^T$. Thus, $D^2$ is the variance within the principal component direction ($V$-s). And for direction $i$, $Var=\\frac{d_i^2}{n-1}$. Combine the equation $\\sum_{i=1}u_i\\frac{d_i^2}{d_i^2+\\lambda}u_i^Tb$, we can know that if the smaller the variance $d_i^2/(n-1)$ is, the more shrink will be applied by ridge regression. Lasso RegressionLASSO is the shorthand of Least Absolute Shrinkage and Selection Operator inspired by Tibshirani. Lasso regression focuses on the problem two, where there are many useless features. Lasso solves this problem by adding a L1-norm in the OLS form. \\min ||y-x\\beta||^2 \\ \\ \\text{s.t } \\sum_{i=1}||\\beta_i||\\le C\\\\ \\Rightarrow \\min L = ||y-x\\beta||^2+\\lambda||\\beta||Biased EstimatorSubgradientHowever, if we take derivative on $\\beta_j$, we will face a problem that $||\\beta||$ may not have the gradient. Therefore, we introduce the concept, which is subgradient, to substitute gradient when it doesnâ€™t exist. Let $f:R^n\\to R$ be a convex function with domain $R^n$. A vector $v$ is called subgradient at $x_0\\in R^n$ if, f(x)-f(x_0)\\ge v*(x-x_0)All subgradient at $x_0$ forms a set called subdifferential, denoted by $\\partial f(x_0)$. And $x_0$ is the minimum point of a convex function if and only if $0\\in\\partial f(x_0)$. Wikipediaen.wikipedia.org/wiki/Subgradient_method EstimatorNow, we can analyze the best point of $\\beta$. In order to get the explicit solution, we suppose $x$ is orthogonal. If we take differential on $\\beta_i$, there may be two scenarios, Gradient exists and $\\beta_i\\ne 0$. Gradient doesnâ€™t exist so according to the subgradient, $\\beta_i=0\\in\\partial f$. For situation 1, \\begin{aligned} \\frac{\\partial L}{\\partial \\beta_i} = (-2x^Ty+2x^Tx\\beta_i)+\\lambda*\\text{sign}(\\beta_i)&=0\\\\ \\Rightarrow x^Ty-x^Tx\\beta_i&=-\\frac{\\lambda}{2}\\text{sign}(\\beta_i)\\\\ \\beta_i^{OLS}-\\beta_i&=-\\frac{\\lambda}{2}\\text{sign}(\\beta_i)\\\\ \\Rightarrow \\beta_i = \\begin{cases} \\beta^{OLS}_i-\\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i>\\frac{\\lambda}{2}\\\\ \\beta^{OLS}_i + \\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i\\frac{\\lambda}{2}\\\\ \\beta^{OLS}_i + \\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i 0 \\end{aligned}It gives the quadratic form and $(\\beta-\\hat{\\beta})$ is positive definite for $x^Tx$. The function should be drawn like below, If we let $z=(\\beta-\\hat{\\beta})^Tx^Tx(\\beta-\\hat{\\beta})$, where $z$ is any constant, we will get a ellipsoid graph on the $\\beta1.\\beta2$ plane. Either ridge or lasso has a constraint, we draw them on the same plane. From this viewpoint, it tells why Lasso generates sparsity solutions (many $\\beta$-s are 0). This is because L1-norm is not smooth and the $(0,\\beta_2)$ or $(\\beta_1,0)$ are easy to touch the ellipsoid. Someone plug Lasso into the Ridge regression and call it as Elastic Net. It can be written as, \\min L= ||y-x\\beta||^2+\\lambda_1||\\beta||^2+\\lambda_2||\\beta||,\\ \\ \\lambda_1+\\lambda_2=1The elastic net combines all the advantages of ridge and lasso so that you can choose it when you know nothing about the dataset. Of course, you can choose the potential type of regression by adjusting $\\lambda_1$ and $\\lambda_2$. The figure of its constraint is, Thus, it is able to get sparse solution and smooth solution by an implemented weighting mechanism. It is the improvement. Thanks For Reading! ReferenceList of Works MIT Course 18.06, Linear Algebra, Gilbert Strang Jiaming Mao, Data Analysis for Economics, Regularization Upenn, Ridge Regression Stackexchange, Relationship between PCA and SVD Robert Tibshirani, Regression Shrinkage and Selection via the Lasso Wikipedia, Subgradient Ryan Tibshirani, Convex Optimization - Coordinate Descent Pictures Cover: &nbsp;&nbsp; Pixiv by Katann Some Figures: &nbsp;&nbsp; Ridge Regression, Upenn &nbsp;&nbsp; Regularization, Jiaming Mao","link":"/2021/05/07/Report-Ridge-Lasso/"},{"title":"Report - VC Dimension","text":"VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a large VC dimension implies a powerful and complex model. However, it may not always be a good signal. If the VC dimension is far away large, then there will be problems in out-sample error, which may present as the overfitting problem. This report aims at discussing why the VC dimension is required and how it comes into being based on the process of machine learning. Notice that the thinking track of the report is based on the lecture Jiaming Mao presents. Figures are from the slides and the Internet. Letâ€™s talk about the feasibility of machine learning first. Feasibility ConditionsProcess of Machine LearningMachine learning is predicting the value of $y$ based on the value of $x$. And the picture below shows the process of a learning. $f(x)$ is the underlying mechanism, so called target function. And $\\mathcal{H}$ is a set of candidate formula (also called hypothesis set or model), from which, through the algorithm $\\mathcal{A}$ (calculating the loss), we can choose a â€œbestâ€ hypothesis $g$ that approximately equals to $f(x)$. The error emerged within the training data is called in-sample error and the one generated in testing data is called out-sample error. In such a process, weâ€™d like to get $E_{out}=0$ if possible. Nevertheless, we take a compromise and make it as $E_{out}\\approx0$ for there always are some disturbing terms. Now, a problem is that we can only observe $E_{in}$, which is the experience from the training data. Thus, we wonder whether $E_{out}\\approx E_{in}\\approx0$ such that $E_{in}$ can be a measurement. And it remains a problem here. Hoeffdingâ€™s InequalityHere we have an example, which is drawing marbles. The bin contains many many differently-colored marbles, and in this case weâ€™d like to figure out the probability of red marbles in the bin, denoted by $\\mu$. It is impossible for us to count every marbles and what we can do is to draw samples. Then, we denote the probability of red marbles in the samples as $\\nu$. According to the law of large numbers, intuitively, the larger the sample is, the closer the distance between $\\mu$ and $\\nu$. One of the pattern is the Hoeffdingâ€™s inequality, (if the proof is wanted, click here.) Pr(|\\mu-\\nu|>\\epsilon)\\epsilon)\\epsilon)\\cup\\cdots] &\\le Pr[(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon)\\ \\text{or}\\ (|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon)\\cdots]\\\\ &\\le Pr(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon) + Pr(|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon) + \\cdots\\\\ &\\le \\sum_{i=1}^M 2\\exp(-2\\epsilon^2N) = 2M\\exp(-2\\epsilon^2N) \\end{aligned}, $M$ denotes the number of hypothesis in $\\mathcal{H}$. Clearly, if $M$ is large, then the model is complex. And letâ€™s summarize the inequality as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2M\\exp(-2\\epsilon^2N)To sum up, there are two conditions if machine learning is feasible. The Algorithm $\\mathcal{A}$ can find a $g$ such that $E_{in}\\approx0$, and it is consistent with the process â€œtrainâ€. Actually, the more complex the model is, the smaller the $E_{in}$ is. $M$ is finite and a $N$ is sufficiently large to make $E_{out}\\approx E_{in}$, and it is consistent with the process â€œtestâ€. Next, weâ€™d like to focus on the second condition, and mainly talk about the $M$. VC DimensionGenerally, $\\mathcal{H}$ contains infinite $h$. For example, a simple linear model $y=ax+b$ generates infinite $h$, because the parameters are changeable. How do we shrink the upper bound by substitute $M$? Effective Number of HypothesisIn the process that expanding Hoeffdingâ€™s inequality, the inequality below is used, Pr(A_1\\ \\cup\\ A_2\\ \\cup\\ A_3\\ \\cdots) \\le \\sum_{i=1}^M Pr(A_i)If and only if $A_i$s are independent, we choose the equal sign. Actually, there are many intersection part in $\\mathcal{H}$. A simple example is shown in the graph. In the two-dimensional space, we can simply divide $\\mathcal{H}$ into two parts. One judges that $x_1$ is positive, like $h_1$. Another one marks $x_1$ as negative, like $h_2$. Here, the effective number of hypothesis is two. Under a specific sample, the effective number is fixed. Thus, the inequality can be written as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2*\\text{effective}(M)*\\exp(-2\\epsilon^2N)Growth FunctionDichotomyFirstly, weâ€™d like to introduce a binary target function and hypothesis set that contains $h:\\mathcal{X}\\to\\{-1,+1\\}$. And, h(X_1,X_2,\\cdots,X_n) = (h(X_1), h(X_2), \\cdots, h(X_n))is called one dichotomy. Generally speaking, dichotomy represents a result which marks all points in the sample. $\\mathcal{H}(X_1,X_2,\\cdots,X_n)$ denotes the number of all dichotomies generated by hypothesis set $\\mathcal{H}$, given $n$ points. For instance, if three points are in the training set, One dichotomy is $(h_1(X_1), h_1(X_2), \\cdots) = (+1,+1,+1)$. Another dichotomy can be $(h_2(X_1), h_2(X_2), \\cdots) = (-1,+1,+1)$. â€¦ $2^3=8$ is the maximum dichotomies that $\\mathcal{H}(X_1,X_2,\\cdots)$ can value. Say, the number of dichotomies a $\\mathcal{H}$ can generate at most is $2^n$. Shatter Given n points, (and their locations are fixed,) if a hypothesis set $\\mathcal{H}$ can generate exactly $2^n$ dichotomies, we call $\\mathcal{H}$ shatters these points. An example is under two-dimensional dataset and the hypothesis set contains all linear model. There are three points, no matter what points are black, there always be a line that can separate black points from white points. (See the figure below.) As we mentioned above, three points have eight dichotomies. And black and white points are like the $+1,-1$ marks. If the model can tell the marks apart in every scenario, then we can say that the model, or $\\mathcal{H}$, shatters the points. However, if there are four points, then the linear model cannot shatter them. We call the smallest number of points as Break Point. Say, 4 is the break point for the two-dimensional linear regressor. Growth FunctionWe define the growth function as, m_\\mathcal{H}(n)=\\max_{X_1,X_2,\\cdots,X_n \\in \\mathcal{X}} |\\mathcal{H}(X_1,X_2,\\cdots,X_n)| That is, $m_\\mathcal{H}(n)$ is the maximum possible number of dichotomies $\\mathcal{H}$ can generate on a dataset of n points. Obviously, $m_\\mathcal{H}(n)\\le2^n$. The larger the $m_\\mathcal{H}(n)$ is, the more powerful the model is. And $m_\\mathcal{H}(n)$ can tell us whether the dataset of size $s$ can be shattered by $\\mathcal{H}$. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) Now, can we simply exchange $\\text{effective}(M)$ to $m_\\mathcal{H}(n)$ to shrink the upper bound? VC boundNo, because the upper bound for $m_\\mathcal{H}(n)$ is still an explosively increasing function. It is hard to tell whether the result can converge to zero. Sauerâ€™s Lemma solves the problem. If the break point $k$ exists, then $m_\\mathcal{H}(n)$ is a polynomial. According to Sauerâ€™s Lemma, m_\\mathcal{H}(n) \\le \\sum_{i=0}^{k-1} \\begin{pmatrix} N\\\\ i \\end{pmatrix}\\le\\begin{cases} N^{k-1}+1,\\\\ (\\frac{eN}{k-1})^{k-1}, \\text{if } N\\ge k-1 \\end{cases} And the polynomial is much easier to converge and it releases a signal that it is feasible to make the upper bound to be zero! Through some complicated transformations, the inequality can be written as, (the proof is here) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*m_\\mathcal{H}(2N)*\\exp(-\\frac{1}{8}\\epsilon^2N) This inequality is the VC bound and the exponential part has a higher rate of convergence than the polynomial part. Thus, the inequality tells that if there is a break point, when $N$ is sufficiently large, for any hypothesis $h$, $E_{out}\\to E_{in}$ is very possible. Then the machine learning in the space is feasible, which meets the second condition. If the significance level is $\\delta$, and the upper bound should be $\\delta$. Then, \\Rightarrow Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le\\delta\\\\ \\epsilon = \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\\\ E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}ï¼Œ\\text{probability 1} -\\deltaThe bound is called VC generalization bound, which is usually applied in training model because we are required to select a best hypothesis. But, we will not apply it onto the testing. Instead, we use the original Hoeffdingâ€™s Inequality for there is only one hypothesis and the VC bound is much looser. Why is VC bound so loose? The basic Hoeffdingâ€™s inequality used in the proof already has a slack. Using $m_\\mathcal{H}(N)$ to quantify the number of dichotomies on N points, regardless of which $N$ points are in the dataset, gives us a worst-case estimate (because we expanded the inequality to all $h$). Bound $m_\\mathcal{H}(N)$ by a simple polynomial. VC DimensionNow, we can talk about the definition of VC dimension. The Vapnik-Chervonenkis (VC) dimension of $\\mathcal{H}$, denoted $d_{vc}(\\mathcal{H})$ is the size of the largest data set that $\\mathcal{H}$ can shatter. Remember that we have already talked about this problem in part Growth Function. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) And here, $d_{vc}(\\mathcal{H})=3$. If $m_\\mathcal{H}(n)=\\infty$, then $d_{vc}(\\mathcal{H})=\\infty$. In general, $d_{vc}(\\mathcal{H})=k-1$, where $k$ is the break point. If the VC dimension is large, the model $\\mathcal{H}$ is complex because a large sample can be shattered by it. Thus, VC dimension tells the ability of learning for the model. The definition of VC dimension has nothing to do with the locations of points in dataset, which leads to a problem. For example, again, under two-dimensional plane, there are three points. However, this time, they are on the same line (See the graph). A linear model cannot shatter the dataset now. Therefore, VC dimension tells you the largest dataset $\\mathcal{H}$ can shatter, but not every same-sized dataset can be shattered. The VC bound can also be written as, (the first inequality in Sauerâ€™s Lemma is usually used.) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*((2N)^{d_{vc}(\\mathcal{H})}+1)*\\exp(-\\frac{1}{8}\\epsilon^2N)Approximation-Generalization TradeoffRecall the two conditions if machine learning is feasible, and now, the two conditions should be, The algorithm $\\mathcal{A}$ can find the $g$ that makes $E_{in}\\approx0$. Actually, the larger the VC dimension is (the more complex the model is), the smaller the $E_{in}$ is. The VC dimension are finite to make the polynomial part easy to converge and $N$ should be sufficiently large so that $E_{out}\\approx E_{in}$. Also, recall the relationship that we talked in VC bound, E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}ï¼Œ\\text{with probability 1} -\\deltaLet $R = \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}} $. If $N$ holds constant, then if $d_{vc}(\\mathcal{H})$ increases, then $R$ will increases, which means the bound will become loose. As a result, $E_{out}$ may get away from $E_{in}$. Nevertheless, $\\mathcal{H}$ will be more complicated. Say, $E_{in}$ decreases, even reaches the scenario that $E_{in}=0$. We call it approximation-generalization tradeoff. When one condition gets too tight, another one will be hard to meet. Therefore, weâ€™d better find a point where we can balance these two well. And based on the inequality, it is how the VC dimension functions to make machine learning feasible. SummaryStarting from the two feasibility conditions, this report mainly discusses how the VC dimension comes into being and how should we understand VC dimension. VC dimension is important because it regulates the two conditions for machine to learn, which is the approximation-generalization tradeoff. Actually, mathematics is just a tool for us to shrink the bound and VC dimension is the aggregation of them, which can help us study further in machine learning. References Jiaming Mao, Data Analysis for Economics Liubai01â€™s blog, æœºå™¨å­¦ä¹ æŽ¨å¯¼åˆé›†01 ECE 901 Lecture 19 Yaser &amp; Malik &amp; Hsuan-tien Lin, Learn from Data [M], California Institute of Technology Hsuan-tien Lin, Machine Learning Foundation Picture References&nbsp;&nbsp; Vector Landscape Vectors by Vecteezy &nbsp;&nbsp; Data Analysis by Jiaming Mao &nbsp;&nbsp; Machine Learning by Hsuan-tien Lin","link":"/2021/03/29/VC%20Dimension/"},{"title":"Report - Regression Splines","text":"Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline belongs to non-parametric family and it is the feature that endows the regression high flexibility. This report aims to tell you a story about Why we need Regression Splines, How we make the regression â€œSplineâ€. However, before the story, itâ€™s inevitable to talk about some basic regression. Letâ€™s briefly review them. Intuitively, we simulate dots based on $f(x)=\\sin(x), x\\in[-\\pi,2\\pi]$ with the Gaussian-distributed error. For the graph, a code will be attached. If you â€˜d like to see the code, please just unfold it. Click to Unfold12345678910111213141516import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinenp.random.seed(15)## Generate 150 dotsx = np.linspace(-np.pi,2*np.pi,150)y0 = np.sin(x)e = np.random.normal(0,1,150)y = y0+e # Add error term# plotfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x,y,color='lightgrey')ax.plot(x,y0,color='r') # oringinal traceplt.show() From Linear to PolynomialLinear RegressionAt the beginning, we always apply the linear regression. Say, the hypothesis set $\\mathcal{H}=\\{h(x)\\} $ consisting of linear functions, assuming there are $N$ samples and $p$ dimensions per sample. h(x) = x'\\beta,\\ \\ \\text{where } x=\\begin{pmatrix} 1\\\\ x_1\\\\ \\vdots\\\\ x_p \\end{pmatrix} ,\\ \\beta=\\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p \\end{pmatrix}And the $\\beta$ determines whether the function is the best one. Of course, L2 loss is always chosen to measure best. The calculation of $E_{in}$ can be written below, \\begin{aligned} E_{in} &= \\frac{1}{N}\\sum_{i=1}^N (x_i'\\beta-y_i)^2\\\\ &= \\frac{1}{N}\\left|\\begin{array}{c} x_1'\\beta - y_1 \\\\ \\vdots \\\\ x_N'\\beta - y_N \\end{array}\\right|^2\\\\ &=\\frac{1}{N}\\left|X\\beta-\\vec{y}\\right|^2,\\ \\text{where } X = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 1 & x_{N1} & \\cdots & x_{Np} \\end{bmatrix} \\end{aligned}To minimize it, the gradient should be zero, \\frac{\\partial}{\\partial \\beta}\\frac{1}{N}(\\beta'X'X\\beta-2\\beta'X'\\vec{y}+\\vec{y}'\\vec{y})=\\vec{0}\\\\ \\frac{2}{N}(X'X\\beta-X'\\vec{y})=\\vec{0}\\\\ \\beta = (X'X)^{-1}X'\\vec{y}Be sure to remember the equation that generates best $\\beta$, it will be the foundation for the following regressions. And the estimator is BLUE (best linear unbiased estimator), proved by Gaussian-Markov Theorem. Click to Unfold1234567891011121314X = np.ones((150,2))X[:,1] = x # Matrix X is generatedY = np.zeros((150,1))Y[:,0] = y # Matrix Y is generatedpseudo = np.dot(np.linalg.inv(np.dot(X.T,X)), X.T) # Calculate the pseudo-inversebeta = np.dot(pseudo,Y)y_lin = beta[0]+beta[1]*xfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x, y, color='lightgrey')ax.plot(x, y0, color='r', label='Original') # oringinal traceax.plot(x, y_lin, color='y', label='Linear')ax.legend()plt.show() PolynomialBasis ExpansionActually, there are many underlying functions that are not typically linear. Then, the question is how to deal with the non-linear terms. Our solution is simply to regard each non-linear term as a whole â€œdegree-oneâ€ term. For example, the model weâ€™d like to apply is $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2$. The only non-linear part is $x^2$, and we use $\\phi(x)=x^2$ to replace it. The result is $h(x)=\\beta_0+\\beta_1x+\\beta_2\\phi(x)$. Practically, we this method can be applied on linear terms to build a kind of consistency. Say, $\\phi$ can be any function including dummies and constants. $\\phi(x)$ is called basis function. A linear basis function model is defined as, y=\\sum_{i=1}^M \\beta_i\\phi_iJiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Linear basis function model ensures that non-linear regression can be calculated by OLS because it shows the consistent linear form. And, $\\hat{\\beta}=(\\Phiâ€™\\Phi)^{-1}\\Phiâ€™Y$, where $\\Phi=(\\phi_1,\\cdots,\\phi_M)$â€™. Polynomial RegressionWe assume that the dots we simulated is underlying a cubic polynomial function, which is, specifically, $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3 x^3$. Follow the procedure, we can get a regression curve shown in the first graph. However, when we increases the polynomial degree, the regression curve tends to be more and more abnormal. Actually, itâ€™s a problem called overfitting, where $E_{out}$ is large while $E_{in}$ is very small. (In order to roughly compare the regression models, $R^2$ is also attached on figures.) In statistics, overfitting is â€œthe production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliablyâ€. LeineberD.J (2007). Stupid data miner tricks Click to Unfold1234567891011121314151617181920212223242526272829303132333435363738394041def polynomial_regression(x,y,degree=3): #define a polynomial regression function Y = np.zeros((len(y),1)) Y[:,0] = y # Matrix Y is generated polynomial_X = np.ones((len(x),degree+1)) for i in range(degree): polynomial_X[:,i+1] = np.power(x,i+1) polynomial_pseudo = np.dot(np.linalg.inv(np.dot(polynomial_X.T,polynomial_X)), polynomial_X.T) # Calculate the pseudo-inverse polynomial_beta = np.dot(polynomial_pseudo,Y) return np.dot(polynomial_X, polynomial_beta)def R2(y,predict_y): # R-square is attached my = np.mean(y) denominator = np.sum((y-my)**2) numerator = np.sum((predict_y-my)**2) return numerator/denominatord3y = polynomial_regression(x,y,3)d5y = polynomial_regression(x,y,5)d7y = polynomial_regression(x,y,7)d9y = polynomial_regression(x,y,9)fig, axes = plt.subplots(2,2,figsize=(10,10))for i in [0,1]: for j in [0,1]: axes[i][j].scatter(x,y,color='lightgrey') axes[i][j].plot(x,y0,color='r',label='Original trace') axes[i][j].grid()axes[0][0].plot(x, d3y, color='g', label='3 Degree Polynomial')axes[0][0].legend()axes[0][0].annotate('$R^2={:.5f}$'.format(R2(y,d3y)), xy=(-2,2))axes[0][1].plot(x, d5y, color='g', label='5 Degree Polynomial')axes[0][1].legend()axes[0][1].annotate('$R^2={:.5f}$'.format(R2(y,d5y)), xy=(-2,2))axes[1][0].plot(x, d7y, color='g', label='7 Degree Polynomial')axes[1][0].legend()axes[1][0].annotate('$R^2={:.5f}$'.format(R2(y,d7y)), xy=(-2,2))axes[1][1].plot(x, d9y, color='g', label='9 Degree Polynomial')axes[1][1].legend()axes[1][1].annotate('$R^2={:.5f}$'.format(R2(y,d9y)), xy=(-2,2))plt.show() A high-degree polynomial does fit samples well, but will the underlying mechanism really perform in that way? What if it meets some extrapolating data? Actually, we are always trapped in a thought that all samples follow a global mechanism. The global regression with high orders induces Rungeâ€™s Phenomenon (overfitting problem). How about break the domain into pieces? Say, piecewise function. Regression SplinePiecewise RegressionPiecewise regression breaks the input space into distinct regions and fit a different relationship in each region. Jiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Piecewise PolynomialsAs mentioned above, a piecewise polynomial function is obtained by dividing the domain of ð‘‹ into contiguous intervals and representing the function by a separate degree-d polynomial in each interval. In mathematics, if there are $n$ knots, h(x) = \\begin{cases} \\beta_0\\phi_0(x), & x","link":"/2021/03/31/Report-Regression-Spline/"}],"tags":[{"name":"å¾®è§‚è®¡é‡","slug":"å¾®è§‚è®¡é‡","link":"/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"}],"categories":[]}