{"pages":[],"posts":[{"title":"Challenge: Ordered Logit","text":"Ordered Logit is in the family of ordinal regression. It’s widely used when facing ordered categorical outcomes. For example, in bond ratings, there are $AAA, AA, A, B$ etc. Of course, we can simply apply multinomial logistics model to fit it. However, we may lose the information of the ranking in the dependent variables. As a result, ordered logit (or ordered probit) is the ‘evolution’ version. Mechanism of Ordered LogitReminder of Multinomial LogisticsIf we consider the binary logit model, the function should be, Pr(y=1|x) = \\frac{\\exp(x'\\beta)}{1+\\exp(x'\\beta)}, which implies that, \\ln \\frac{Pr(y=1|x)}{Pr(y=0|x)} = x'\\betaExpanding the kind of thoughts to the multinomial part, there is the soft-max function for n choices, \\sigma_j(x'\\beta_j) = \\frac{\\exp(x'\\beta_j)}{\\sum_{i=1}^n\\exp(x'\\beta_i)}The core thinking is that we can pick any two categories to do a binary logistic regression. Even if each pair has different underlying classification criteria (which generates different $\\beta$), we can still merge these regressions together to form the soft-max function. Therefore, the fraction of probabilities is, \\ln \\frac{Pr(y=j|x)}{Pr(y=k|x)} = x'(\\beta_j-\\beta_k)And we use the MLE (Maximum Likelihood Estimation) method to estimate it. Parallel Regression AssumptionNevertheless, if ordered logit is desired, then we cannot belike soft-max function to have many different $\\beta$-s. Simply speaking, the dependent variables of ordinal regression always contain the rank information. Rank always implies comparisons. For example, is it reasonable or fair for a bond credit rating firm to have different grading criteria onto the very bond that is not evaluated yet? The answer is certainly no. We call the assumption as Parallel Assumption or Proportional Odds Assumption. TestsIt is a strong assumption so that we unify the coefficients among categories. Practically, we have many methods to test whether the dataset can pass the assumption. Brant test, Wald test and LR test are the most popular ways to verify it. If not all variables pass the examinations, then a significant result will be thrown out. Generally, Wald test is looser than Brant test. Here, Wald test will be introduced briefly. Wald test is a kind of likelihood ratio test but lowers much more computation burdens than that within LR. WikiPediaen.wikipedia.org/wiki/Wald_test Wald test performs one-by-one in finding the bad variables in ordered logit model within Stata. H_0: \\hat{\\beta} = \\beta_0\\\\ H_1: \\hat{\\beta} \\ne \\beta_0\\\\ W = \\frac{(\\hat{\\beta}-\\beta_0)^2}{Var(\\hat{\\beta})}\\to \\chi^2, where $\\hat{\\beta}$ was found as the maximizing argument of the unconstrained likelihood function and it is compared with a hypothesized one $\\beta_0$. We hope that null hypothesis passes so an insignificant p-value is desired. Details of both tests can tell what variables violate the assumption. It’s also called Partial Proportional Odds and we will talk about the situation later. Distinguish CategoriesThresholdsYou must have questions about how to distinguish categories under the circumstance of $\\beta$-s being the same. Notice that it indicates that every category shares the same Logit function. And a idea comes out. One variable increases, it would result in a shift toward either end of the spectrum of ordinal responses. Said another way, the probability of responding toward either end of the spectrum would increase as the predictor variables change in a given direction. Therefore, we assume some thresholds $\\alpha$ aiming at dividing the rank for categories. For example, if an index model for a single latent variable $y_i^*=x_i’\\beta+u_i$ and $u_i$ follows the Gumbel, then, y_i = j \\hspace{1em} \\text{if} \\hspace{1em} \\alpha_{j-1}\\le y_i^*\\le \\alpha_jSuppose it is a health survey with three ($J$) levels, then there should be two ($J-1$) intercepts. What should be kept in mind is that we don’t know the thresholds beforehand and thresholds are unlikely to be averagely divided. A good example is that good health might have little differences with normal health, but normal health can be much better than unhealthy. Thus, the probability of individual $i$ choosing $j$ (just like the normal health case in the graph) is, Pr(y_i=j) = Pr(\\alpha_{j-1}-x_i'\\beta\\le u_i\\le \\alpha_j-x'_i\\beta) = F(\\alpha_j-x'_i\\beta)-F(\\alpha_{j-1}-x_i'\\beta) , where $F = \\frac{\\exp(z)}{1+\\exp(z)}$, for $u_i$ follows standard $\\text{Gumbel}$ distribution. (Of course, if $u_i$ is distributed under the standard normal, then the $F$ can be the $cdf$ of it. And that is the ordered probit.) Particularly, $Pr(y_i=1)=F(\\alpha_1-x’\\beta)$ and $Pr(y_i=J) = 1-F(\\alpha_{J-1}-x’\\beta)$. And the fraction of probability is, \\ln \\frac{Pr(y_i=j|x_i)}{Pr(y_i=k|x_i)} = \\alpha_j-\\alpha_{j-1} - (\\alpha_k - \\alpha_{k-1})As you can see that the equation is $\\beta$-free because it satisfies the parallel assumption. EstimationOnce we have the probability function, we can estimate it by MLE. The log-likelihood function is, \\ln \\mathcal{L}(\\beta, \\vec{\\alpha}|x_i,y_i) = \\sum_{j=1}^J [y_i=j]\\ln [F(\\alpha_{j-1}-x'\\beta)-F(\\alpha_j-x'\\beta)], where $[y_i=j]$ is an Iverson bracket. If we continue using the core thinking in the multinomial logistics here, we can separate each $\\alpha$ independently and the binary (smaller than $j$ or larger than $j$ are the two groups) logistics function is easy to get, \\ln\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)} = \\alpha_j -x'\\beta\\\\ \\Rightarrow Pr(y_i\\le j) = \\frac{\\exp(\\alpha_j-x'\\beta)}{1+\\exp(\\alpha_j-x'\\beta)}And here, we can operate it as if we were doing a binary logistics regression but for many times. InterpretationCoefficientsOne way to interpret the coefficients is via a proportional odds ratio. Say, you can interpret it mainly on the latent variable then interpret it in the exponential way. For example, one ordered logit regression outcome is, \\ln (\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)}) = \\alpha_j + 0.2*\\text{Gender}, where $\\text{Gender}=\\begin{cases}1, \\text{if Male}\\ 0, \\text{if Female} \\end{cases}$. The odds ratio for gender is $\\exp(-\\beta)=\\exp(0.2)=1.22$. Say, the latent variable will increase $0.2$ so will the likelihood of a higher rank. Then the odds of rating for a man is 1.22 times higher than that for a woman. Marginal EffectsAn advantage of the ordered logit regression is that we can estimate the marginal effect of each regressor on the probability that individual $i$ chooses alternative $j$ ($p_{ij}$), \\frac{\\partial p_{ij}}{\\partial x_r} = \\beta_r[f(\\alpha_{j-1}-x'\\beta)-f(\\alpha_j-x'\\beta)]=\\beta_r\\Delta f, where $f$ is the $pdf$ of logit function and $x_r$ is the $r^{th}$ regressor. And the marginal effects across all alternatives for individual $i$ sums up to zero. This interesting property inherits from logit-like functions (like logit function and soft-max function). \\begin{aligned} \\sum_{j=1}^J \\frac{\\partial p_{ij}}{\\partial x_r} &= \\beta_r [f(\\alpha_1-x'\\beta)+f(\\alpha_2-x'\\beta)-f(\\alpha_1-x'\\beta)+\\cdots\\\\ &+f(\\alpha_{J-1}-x'\\beta)-f(\\alpha_{J-2}-x'\\beta)-f(\\alpha_{J-1}-x'\\beta)]\\\\ &=\\beta_r*0=0 \\end{aligned}The interpretation of marginal effect is that each unit increase in independent variable increase/decrease the probability of selecting alternative $j$ by the amount expressed as a percent (for probability is percentage). We can see from the graph that $\\Delta f$ is actually the difference of boundary, if selecting ‘Normal Health’. Generalized Ordered LogitRemember that we remained a problem called Parital Proprtional Odds? And we’d like to talk about it now. If some $\\beta$-s differ across different categories, what should we do? Here comes the Generalized Ordered Logit, which looses the strong parallel assumption while somehow remaining the rank information in regression formula. In Parallel Assumption, we talked about some test like Brant test, Wald test and you can immediately find the variables that violate the assumption. Then, we can reconstruct our formula based on ordered logit model, \\ln\\frac{Pr(y_i\\le j)}{1-Pr(y_i\\le j)} = \\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j}\\\\ \\Rightarrow Pr(y_i\\le j) = \\frac{\\exp(\\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j})}{1 + \\exp(\\alpha_j - x'_{\\text{not},i}\\beta_{\\text{not}} - x'_{\\text{vio},i}\\beta_{\\text{vio},j})}, where the undermark $\\text{not}$ represents not violate. The $\\alpha$-s here inherits from the idea. It is still the threshold to represent the ranks of categories even if every binary logistics doesn’t share the same $pdf$. What’s more, the estimation or interpretations are still the same. We can say that ordered logit is a particular of generalized ordered logit. NOTE! In some software packages, Stata as the example, if you use the command gologit to do a generalized ordered logit, there will be a totally different result with ologit even if no $\\beta$ varies. This is because gologit is a user-written package, and the model is, \\ln\\frac{Pr(y_i>j)}{1-Pr(y_i>j)} = \\alpha_j + x'_{\\text{not},i}\\beta_{\\text{not}} + x'_{\\text{vio},i}\\beta_{\\text{vio},j}, where the greater-than sign matters and change all the results. So plz be very careful! Empirical ExampleAbout the DatasetLong and Freese (2006) present data from the 1977/1989 General Social Survey. Respondents are asked to evaluate the following statement: “A working mother can establish just as warm and secure a relationship with her child as a mother who does not work.” Responses were coded as 1 = Strongly Disagree (1SD), 2 = Disagree (2D), 3 = Agree (3A), and 4 = Strongly Agree (4SA). Practically, the number should impart the rank information. Explanatory variables are yr89 (survey year; 0 = 1977, 1 = 1989), male (0 = female, 1 = male), white (0 = nonwhite, 1 = white), age (measured in years), ed (years of education), and prst (occupational prestige scale). In this example, we will use 3A as the base outcome. Assumption TestAs we mentioned in tests, there are many can figure out whether the dataset satisfies the parallel assumption. 1oparallel And the result is shown below, Test Method Chi2 df P&gt;Chi2 Wolfe Gould 48.91 12 0.000 Brant 49.18 12 0.000 score 48.42 12 0.000 likelihood ratio 49.2 12 0.000 Wald 48.55 12 0.000 For the reason why the degree of freedom is 12, it is because we have 6 variables and each of them provides a $\\chi^2$. A significant test statistic provides evidence that the parallel regression assumption has been violated. 1auto lrf store(gologit2) It’s an optional command following after the gologit2 regression and we show it here in advance. In the detail, you will find that only yr89, male violate the assumption. variables p-value white 0.7136 ed 0.1589 prst 0.2046 age 0.0743 yr89 0.00093 male 0.00002 Ordered LogitHowever, in order to perform the ordered logit, we kick those two bad variables out. 1ologit warm white age ed prst Here is the result, Variables Coef. white -.4478496* age -.0186559* ed .078407* prst .0053391 /cut1 -2.056959 /cut2 -.2866278 /cut3 1.525569 Generalized Ordered LogitIf we’d like to take all variables into account, we should use the generalized ordered logit since it doesn’t require a $\\beta$ that across all categories. And before we operate it in Stata, we must consider the true model that gologit give, which we talked at the note. 1gologit2 warm yr89 male white age ed prst, auto lrf store(gologit2) Note that gologit2 is used when some but not all variables are parallel in $\\beta$. And the result is, Variables 1SD 2D 3A yr89 0.984* 0.534* 0.326* male -0.333* -0.693* -1.098* white -0.383* -0.383* -0.383* age -0.022* -0.022* -0.022* ed 0.067* 0.067* 0.067* prst 0.006* 0.006* 0.006* _cons 2.122* 0.602* -1.048* The model parameterization dictates the interpretation of the odds ratio. Take the odds ratio for gender as example, using Stat’s estimates, the odds ratio for gender is $exp(-\\beta_{male})$, which is 0.7168, 0.5, 0.3353 when calculating $P(Y&gt;1)$, $P(Y&gt;2)$, $P(Y&gt;3)$. The impact of gender shows heterogeneity. The odds of rating score higher than 1, 2, 3 is respectively 0.7168, 0.5, 0.3353 times lower for man than it is for women. We calculate the accuracy and find it is $43.26\\%$. Comparison to Multinomial LogisticsWe also implement a multinomial logistics regression on the dataset for you to show the difference. 1mlogit warm yr89 male white age ed prst The accuracy here is $42.4\\%$, which is lower than the generalized ordered logit. Thus, generalized ordered logit is more efficient when facing a category with rank. ReferencesList of Works Richard Williams, Generalized Ordered Logit Models Agresti, Alan. “Categorical Data Analysis.” New York: Wiley, 2002. Jiaming Mao, Data Analysis for Economics Cornell Consulting Unit #91 Econometrics Academy, Ordered Logit and Ordered Probit Picture References&nbsp;&nbsp; Pixiv by Alcxome","link":"/2021/05/04/Challenge-Ordered-Logit/"},{"title":"Data Analysis for Economics Intro","text":"终于有一个相对轻松的学期了，就在这里记录一下茅家铭老师的微观计量课程。这是本学期观感最好的一门课程了，它在泰式英语，连环逼问的门门课程中太过鹤立鸡群，老师也是WISE一顶一的男神，教学资源也是相当好的。（教学网站：点击这里。）于是想在这里用通俗一些的话来过一下老师上课的一些内容，然后推导一些公式，也防止后面摸鱼摸得什么都忘了。这也是我的第一篇博文，是有很多不足之处的，写下来也很生疏，是我脑子里意识流的Intro part，详细还是参见茅神的课程吧。 正文1&lt;span style=&quot;text-align:center&quot;&gt;Machine learning $\\to$ Statistics $\\to$ Econometrics&lt;/span&gt; 从左到右是一个从结果推断(Prediction)向因果推断(Causality)加强的链。基于统计学知识，前者主要观测特征，后者想找出作用机理。而机器学习模型的准确度实际上是大大超过传统的一些计量方法的，比如Panel Data Regression和Logit Model。但是这里的缺陷在于很难表示出经济学的一些insights，类如随机森林等算法，很难有一个经济学标准的interpretation，而这些却是计量中很需要的。这个问题的出现主要是因为Relevance不能说明Causality，两者的终极目标也因此分开。 Statistics$x$ 和 $y$ 是两个变量，寻找这两个变量的关系应该是寻找他们的联合概率分布 $p(x,y)$。而实际上我们更多需要的是从 $x$ 得到 $y$ 的过程，即建立一个方程 $f(x)$ 来测定 $y$。 损失函数选择范数选择是由 $f(x)$ 引出的问题，是一个如何衡量”best“的问题。因为当得到预测方程时，我们总有$e = y-f(x)$, 误差 $e$ 越大是对预测模型越不好的。几种损失函数所得到的$f(x)$是完全不同的。 L1与MAEL1范数是$|x_1|+|x_2|+\\cdots+|x_n|$, 这里的损失函数是MAE，即：$\\frac{1}{n}\\sum |e_i|$，MAE是L1正则化的代表 (Lasso回归就是利用了L1的正则化)。最小化MAE产生的将是中位数。具体可以参见知乎-子元的回答。 MAE求解的下降速度统一。不同与MSE，会对误差小的项有大的包容性，MAE一视同仁。对于一些噪声异常值，MAE没有很大的反应，这是它相较于MSE优秀的地方。 L2与MSEL2 范数是 $\\sqrt{(x_1^2+\\cdots+x_n^2)}$ ,这里用于估算损失的函数为MSE，即：$\\frac{1}{n}\\sum_{i}^ne_i^2\\to E[(y-f(x))^2]$ 。L2范数和MSE的最小化过程实际上是一样的，MSE是L2正则化的一个代表 (Ridge回归就是用了L2的正则化)。如果选择这一种方法，得到 $f(x)$ 的过程就是最小化MSE的过程。这一个过程即是对目标函数$f(x)$求导的过程，最后的结果即是$f(x)$ 。要注意的是，这里 $y_i$ 是给定 $x$ 之后的数据点，因此$f(x)=f$,是一个定值。 \\partial \\sum (y_i-f)^2 = \\sum(y_i-f)=0\\\\ \\to f=\\frac{1}{n}\\sum{y_i}=E(y|x)用MSE的一大优点就是好算。相比于其他的方法，可能还会遇到稀疏等问题，而这个只需要求导就可以得到了，求导的具体过程是需要有对$f(x)$的预先估计的。例如，一元线性回归。 另一个优势是MSE对于异常值是很敏感的。因为次方的关系，一旦 $y-f(x)&gt;1$ ，那么MSE对与这类异常值的惩罚是要比MAE高很多的，这样的性质让MSE有防止过拟合的功效。因为这个代价函数会让$f(x)$尽可能贴近异常值，这会给异常值更大的权重。直觉上来讲，只有平均数可以让每个 $y-f(x)$ 都尽可能地小，从而让MSE最小。 下图里，左边是MAE，右边是MSE。 0-1 Loss这里需要建立一个Indicator Function, $\\hat{y}$ 是预测值。 I(y) = \\begin{cases} 0, y = \\hat{y}\\\\ 1, y\\neq \\hat{y} \\end{cases}我们将损失函数记为$\\sum_i I(y_i)$，自然，能达到最小损失函数的 $\\hat{y}$ 是我们想要的。能达到这一要求的是众数(mode)。这一直觉是，找到最有可能是正确的 $y$ 值。 Learning f对于$f(x)$的具体寻找方法，有两个方法。 Parametric Method它需要对数据的分布有基本的经验判断，这一判断是用于给定model/hypothesis的。这可能会导致有Wrong Functional Form这一个问题。对于复杂的数据将要面对approximation-generalization tradeoff。但这一好处是他们的经济学意义是有据可循的。在样本比较少，维度较多的情况下，参数估计是行之有效的。后面主要会讨论的是参数估计法。 Nonparametric Method这是需要很多数据支撑才能够使用的一种方法。在MSE下，根据$E(y|x)$，非参数估计都是以取平均值为基础来延伸的。 当某处的数据点不够充足的情况下，就采集这一位置附近的点，即Nearest Neighborhood method。 但是这一方法不一定非常实用。这一方法受到高纬度的“诅咒”：即，每多一个维度，neighborhood的空间会被压榨地更小，这样neighborhood里的点数量就会更少，这样取到的平均值（甚至可能取不出平均值）是不够可信的。 Goodness of Fit拟合优度是在预测中的一个重要属性。它和系数的显著性没有关系，是一个衡量预测出的模型和给定的数据点分布长得多像的一个指标。在计量中，我们有调整R方这一指标：$\\bar{R}^2 = 1- \\frac{n-1}{n-k-1}\\frac{SSR}{TSS}$. 即残差越小，拟合越好。 通常地，因为只有一个数据集，如果全部用于训练模型，那么其实很难有知道模型的普适性。所以一个数据集会被划分成训练集和测试集。训练集产生模型$g$，其中的误差为in-sample error。而测试集中，使用$g$来算出误差，为out-sample error。测试集中的误差（代表generalization）是我们重点要关注的。公式为$e_{TE} =\\frac{1}{M}\\sum (y_i-\\hat{f})^2 \\to E[(y-g)^2]$，实际上也是MSE。 当然，如果运用复杂的模型，那么in-sample error是要减小的，而模型的精度越大越可能导致out-sample error异常的大，从而失去普适性。这主要看$f(x)$的准确分布。这就是过拟合与欠拟合问题，也是即approximation-generalization tradeoff。在Hoeffding’s inequality里，机器学习的关键在于让$E_{in} = E_{out}$，且$E_{out}=0$。换而言之，这就要求$E_{in}=0$，$E_{out}\\to E_{in}$。前者是approximation，后者是generalization。 如果从out-sample error来看，我们可以把他分解成Bias和Variance： \\begin{aligned} E_{out}(g) &= E[(g(x)-f(x))^2]\\\\ &=Var(g-f)+E^2(g-f)\\\\ &=Var(g)+bias^2 \\end{aligned}注意这里的$g$是根据训练集得到的（先观测，后挑选出最适合的函数$g$），他同时condition on x和D(训练集)。 所以，如果从预测误差来看拟合好坏，这就是著名的Bias-Variance Tradeoff. Causality在Statistics里，我们更关注的是Relevance。然而在计量中，我们更要关注Causality。 Seeing vs. DoingSeeing 是直接观测历史数据，而doing是physically进行动作，得到结果。 我们用气压计作为例子。Seeing：气压低的时候比气压高时容易下雨。Doing：手动调整气压计，不论怎么调整气压，今天下雨的可能性都是一样的（与气压计无关）。我们可以了解到，Seeing可以观测Relevance，Doing可以看出是否拥有Causality（如果调低气压计使得容易下雨，那么有causality）。 另一个是AB医院的例子，两个医院的治愈率不同（Observation）。有可能重症都送往好的医院，但反而导致其治愈率下降了。现在一个人生病，送去好的医院治愈的可能性高（Causality）。 Just do it在Statistics learning中，我们知道得到$f(x)$的过程，$f(x)$是研究relevance的产物。如果我们构造一个数据集，都由do这一行动得到，这样，数据本身的relevance是由causality而来。即，这时候我们的$f(x)$就是causality。所以研究causality的方法，就是直接进行do operation，尽量构造一个RCT。 Causality DiagramCommon cause和common effect是两个典例。 对于common cause，我们的例子是，假设抽烟会导致带打火机(A)和癌症(Y)，且只考虑这三个因素。那么，我们若研究带打火机造成癌症的causality，就会因为common cause而产生一定的联系。因为我们让一个人$do(A=1)$，他得病还由是否吸烟导致，即$E(Y|A)\\neq E(Y|do(A=1))$。此时，我们只需要固定测试人为吸烟或不吸烟。就可以得到比较纯粹的causality。 \\begin{aligned} E[y|do(x)] &= \\sum E[y|do(x),z]p(z)\\\\ &=\\sum E[y|x,z]p(z) \\end{aligned}这里，我们的$E$ 就可以apply一些模型，就像我们在statistics里一样。","link":"/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/"},{"title":"Report - VC Dimension","text":"VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a large VC dimension implies a powerful and complex model. However, it may not always be a good signal. If the VC dimension is far away large, then there will be problems in out-sample error, which may present as the overfitting problem. This report aims at discussing why the VC dimension is required and how it comes into being based on the process of machine learning. Notice that the thinking track of the report is based on the lecture Jiaming Mao presents. Figures are from the slides and the Internet. Let’s talk about the feasibility of machine learning first. Feasibility ConditionsProcess of Machine LearningMachine learning is predicting the value of $y$ based on the value of $x$. And the picture below shows the process of a learning. $f(x)$ is the underlying mechanism, so called target function. And $\\mathcal{H}$ is a set of candidate formula (also called hypothesis set or model), from which, through the algorithm $\\mathcal{A}$ (calculating the loss), we can choose a “best” hypothesis $g$ that approximately equals to $f(x)$. The error emerged within the training data is called in-sample error and the one generated in testing data is called out-sample error. In such a process, we’d like to get $E_{out}=0$ if possible. Nevertheless, we take a compromise and make it as $E_{out}\\approx0$ for there always are some disturbing terms. Now, a problem is that we can only observe $E_{in}$, which is the experience from the training data. Thus, we wonder whether $E_{out}\\approx E_{in}\\approx0$ such that $E_{in}$ can be a measurement. And it remains a problem here. Hoeffding’s InequalityHere we have an example, which is drawing marbles. The bin contains many many differently-colored marbles, and in this case we’d like to figure out the probability of red marbles in the bin, denoted by $\\mu$. It is impossible for us to count every marbles and what we can do is to draw samples. Then, we denote the probability of red marbles in the samples as $\\nu$. According to the law of large numbers, intuitively, the larger the sample is, the closer the distance between $\\mu$ and $\\nu$. One of the pattern is the Hoeffding’s inequality, (if the proof is wanted, click here.) Pr(|\\mu-\\nu|>\\epsilon)\\epsilon)\\epsilon)\\cup\\cdots] &\\le Pr[(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon)\\ \\text{or}\\ (|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon)\\cdots]\\\\ &\\le Pr(|E_{out}(h_1)-E_{in}(h_1)|>\\epsilon) + Pr(|E_{out}(h_2)-E_{in}(h_2)|>\\epsilon) + \\cdots\\\\ &\\le \\sum_{i=1}^M 2\\exp(-2\\epsilon^2N) = 2M\\exp(-2\\epsilon^2N) \\end{aligned}, $M$ denotes the number of hypothesis in $\\mathcal{H}$. Clearly, if $M$ is large, then the model is complex. And let’s summarize the inequality as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2M\\exp(-2\\epsilon^2N)To sum up, there are two conditions if machine learning is feasible. The Algorithm $\\mathcal{A}$ can find a $g$ such that $E_{in}\\approx0$, and it is consistent with the process “train”. Actually, the more complex the model is, the smaller the $E_{in}$ is. $M$ is finite and a $N$ is sufficiently large to make $E_{out}\\approx E_{in}$, and it is consistent with the process “test”. Next, we’d like to focus on the second condition, and mainly talk about the $M$. VC DimensionGenerally, $\\mathcal{H}$ contains infinite $h$. For example, a simple linear model $y=ax+b$ generates infinite $h$, because the parameters are changeable. How do we shrink the upper bound by substitute $M$? Effective Number of HypothesisIn the process that expanding Hoeffding’s inequality, the inequality below is used, Pr(A_1\\ \\cup\\ A_2\\ \\cup\\ A_3\\ \\cdots) \\le \\sum_{i=1}^M Pr(A_i)If and only if $A_i$s are independent, we choose the equal sign. Actually, there are many intersection part in $\\mathcal{H}$. A simple example is shown in the graph. In the two-dimensional space, we can simply divide $\\mathcal{H}$ into two parts. One judges that $x_1$ is positive, like $h_1$. Another one marks $x_1$ as negative, like $h_2$. Here, the effective number of hypothesis is two. Under a specific sample, the effective number is fixed. Thus, the inequality can be written as, \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le2*\\text{effective}(M)*\\exp(-2\\epsilon^2N)Growth FunctionDichotomyFirstly, we’d like to introduce a binary target function and hypothesis set that contains $h:\\mathcal{X}\\to\\{-1,+1\\}$. And, h(X_1,X_2,\\cdots,X_n) = (h(X_1), h(X_2), \\cdots, h(X_n))is called one dichotomy. Generally speaking, dichotomy represents a result which marks all points in the sample. $\\mathcal{H}(X_1,X_2,\\cdots,X_n)$ denotes the number of all dichotomies generated by hypothesis set $\\mathcal{H}$, given $n$ points. For instance, if three points are in the training set, One dichotomy is $(h_1(X_1), h_1(X_2), \\cdots) = (+1,+1,+1)$. Another dichotomy can be $(h_2(X_1), h_2(X_2), \\cdots) = (-1,+1,+1)$. … $2^3=8$ is the maximum dichotomies that $\\mathcal{H}(X_1,X_2,\\cdots)$ can value. Say, the number of dichotomies a $\\mathcal{H}$ can generate at most is $2^n$. Shatter Given n points, (and their locations are fixed,) if a hypothesis set $\\mathcal{H}$ can generate exactly $2^n$ dichotomies, we call $\\mathcal{H}$ shatters these points. An example is under two-dimensional dataset and the hypothesis set contains all linear model. There are three points, no matter what points are black, there always be a line that can separate black points from white points. (See the figure below.) As we mentioned above, three points have eight dichotomies. And black and white points are like the $+1,-1$ marks. If the model can tell the marks apart in every scenario, then we can say that the model, or $\\mathcal{H}$, shatters the points. However, if there are four points, then the linear model cannot shatter them. We call the smallest number of points as Break Point. Say, 4 is the break point for the two-dimensional linear regressor. Growth FunctionWe define the growth function as, m_\\mathcal{H}(n)=\\max_{X_1,X_2,\\cdots,X_n \\in \\mathcal{X}} |\\mathcal{H}(X_1,X_2,\\cdots,X_n)| That is, $m_\\mathcal{H}(n)$ is the maximum possible number of dichotomies $\\mathcal{H}$ can generate on a dataset of n points. Obviously, $m_\\mathcal{H}(n)\\le2^n$. The larger the $m_\\mathcal{H}(n)$ is, the more powerful the model is. And $m_\\mathcal{H}(n)$ can tell us whether the dataset of size $s$ can be shattered by $\\mathcal{H}$. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) Now, can we simply exchange $\\text{effective}(M)$ to $m_\\mathcal{H}(n)$ to shrink the upper bound? VC boundNo, because the upper bound for $m_\\mathcal{H}(n)$ is still an explosively increasing function. It is hard to tell whether the result can converge to zero. Sauer’s Lemma solves the problem. If the break point $k$ exists, then $m_\\mathcal{H}(n)$ is a polynomial. According to Sauer’s Lemma, m_\\mathcal{H}(n) \\le \\sum_{i=0}^{k-1} \\begin{pmatrix} N\\\\ i \\end{pmatrix}\\le\\begin{cases} N^{k-1}+1,\\\\ (\\frac{eN}{k-1})^{k-1}, \\text{if } N\\ge k-1 \\end{cases} And the polynomial is much easier to converge and it releases a signal that it is feasible to make the upper bound to be zero! Through some complicated transformations, the inequality can be written as, (the proof is here) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*m_\\mathcal{H}(2N)*\\exp(-\\frac{1}{8}\\epsilon^2N) This inequality is the VC bound and the exponential part has a higher rate of convergence than the polynomial part. Thus, the inequality tells that if there is a break point, when $N$ is sufficiently large, for any hypothesis $h$, $E_{out}\\to E_{in}$ is very possible. Then the machine learning in the space is feasible, which meets the second condition. If the significance level is $\\delta$, and the upper bound should be $\\delta$. Then, \\Rightarrow Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le\\delta\\\\ \\epsilon = \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\\\ E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4m_\\mathcal{H}(2N)}{\\delta}}，\\text{probability 1} -\\deltaThe bound is called VC generalization bound, which is usually applied in training model because we are required to select a best hypothesis. But, we will not apply it onto the testing. Instead, we use the original Hoeffding’s Inequality for there is only one hypothesis and the VC bound is much looser. Why is VC bound so loose? The basic Hoeffding’s inequality used in the proof already has a slack. Using $m_\\mathcal{H}(N)$ to quantify the number of dichotomies on N points, regardless of which $N$ points are in the dataset, gives us a worst-case estimate (because we expanded the inequality to all $h$). Bound $m_\\mathcal{H}(N)$ by a simple polynomial. VC DimensionNow, we can talk about the definition of VC dimension. The Vapnik-Chervonenkis (VC) dimension of $\\mathcal{H}$, denoted $d_{vc}(\\mathcal{H})$ is the size of the largest data set that $\\mathcal{H}$ can shatter. Remember that we have already talked about this problem in part Growth Function. If $m_\\mathcal{H}(n)=10\\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) And here, $d_{vc}(\\mathcal{H})=3$. If $m_\\mathcal{H}(n)=\\infty$, then $d_{vc}(\\mathcal{H})=\\infty$. In general, $d_{vc}(\\mathcal{H})=k-1$, where $k$ is the break point. If the VC dimension is large, the model $\\mathcal{H}$ is complex because a large sample can be shattered by it. Thus, VC dimension tells the ability of learning for the model. The definition of VC dimension has nothing to do with the locations of points in dataset, which leads to a problem. For example, again, under two-dimensional plane, there are three points. However, this time, they are on the same line (See the graph). A linear model cannot shatter the dataset now. Therefore, VC dimension tells you the largest dataset $\\mathcal{H}$ can shatter, but not every same-sized dataset can be shattered. The VC bound can also be written as, (the first inequality in Sauer’s Lemma is usually used.) \\forall h\\in \\mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\\epsilon)\\le4*((2N)^{d_{vc}(\\mathcal{H})}+1)*\\exp(-\\frac{1}{8}\\epsilon^2N)Approximation-Generalization TradeoffRecall the two conditions if machine learning is feasible, and now, the two conditions should be, The algorithm $\\mathcal{A}$ can find the $g$ that makes $E_{in}\\approx0$. Actually, the larger the VC dimension is (the more complex the model is), the smaller the $E_{in}$ is. The VC dimension are finite to make the polynomial part easy to converge and $N$ should be sufficiently large so that $E_{out}\\approx E_{in}$. Also, recall the relationship that we talked in VC bound, E_{in}(h)-\\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}\\le E_{out}(h)\\le E_{in}(h) + \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}}，\\text{with probability 1} -\\deltaLet $R = \\sqrt{\\frac{8}{N}\\ln\\frac{4((2N)^{d_{vc}(\\mathcal{H})}+1)}{\\delta}} $. If $N$ holds constant, then if $d_{vc}(\\mathcal{H})$ increases, then $R$ will increases, which means the bound will become loose. As a result, $E_{out}$ may get away from $E_{in}$. Nevertheless, $\\mathcal{H}$ will be more complicated. Say, $E_{in}$ decreases, even reaches the scenario that $E_{in}=0$. We call it approximation-generalization tradeoff. When one condition gets too tight, another one will be hard to meet. Therefore, we’d better find a point where we can balance these two well. And based on the inequality, it is how the VC dimension functions to make machine learning feasible. SummaryStarting from the two feasibility conditions, this report mainly discusses how the VC dimension comes into being and how should we understand VC dimension. VC dimension is important because it regulates the two conditions for machine to learn, which is the approximation-generalization tradeoff. Actually, mathematics is just a tool for us to shrink the bound and VC dimension is the aggregation of them, which can help us study further in machine learning. References Jiaming Mao, Data Analysis for Economics Liubai01’s blog, 机器学习推导合集01 ECE 901 Lecture 19 Yaser &amp; Malik &amp; Hsuan-tien Lin, Learn from Data [M], California Institute of Technology Hsuan-tien Lin, Machine Learning Foundation Picture References&nbsp;&nbsp; Vector Landscape Vectors by Vecteezy &nbsp;&nbsp; Data Analysis by Jiaming Mao &nbsp;&nbsp; Machine Learning by Hsuan-tien Lin","link":"/2021/03/29/VC%20Dimension/"},{"title":"Report - Regression Splines","text":"Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline belongs to non-parametric family and it is the feature that endows the regression high flexibility. This report aims to tell you a story about Why we need Regression Splines, How we make the regression “Spline”. However, before the story, it’s inevitable to talk about some basic regression. Let’s briefly review them. Intuitively, we simulate dots based on $f(x)=\\sin(x), x\\in[-\\pi,2\\pi]$ with the Gaussian-distributed error. For the graph, a code will be attached. If you ‘d like to see the code, please just unfold it. Click to Unfold12345678910111213141516import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinenp.random.seed(15)## Generate 150 dotsx = np.linspace(-np.pi,2*np.pi,150)y0 = np.sin(x)e = np.random.normal(0,1,150)y = y0+e # Add error term# plotfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x,y,color='lightgrey')ax.plot(x,y0,color='r') # oringinal traceplt.show() From Linear to PolynomialLinear RegressionAt the beginning, we always apply the linear regression. Say, the hypothesis set $\\mathcal{H}=\\{h(x)\\} $ consisting of linear functions, assuming there are $N$ samples and $p$ dimensions per sample. h(x) = x'\\beta,\\ \\ \\text{where } x=\\begin{pmatrix} 1\\\\ x_1\\\\ \\vdots\\\\ x_p \\end{pmatrix} ,\\ \\beta=\\begin{pmatrix} \\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p \\end{pmatrix}And the $\\beta$ determines whether the function is the best one. Of course, L2 loss is always chosen to measure best. The calculation of $E_{in}$ can be written below, \\begin{aligned} E_{in} &= \\frac{1}{N}\\sum_{i=1}^N (x_i'\\beta-y_i)^2\\\\ &= \\frac{1}{N}\\left|\\begin{array}{c} x_1'\\beta - y_1 \\\\ \\vdots \\\\ x_N'\\beta - y_N \\end{array}\\right|^2\\\\ &=\\frac{1}{N}\\left|X\\beta-\\vec{y}\\right|^2,\\ \\text{where } X = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1p}\\\\ \\vdots & \\vdots & \\ddots & \\vdots\\\\ 1 & x_{N1} & \\cdots & x_{Np} \\end{bmatrix} \\end{aligned}To minimize it, the gradient should be zero, \\frac{\\partial}{\\partial \\beta}\\frac{1}{N}(\\beta'X'X\\beta-2\\beta'X'\\vec{y}+\\vec{y}'\\vec{y})=\\vec{0}\\\\ \\frac{2}{N}(X'X\\beta-X'\\vec{y})=\\vec{0}\\\\ \\beta = (X'X)^{-1}X'\\vec{y}Be sure to remember the equation that generates best $\\beta$, it will be the foundation for the following regressions. And the estimator is BLUE (best linear unbiased estimator), proved by Gaussian-Markov Theorem. Click to Unfold1234567891011121314X = np.ones((150,2))X[:,1] = x # Matrix X is generatedY = np.zeros((150,1))Y[:,0] = y # Matrix Y is generatedpseudo = np.dot(np.linalg.inv(np.dot(X.T,X)), X.T) # Calculate the pseudo-inversebeta = np.dot(pseudo,Y)y_lin = beta[0]+beta[1]*xfig, ax = plt.subplots(figsize=(5,5))plt.grid(ls='--')ax.scatter(x, y, color='lightgrey')ax.plot(x, y0, color='r', label='Original') # oringinal traceax.plot(x, y_lin, color='y', label='Linear')ax.legend()plt.show() PolynomialBasis ExpansionActually, there are many underlying functions that are not typically linear. Then, the question is how to deal with the non-linear terms. Our solution is simply to regard each non-linear term as a whole “degree-one” term. For example, the model we’d like to apply is $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2$. The only non-linear part is $x^2$, and we use $\\phi(x)=x^2$ to replace it. The result is $h(x)=\\beta_0+\\beta_1x+\\beta_2\\phi(x)$. Practically, we this method can be applied on linear terms to build a kind of consistency. Say, $\\phi$ can be any function including dummies and constants. $\\phi(x)$ is called basis function. A linear basis function model is defined as, y=\\sum_{i=1}^M \\beta_i\\phi_iJiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Linear basis function model ensures that non-linear regression can be calculated by OLS because it shows the consistent linear form. And, $\\hat{\\beta}=(\\Phi’\\Phi)^{-1}\\Phi’Y$, where $\\Phi=(\\phi_1,\\cdots,\\phi_M)$’. Polynomial RegressionWe assume that the dots we simulated is underlying a cubic polynomial function, which is, specifically, $h(x)=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3 x^3$. Follow the procedure, we can get a regression curve shown in the first graph. However, when we increases the polynomial degree, the regression curve tends to be more and more abnormal. Actually, it’s a problem called overfitting, where $E_{out}$ is large while $E_{in}$ is very small. (In order to roughly compare the regression models, $R^2$ is also attached on figures.) In statistics, overfitting is “the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably”. LeineberD.J (2007). Stupid data miner tricks Click to Unfold1234567891011121314151617181920212223242526272829303132333435363738394041def polynomial_regression(x,y,degree=3): #define a polynomial regression function Y = np.zeros((len(y),1)) Y[:,0] = y # Matrix Y is generated polynomial_X = np.ones((len(x),degree+1)) for i in range(degree): polynomial_X[:,i+1] = np.power(x,i+1) polynomial_pseudo = np.dot(np.linalg.inv(np.dot(polynomial_X.T,polynomial_X)), polynomial_X.T) # Calculate the pseudo-inverse polynomial_beta = np.dot(polynomial_pseudo,Y) return np.dot(polynomial_X, polynomial_beta)def R2(y,predict_y): # R-square is attached my = np.mean(y) denominator = np.sum((y-my)**2) numerator = np.sum((predict_y-my)**2) return numerator/denominatord3y = polynomial_regression(x,y,3)d5y = polynomial_regression(x,y,5)d7y = polynomial_regression(x,y,7)d9y = polynomial_regression(x,y,9)fig, axes = plt.subplots(2,2,figsize=(10,10))for i in [0,1]: for j in [0,1]: axes[i][j].scatter(x,y,color='lightgrey') axes[i][j].plot(x,y0,color='r',label='Original trace') axes[i][j].grid()axes[0][0].plot(x, d3y, color='g', label='3 Degree Polynomial')axes[0][0].legend()axes[0][0].annotate('$R^2={:.5f}$'.format(R2(y,d3y)), xy=(-2,2))axes[0][1].plot(x, d5y, color='g', label='5 Degree Polynomial')axes[0][1].legend()axes[0][1].annotate('$R^2={:.5f}$'.format(R2(y,d5y)), xy=(-2,2))axes[1][0].plot(x, d7y, color='g', label='7 Degree Polynomial')axes[1][0].legend()axes[1][0].annotate('$R^2={:.5f}$'.format(R2(y,d7y)), xy=(-2,2))axes[1][1].plot(x, d9y, color='g', label='9 Degree Polynomial')axes[1][1].legend()axes[1][1].annotate('$R^2={:.5f}$'.format(R2(y,d9y)), xy=(-2,2))plt.show() A high-degree polynomial does fit samples well, but will the underlying mechanism really perform in that way? What if it meets some extrapolating data? Actually, we are always trapped in a thought that all samples follow a global mechanism. The global regression with high orders induces Runge’s Phenomenon (overfitting problem). How about break the domain into pieces? Say, piecewise function. Regression SplinePiecewise RegressionPiecewise regression breaks the input space into distinct regions and fit a different relationship in each region. Jiaming Maogithub.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf Piecewise PolynomialsAs mentioned above, a piecewise polynomial function is obtained by dividing the domain of 𝑋 into contiguous intervals and representing the function by a separate degree-d polynomial in each interval. In mathematics, if there are $n$ knots, h(x) = \\begin{cases} \\beta_0\\phi_0(x), & x","link":"/2021/03/31/Report-Regression-Spline/"},{"title":"Ridge and Lasso Regression","text":"Ridge and Lasso Regressions are expansions of OLS. The key difference is that they restrict the coefficients in loss function. You may wonder why we should implement constraints and this report tries to explain the reason. Of course, there must be some problems in OLS and let’s have a look. Problems of Ordinary Least SquaresIn this report, I plan to construct a view towards it based on linear algebra. Basically, we have $Ax=b$ for $m$ equations and $n$ features and they are contained into $A$ and $b$. OLS is born to tackle the problem where we tend to give the appropriate solution but we ‘cannot solve it’. Said another way, what can we do if $A$ is singular ($b$ is out of the column space $C(A)$)? And this situation is quite often when $m\\ne n$. We can show it in the figure 1 where the line is the column space consisting of all the linear combinations of $a$ (basis). However, if we were forced to get the solution, we can only get the closest one. And that’s what regression typically does. We should select $v_1$ instead of any others, like $v_2$, because $e_1$ is the shortest segment, which puts $v_1$ where it is. Based on the perpendicular condition (projection on $a$), we get, v_1 = ax, \\text{for }v_1\\parallel a\\\\ a^T(b-ax) = 0, \\text{for }a\\perp e_1\\\\ \\Rightarrow a^Tax=a^Tb\\\\ \\Rightarrow x = (a^Ta)^{-1}a^Tb, where $x$ is the coefficient of $a$ and it is the solution. In practice, if you calculate, \\min ||b-xa||^2, then $a^Tax=a^Tb$ is the F.O.C of it. Of course, the matrix operating process can be applied in the higher-dimensional cases. It is the perpendicular property (uncorrelated) that makes the OLS estimator unbiased. However, the crucial part is that we require $a^Ta$ is invertible! Note that $Rank(a^Ta)=Rank(a)$. Therefore, we will easily proceed further, which is $a$ should have a full column rank. When the requirement is not satisfied (a ill-conditioned $A$), there are two probabilities, $m\\ge n$, but linearly dependent vectors (Or highly correlated ones) ‘decrease’ the rank. $m&lt;n$. Some vectors must be linearly dependent. Thus, we need Ridge regression and Lasso regression to solve the multi-collinearity problem. And let’s see how. Ridge RegressionBiased EstimatorRidge regression is widely used when most features are important, which is the problem one. Ridge regression solve the method by building a ‘ridge’, which is adding $\\lambda I$ ($\\lambda&gt;0$) onto the $A^TA$. Say, if a full rank matrix is plus in, then the matrix $A^TA$ will also be nonsingular. However, one condition is $A$ should be standardized. Now, the solution to $x$ is, x = (A^TA+\\lambda I)^{-1}A^TbLike real numbers, an adding term makes the inverse a smaller multiplier. As a result, $x$ will become smaller. For simplicity, suppose $A$ is an orthogonal matrix. Say, $A^TA=I$. Then, $x_{OLS}=(A^TA)^{-1}A^Tb=A^Tb$, x_{Ridge} = (I+\\lambda I)^{-1}x_{OLS}=\\frac{x_{OLS}}{1+\\lambda} Thus, you can clearly notice that ridge regression is a shrink on OLS estimator but how to imagine it intuitively and what about a common case? After simple transposition in dimension one, we get, a^T(b-ax)=\\lambda x,so the perpendicular condition is not satisfied. As a not surprising result, the answer is not closest any more, which leads it as a biased estimator. We can draw the Fig2, Since that we will have $ax_r+pa=x_{OLS}$ (where $x_r$ is the ridge estimator) in the figure, if we go deeper for the shrinkage details, \\begin{aligned} a^T(b-ax_r) &= \\lambda x_r\\\\ ||a||*||b-ax_r||*\\cos \\theta &=\\lambda x_r\\\\ p*||a||^2 &= \\lambda x_r\\\\ p &= \\frac{\\lambda x_r}{||a||^2}\\\\ \\Rightarrow ax_r+ap &= ax_{OLS}\\\\ \\Rightarrow x_r + \\frac{\\lambda x_r}{||a||^2}&=x_{OLS}\\\\ \\Rightarrow x_r &= \\frac{||a||^2}{||a||^2+\\lambda}x_{OLS} \\end{aligned}Note that the brown dashed segment is $b-ax$ and $x_r$ denotes the ridge estimator. As you can see, orthogonal cases is special one because $a$ is in a unit length. The result shows that it is because the shrinkage that makes $x_r$ biased. If $\\lambda=0$, ridge regression is actually applied the OLS. If $\\lambda\\to\\infty$, $x_r$ will be really close to zero but not real zero, which makes it suit for problem one (maintaining all features). Say, the larger the $\\lambda$ is, the more biased the estimator is. In mathematics, the amount of bias is, ($E(x_{OLS})=\\beta$, $\\beta$ is the true solution) E(x_r-x_{OLS}) = [(A^TA+\\lambda I)^{-1}-(A^TA)^{-1}]A^TbRelationship with SVDSVD DecompositionIn linear algebra, we have an extraordinary magic called SVD (singular value decomposition). In mathematics, A = UDV^T , where $U$ and $V$ are orthogonal matrices. If we apply the magic to $\\hat{y}=Ax_r$, \\begin{aligned} \\hat{y} &= A(A^TA+\\lambda I)^{-1}A^Tb\\\\ &= UDV^T(VD^2V^T+\\lambda VV^T)^{-1} VDU^Tb,\\text{ if orthogonal, } VV^T=I\\\\ &= UDV^T[V(D^2+\\lambda)V^T]^{-1}VDU^Tb\\\\ &= UD(D^2+\\lambda)^{-1}DU^Tb\\\\ &=\\sum_{i=1}u_i\\frac{d_i^2}{d_i^2+\\lambda}u_i^Tb \\end{aligned}, where $D$ is diagonal. Compare with $\\hat{y}=Ax_{OLS}$, \\begin{aligned} \\hat{y}&=A(A^TA)^{-1}A^Tb\\\\ &=UDV^T(VD^2V^T)^{-1}VDU^Tb\\\\ &=UU^Tb\\\\ &=\\sum_{i=1}u_iu_i^Tb \\end{aligned}We can rewrite $\\hat{y} = Ax_{OLS}$ to be $\\hat{y}=Ux^U_{OLS}$, since $U$ is orthogonal. (Notice that $U^TUx=U^Tb\\to x=U^Tb$). We can treat it as if projecting on $U$. So does the $\\hat{y}$ of ridge. \\hat{y} = \\underbrace{(D^2+\\lambda)^{-1}}_{\\text{coefficient}}\\ \\ UD(UD)^Tb = UDx_r^USay, we can treat it as if the input for the projection was $UD$ instead of $A$, and we should multiply a coefficient which is included in $x^U_r$. Actually, we are using a new variable, which is more concentrated, to replace original variables. You might realize that it is the PCA (Principal Component Analysis). Linkage to PCAFor data $X$, we have covariance matrix $C=X^TX/(n-1)$. In eigen-decomposition, we have $C=Q\\Lambda Q^T1$, for $C$ is symmetric. $\\Lambda$ is diagonal of its all eigenvalues with decreasing order, and $Q$ is an orthogonal eigenvector matrix. Eigenvectors are Principal Axes. Projections on the axes are the Principal Components. The $j$-th principal component is given by the $j$-th column of $XQ$. ($XQ=\\lambda Q$) In SVD, $C=VD^2V^T$, where $Q=V$. The covariance matrix is $D$, and Principal Component is $XV=UDV^TV=UD$. Stackexchangestats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca Therefore, OLS and ridge can both be regarded as projecting on the Principal Component Direction $U$ because $D$ is just coefficient. Fig4 may help you to understand it (make regression on the second one). And if we back to the Fig3 or Fig1, we can regard $a$ as the principal component. The length of it is $d_i$ from the $AV=UD$, where $U$ is in a unit length. So it is not a surprise to find that $\\frac{d_i^2}{d_i^2+\\lambda}$ in $\\hat{y}=Ax_r$ is consistent with what we talked at dimension-one illstration. For high dimensions, we can illustrate like this, As you can see, the high dimensional case can be decomposed to be many Fig3, which is one dimension. From this aspect by the thinking of PCA, we should standardize input $A$ because ridge regression is sensitive to it. Variance DecreaseExcept for solving the problem of ill-conditioned $A$, the ridge regression also decreases the variance of estimator $x_r$. Intuitively, estimators are shrink, so the variance of estimators are smaller. Since that we have $x_r = (A^TA+\\lambda I)^{-1}A^Tb$, then the variance should be below, provided $A$ is full-rank, \\begin{aligned} x_r &= (A^TA+\\lambda I)^{-1}A^T(A\\beta+\\epsilon)\\\\ x_r-\\beta &= (A^TA+\\lambda I)^{-1}A^T\\epsilon\\\\ Var(x_r-\\beta) &= (A^TA+\\lambda I)^{-1}A^T\\epsilon \\epsilon^TA[(A^TA+\\lambda I)^{-1}]^T\\\\ Var(x_r)&=\\sigma^2(A^TA+\\lambda I)^{-1}A^TA (A^TA+\\lambda I)^{-1} \\end{aligned} And we know that, \\begin{aligned} x_{OLS}-\\beta&= (A^TA)^{-1}A^T\\epsilon\\\\ Var(x_{OLS}) &= (A^TA)^{-1}A^T\\epsilon \\epsilon^TA(A^TA)^{-1}\\\\ &= \\sigma^2 (A^TA)^{-1} A^TA(A^TA)^{-1} \\end{aligned}As we mentioned above, $(A^TA+\\lambda I)^{-1}$ can be regarded as a smaller multiplier than $(A^TA)^{-1}$, so $Var(x_{OLS})&gt;Var(x_r)$. If a detailed proof is wanted, click here. Let’s back to why we want to apply ridge. In the case that vectors are highly correlated (multicollinearity) or linearly dependent, the extra ‘ridge’ $\\lambda I$ can differ them from each other. If not differed, then $x$ for two multicollinear vectors can vary a lot, which increases the variance much. And the MSE difference can be calculated as, MSE_{OLS}-MSE_{r} = Var(x_{OLS})-Var(x_{r}) - E^2(x_r-x_{OLS})It’s clear that if $E^2&lt;Var(x_{OLS})-Var(x_r)$, which depends on $\\lambda$, the ridge regression has the smaller MSE. We may also wonder the concrete effects on the variance. The covariance matrix is $A^TA/(n-1)$, as we mentioned above, we can rewrite it to be $VD^2V^T$. Thus, $D^2$ is the variance within the principal component direction ($V$-s). And for direction $i$, $Var=\\frac{d_i^2}{n-1}$. Combine the equation $\\sum_{i=1}u_i\\frac{d_i^2}{d_i^2+\\lambda}u_i^Tb$, we can know that if the smaller the variance $d_i^2/(n-1)$ is, the more shrink will be applied by ridge regression. Lasso RegressionLASSO is the shorthand of Least Absolute Shrinkage and Selection Operator inspired by Tibshirani. Lasso regression focuses on the problem two, where there are many useless features. Lasso solves this problem by adding a L1-norm in the OLS form. \\min ||y-x\\beta||^2 \\ \\ \\text{s.t } \\sum_{i=1}||\\beta_i||\\le C\\\\ \\Rightarrow \\min L = ||y-x\\beta||^2+\\lambda||\\beta||Biased EstimatorSubgradientHowever, if we take derivative on $\\beta_j$, we will face a problem that $||\\beta||$ may not have the gradient. Therefore, we introduce the concept, which is subgradient, to substitute gradient when it doesn’t exist. Let $f:R^n\\to R$ be a convex function with domain $R^n$. A vector $v$ is called subgradient at $x_0\\in R^n$ if, f(x)-f(x_0)\\ge v*(x-x_0)All subgradient at $x_0$ forms a set called subdifferential, denoted by $\\partial f(x_0)$. And $x_0$ is the minimum point of a convex function if and only if $0\\in\\partial f(x_0)$. Wikipediaen.wikipedia.org/wiki/Subgradient_method EstimatorNow, we can analyze the best point of $\\beta$. In order to get the explicit solution, we suppose $x$ is orthogonal. If we take differential on $\\beta_i$, there may be two scenarios, Gradient exists and $\\beta_i\\ne 0$. Gradient doesn’t exist so according to the subgradient, $\\beta_i=0\\in\\partial f$. For situation 1, \\begin{aligned} \\frac{\\partial L}{\\partial \\beta_i} = (-2x^Ty+2x^Tx\\beta_i)+\\lambda*\\text{sign}(\\beta_i)&=0\\\\ \\Rightarrow x^Ty-x^Tx\\beta_i&=-\\frac{\\lambda}{2}\\text{sign}(\\beta_i)\\\\ \\beta_i^{OLS}-\\beta_i&=-\\frac{\\lambda}{2}\\text{sign}(\\beta_i)\\\\ \\Rightarrow \\beta_i = \\begin{cases} \\beta^{OLS}_i-\\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i>\\frac{\\lambda}{2}\\\\ \\beta^{OLS}_i + \\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i\\frac{\\lambda}{2}\\\\ \\beta^{OLS}_i + \\frac{\\lambda}{2}, \\text{ if }\\beta^{OLS}_i 0 \\end{aligned}It gives the quadratic form and $(\\beta-\\hat{\\beta})$ is positive definite for $x^Tx$. The function should be drawn like below, If we let $z=(\\beta-\\hat{\\beta})^Tx^Tx(\\beta-\\hat{\\beta})$, where $z$ is any constant, we will get a ellipsoid graph on the $\\beta1.\\beta2$ plane. Either ridge or lasso has a constraint, we draw them on the same plane. From this viewpoint, it tells why Lasso generates sparsity solutions (many $\\beta$-s are 0). This is because L1-norm is not smooth and the $(0,\\beta_2)$ or $(\\beta_1,0)$ are easy to touch the ellipsoid. Someone plug Lasso into the Ridge regression and call it as Elastic Net. It can be written as, \\min L= ||y-x\\beta||^2+\\lambda_1||\\beta||^2+\\lambda_2||\\beta||,\\ \\ \\lambda_1+\\lambda_2=1The elastic net combines all the advantages of ridge and lasso so that you can choose it when you know nothing about the dataset. Of course, you can choose the potential type of regression by adjusting $\\lambda_1$ and $\\lambda_2$. The figure of its constraint is, Thus, it is able to get sparse solution and smooth solution by an implemented weighting mechanism. It is the improvement. Thanks For Reading! ReferenceList of Works MIT Course 18.06, Linear Algebra, Gilbert Strang Jiaming Mao, Data Analysis for Economics, Regularization Upenn, Ridge Regression Stackexchange, Relationship between PCA and SVD Robert Tibshirani, Regression Shrinkage and Selection via the Lasso Wikipedia, Subgradient Ryan Tibshirani, Convex Optimization - Coordinate Descent Pictures Cover: &nbsp;&nbsp; Pixiv by Katann Some Figures: &nbsp;&nbsp; Ridge Regression, Upenn &nbsp;&nbsp; Regularization, Jiaming Mao","link":"/2021/05/07/Report-Ridge-Lasso/"}],"tags":[{"name":"微观计量","slug":"微观计量","link":"/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"}],"categories":[]}