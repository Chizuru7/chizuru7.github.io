<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Ridge and Lasso Regression - Chizuru7&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Chizuru7&#039;s Blog"><meta name="msapplication-TileImage" content="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Chizuru7&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Ridge and Lasso Regressions are expansions of OLS. The key difference is that they restrict the coefficients in loss function. You may wonder why we should implement constraints and this report tries"><meta property="og:type" content="blog"><meta property="og:title" content="Ridge and Lasso Regression"><meta property="og:url" content="http://example.com/2021/05/07/Report-Ridge-Lasso/"><meta property="og:site_name" content="Chizuru7&#039;s Blog"><meta property="og:description" content="Ridge and Lasso Regressions are expansions of OLS. The key difference is that they restrict the coefficients in loss function. You may wonder why we should implement constraints and this report tries"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/gallery/Chenswire.png"><meta property="article:published_time" content="2021-05-07T11:06:03.000Z"><meta property="article:modified_time" content="2021-05-12T14:55:24.059Z"><meta property="article:author" content="Junwei Lin"><meta property="article:tag" content="微观计量"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/Chenswire.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2021/05/07/Report-Ridge-Lasso/"},"headline":"Chizuru7's Blog","image":["http://example.com/gallery/Chenswire.png"],"datePublished":"2021-05-07T11:06:03.000Z","dateModified":"2021-05-12T14:55:24.059Z","author":{"@type":"Person","name":"Junwei Lin"},"description":"Ridge and Lasso Regressions are expansions of OLS. The key difference is that they restrict the coefficients in loss function. You may wonder why we should implement constraints and this report tries"}</script><link rel="canonical" href="http://example.com/2021/05/07/Report-Ridge-Lasso/"><link rel="icon" href="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png" alt="Chizuru7&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Chizuru7"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/gallery/Chenswire.png" alt="Ridge and Lasso Regression"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-05-07T11:06:03.000Z" title="2021/5/7 下午7:06:03">2021-05-07</time>发表</span><span class="level-item"><time dateTime="2021-05-12T14:55:24.059Z" title="2021/5/12 下午10:55:24">2021-05-12</time>更新</span><span class="level-item">14 分钟读完 (大约2172个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">Ridge and Lasso Regression</h1><div class="content"><p><strong>Ridge</strong> and <strong>Lasso</strong> Regressions are expansions of <strong>OLS</strong>. The <strong>key difference</strong> is that they restrict the <strong>coefficients in loss function</strong>. You may wonder why we should implement constraints and this report tries to explain the reason. Of course, there must be some problems in <strong>OLS</strong> and let’s have a look.</p>
<span id="more"></span>
<h2 id="Problems-of-Ordinary-Least-Squares"><a href="#Problems-of-Ordinary-Least-Squares" class="headerlink" title="Problems of Ordinary Least Squares"></a>Problems of Ordinary Least Squares</h2><p>In this report, I plan to construct a view towards it based on <strong>linear algebra</strong>. Basically, we have $Ax=b$ for $m$ equations and $n$ features and they are contained into $A$ and $b$. OLS is born to tackle the problem where we tend to give the appropriate solution but we ‘cannot solve it’. Said another way, what can we do if $A$ is <strong>singular</strong> ($b$ is <strong>out of</strong> the column space $C(A)$)? And this situation is quite often when $m\ne n$. We can show it in the figure 1 where the line is the column space consisting of all the <strong>linear combinations of $a$ (basis)</strong>.</p>
<p><span id='fig1'></span></p>
<div align=center>
    <img src = 'https://i.loli.net/2021/05/08/anohKl2A759L3FB.png' style='zoom:50%' alt='Fig1: Out of Col Space'>
</div>

<p>However, if we were forced to get the solution, we can only get the <strong>closest</strong> one. And that’s what <strong>regression</strong> typically does. We should select $v_1$ instead of any others, like $v_2$, because $e_1$ is the <strong>shortest</strong> segment, which puts $v_1$ where it is. Based on the <strong>perpendicular condition</strong> (projection on $a$), we get,</p>
<script type="math/tex; mode=display">
v_1 = ax, \text{for }v_1\parallel a\\
a^T(b-ax) = 0, \text{for }a\perp e_1\\
\Rightarrow a^Tax=a^Tb\\
\Rightarrow x = (a^Ta)^{-1}a^Tb</script><p>, where $x$ is the coefficient of $a$ and it is the solution. In practice, if you calculate,</p>
<script type="math/tex; mode=display">
\min ||b-xa||^2</script><p>, then $a^Tax=a^Tb$ is the <strong>F.O.C</strong> of it. Of course, the matrix operating process can be applied in the higher-dimensional cases. It is the perpendicular property (uncorrelated) that makes the OLS estimator <strong>unbiased</strong>. However, the crucial part is that we require $a^Ta$ is <strong>invertible</strong>! Note that $Rank(a^Ta)=Rank(a)$. Therefore, we will easily proceed further, which is $a$ should have a <strong>full column rank</strong>.</p>
<p>When the requirement is not satisfied (a <strong>ill-conditioned $A$</strong>), there are two probabilities, <span id='probs'></span></p>
<ul>
<li>$m\ge n$, but <strong>linearly dependent vectors</strong> (Or highly correlated ones) ‘decrease’ the rank. </li>
<li>$m&lt;n$. Some vectors <strong>must be linearly dependent</strong>.</li>
</ul>
<p>Thus, we need <strong>Ridge regression</strong> and <strong>Lasso regression</strong> to solve the multi-collinearity problem. And let’s see how.</p>
<h2 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h2><h3 id="Biased-Estimator"><a href="#Biased-Estimator" class="headerlink" title="Biased Estimator"></a>Biased Estimator</h3><p>Ridge regression is widely used when <strong>most features are important</strong>, which is the <a href="#probs">problem one</a>. Ridge regression solve the method by building a ‘ridge’, which is <strong>adding $\lambda I$ ($\lambda&gt;0$) onto the $A^TA$</strong>. Say, if a full rank matrix is plus in, then the matrix $A^TA$ will also be <strong>nonsingular</strong>. However, one condition is $A$ should be <strong>standardized</strong>. Now, the solution to $x$ is,</p>
<script type="math/tex; mode=display">
x = (A^TA+\lambda I)^{-1}A^Tb</script><p>Like real numbers, <strong>an adding term makes the inverse a smaller multiplier</strong>. As a result, $x$ will become smaller. For simplicity, suppose $A$ is an orthogonal matrix. Say, $A^TA=I$. Then, $x_{OLS}=(A^TA)^{-1}A^Tb=A^Tb$,</p>
<script type="math/tex; mode=display">
x_{Ridge} = (I+\lambda I)^{-1}x_{OLS}=\frac{x_{OLS}}{1+\lambda}</script><div align=center>
    <img src = 'https://i.loli.net/2021/05/11/mXBDhRc1CqFjJly.gif' alt='Fig2: Ridge Estimator'>
</div>

<p>Thus, you can clearly notice that ridge regression is a <strong>shrink</strong> on OLS estimator but how to imagine it intuitively and what about a common case? After simple transposition in <strong>dimension one</strong>, we get,</p>
<script type="math/tex; mode=display">
a^T(b-ax)=\lambda x</script><p>,so the perpendicular condition is <strong>not satisfied</strong>. As a not surprising result, the answer is <strong>not closest</strong> any more, which leads it as a <strong>biased</strong> estimator. We can draw the Fig2, <span id='greenline'></span></p>
<div align=center>
    <img src = 'https://i.loli.net/2021/05/10/KVo1Pnalcgk4hj6.png' style='zoom:50%' alt='Fig3: Ridge Geomatric'>
</div>


<p><span id='consistent'></span>Since that we will have $ax_r+pa=x_{OLS}$ (where $x_r$ is the ridge estimator) in the figure, if we go <strong>deeper for the shrinkage</strong> details,</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^T(b-ax_r) &= \lambda x_r\\
||a||*||b-ax_r||*\cos \theta &=\lambda x_r\\
p*||a||^2 &= \lambda x_r\\
p &= \frac{\lambda x_r}{||a||^2}\\
\Rightarrow ax_r+ap &= ax_{OLS}\\
\Rightarrow x_r + \frac{\lambda x_r}{||a||^2}&=x_{OLS}\\
\Rightarrow x_r &= \frac{||a||^2}{||a||^2+\lambda}x_{OLS}
\end{aligned}</script><p>Note that the brown dashed segment is $b-ax$ and $x_r$ denotes the ridge estimator. As you can see, <strong>orthogonal cases</strong> is special one because $a$ is in a unit length. The result shows that it is because the shrinkage that makes $x_r$ biased. If  <strong>$\lambda=0$, ridge regression is actually applied the OLS</strong>. If $\lambda\to\infty$, $x_r$ <strong>will be really close to zero but not real zero</strong>, which makes it suit for problem one (maintaining all features). Say, the larger the $\lambda$ is, the more biased the estimator is. In mathematics, the amount of bias is, ($E(x_{OLS})=\beta$, $\beta$ is the true solution)</p>
<script type="math/tex; mode=display">
E(x_r-x_{OLS}) = [(A^TA+\lambda I)^{-1}-(A^TA)^{-1}]A^Tb</script><h3 id="Relationship-with-SVD"><a href="#Relationship-with-SVD" class="headerlink" title="Relationship with SVD"></a>Relationship with SVD</h3><h4 id="SVD-Decomposition"><a href="#SVD-Decomposition" class="headerlink" title="SVD Decomposition"></a>SVD Decomposition</h4><p>In linear algebra, we have an extraordinary magic called <strong>SVD (singular value decomposition)</strong>. In mathematics,</p>
<script type="math/tex; mode=display">
A = UDV^T</script><p> , where $U$ and $V$ are orthogonal matrices. If we apply the magic to $\hat{y}=Ax_r$,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{y} &= A(A^TA+\lambda I)^{-1}A^Tb\\
&= UDV^T(VD^2V^T+\lambda VV^T)^{-1} VDU^Tb,\text{ if orthogonal, } VV^T=I\\
&= UDV^T[V(D^2+\lambda)V^T]^{-1}VDU^Tb\\
&= UD(D^2+\lambda)^{-1}DU^Tb\\
&=\sum_{i=1}u_i\frac{d_i^2}{d_i^2+\lambda}u_i^Tb
\end{aligned}</script><p>, where $D$ is diagonal. Compare with $\hat{y}=Ax_{OLS}$,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{y}&=A(A^TA)^{-1}A^Tb\\
&=UDV^T(VD^2V^T)^{-1}VDU^Tb\\
&=UU^Tb\\
&=\sum_{i=1}u_iu_i^Tb
\end{aligned}</script><p>We can rewrite $\hat{y} = Ax_{OLS}$ to be $\hat{y}=Ux^U_{OLS}$, since $U$ is orthogonal. (Notice that $U^TUx=U^Tb\to x=U^Tb$). We can treat it <strong>as if projecting on $U$</strong>. So does the $\hat{y}$ of ridge.</p>
<script type="math/tex; mode=display">
\hat{y} = \underbrace{(D^2+\lambda)^{-1}}_{\text{coefficient}}\ \ UD(UD)^Tb = UDx_r^U</script><p>Say, we can treat it as if the input for the projection was $UD$ instead of $A$, and we should multiply a coefficient which is included in $x^U_r$. Actually, we are using a <strong>new</strong> variable, which is more concentrated, to replace <strong>original</strong> variables. You might realize that it is the <strong>PCA (Principal Component Analysis)</strong>. <span id='svdlink'></span></p>
<h4 id="Linkage-to-PCA"><a href="#Linkage-to-PCA" class="headerlink" title="Linkage to PCA"></a>Linkage to PCA</h4><blockquote><p>For data $X$, we have covariance matrix $C=X^TX/(n-1)$.</p>
<p>In eigen-decomposition, we have $C=Q\Lambda Q^T1$, for $C$ is symmetric. $\Lambda$ is diagonal of its all eigenvalues with decreasing order, and $Q$ is an orthogonal eigenvector matrix. <strong>Eigenvectors</strong> are <strong>Principal Axes</strong>. <strong>Projections</strong> on the axes are the <strong>Principal Components</strong>. The $j$-th principal component <strong>is given by the $j$-th column of</strong> $XQ$. ($XQ=\lambda Q$)</p>
<p>In <strong>SVD</strong>, $C=VD^2V^T$, where $Q=V$. The covariance matrix is $D$, and <strong>Principal Component</strong> is $XV=UDV^TV=UD$.</p>
<footer><strong>Stackexchange</strong><cite><a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca</a></cite></footer></blockquote>
<p>Therefore, OLS and ridge can both be regarded as <strong>projecting on the Principal Component Direction</strong> $U$ because $D$ is just coefficient. Fig4 may help you to understand it (make regression on the second one).</p>
<div align=center>
    <img src = 'https://i.loli.net/2021/05/10/65Par3NpJ9KtmvE.png' alt='Fig4: Project on U'>
</div>

<p>And if we back to the <a href="#greenline">Fig3</a> or <a href="#fig1">Fig1</a>, we can regard $a$ as the <strong>principal component</strong>. The length of it is $d_i$ from the $AV=UD$, where $U$ is in a unit length. So it is not a surprise to find that $\frac{d_i^2}{d_i^2+\lambda}$ in $\hat{y}=Ax_r$ is consistent with what we talked at <a href="#consistent">dimension-one illstration</a>. For high dimensions, we can illustrate like this, </p>
<div align=center>
    <img src = 'https://i.loli.net/2021/05/11/EZJjMuxY9KB3lm6.png' alt='Fig5: High Dimension Geomatric'>
</div>


<p>As you can see, the high dimensional case can <strong>be decomposed to be many <a href="#greenline">Fig3</a></strong>, which is one dimension. From this aspect by the thinking of <strong>PCA</strong>, we should <strong>standardize</strong> input $A$ because ridge regression is sensitive to it.  </p>
<h3 id="Variance-Decrease"><a href="#Variance-Decrease" class="headerlink" title="Variance Decrease"></a>Variance Decrease</h3><p>Except for solving the problem of ill-conditioned $A$, the ridge regression also <strong>decreases the variance of estimator</strong> $x_r$. Intuitively, estimators are shrink, so the variance of estimators are smaller. Since that we have $x_r = (A^TA+\lambda I)^{-1}A^Tb$, then the variance should be below, provided $A$ is full-rank,</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_r &= (A^TA+\lambda I)^{-1}A^T(A\beta+\epsilon)\\
x_r-\beta &= (A^TA+\lambda I)^{-1}A^T\epsilon\\
Var(x_r-\beta) &= (A^TA+\lambda I)^{-1}A^T\epsilon \epsilon^TA[(A^TA+\lambda I)^{-1}]^T\\
Var(x_r)&=\sigma^2(A^TA+\lambda I)^{-1}A^TA (A^TA+\lambda I)^{-1}
\end{aligned}</script><p> And we know that,</p>
<script type="math/tex; mode=display">
\begin{aligned}
x_{OLS}-\beta&= (A^TA)^{-1}A^T\epsilon\\
Var(x_{OLS}) &= (A^TA)^{-1}A^T\epsilon \epsilon^TA(A^TA)^{-1}\\
&= \sigma^2 (A^TA)^{-1} A^TA(A^TA)^{-1}
\end{aligned}</script><p>As we mentioned above, $(A^TA+\lambda I)^{-1}$ can be regarded as a <strong>smaller</strong> multiplier than $(A^TA)^{-1}$, so $Var(x_{OLS})&gt;Var(x_r)$. If a detailed proof is wanted, <a target="_blank" rel="noopener" href="https://www.statlect.com/fundamentals-of-statistics/ridge-regression">click here</a>. Let’s back to <strong>why</strong> we want to apply <strong>ridge</strong>. In the case that vectors are <strong>highly correlated (multicollinearity) or linearly dependent</strong>, the extra ‘ridge’ $\lambda I$ can <strong>differ them from each other</strong>. If not differed, then $x$ for two multicollinear vectors can <strong>vary a lot</strong>, which increases the variance much.</p>
<p>And the <strong>MSE difference</strong> can be calculated as,</p>
<script type="math/tex; mode=display">
MSE_{OLS}-MSE_{r} = Var(x_{OLS})-Var(x_{r}) - E^2(x_r-x_{OLS})</script><p>It’s clear that if $E^2&lt;Var(x_{OLS})-Var(x_r)$, which depends on $\lambda$, the ridge regression has the <strong>smaller MSE</strong>.</p>
<p>We may also wonder the concrete effects on the variance. The covariance matrix is $A^TA/(n-1)$, as we mentioned <a href="#svdlink">above</a>, we can rewrite it to be $VD^2V^T$. Thus, $D^2$ is the <strong>variance within the principal component direction ($V$-s)</strong>. And for direction $i$, $Var=\frac{d_i^2}{n-1}$. Combine the equation $\sum_{i=1}u_i\frac{d_i^2}{d_i^2+\lambda}u_i^Tb$, we can know that if <strong>the smaller the variance $d_i^2/(n-1)$ is</strong>, the more shrink will be applied by ridge regression.</p>
<h2 id="Lasso-Regression"><a href="#Lasso-Regression" class="headerlink" title="Lasso Regression"></a>Lasso Regression</h2><p>LASSO is the shorthand of <strong>Least Absolute Shrinkage and Selection Operator</strong> inspired by Tibshirani. Lasso regression focuses on the <a href="#probs">problem two</a>, where there are many <strong>useless features</strong>. Lasso solves this problem by adding a <strong>L1-norm</strong> in the OLS form.</p>
<script type="math/tex; mode=display">
\min ||y-x\beta||^2 \ \ \text{s.t } \sum_{i=1}||\beta_i||\le C\\ 
\Rightarrow \min L = ||y-x\beta||^2+\lambda||\beta||</script><h3 id="Biased-Estimator-1"><a href="#Biased-Estimator-1" class="headerlink" title="Biased Estimator"></a>Biased Estimator</h3><h4 id="Subgradient"><a href="#Subgradient" class="headerlink" title="Subgradient"></a>Subgradient</h4><p>However, if we take derivative on $\beta_j$, we will face a problem that $||\beta||$ may not have the gradient. Therefore, we introduce the concept, which is <strong>subgradient</strong>, to substitute gradient when it doesn’t exist.</p>
<blockquote><p>Let $f:R^n\to R$ be a <strong>convex</strong> function with domain $R^n$. A vector $v$ is called <strong>subgradient</strong> at $x_0\in R^n$ if,</p>
<script type="math/tex; mode=display">
f(x)-f(x_0)\ge v*(x-x_0)</script><p>All subgradient at $x_0$ forms a set called <strong>subdifferential</strong>, denoted by $\partial f(x_0)$. And $x_0$ is the minimum point of a convex function <strong>if and only if</strong> $0\in\partial f(x_0)$.</p>
<footer><strong>Wikipedia</strong><cite><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Subgradient_method">en.wikipedia.org/wiki/Subgradient_method</a></cite></footer></blockquote>
<h4 id="Estimator"><a href="#Estimator" class="headerlink" title="Estimator"></a>Estimator</h4><p>Now, we can analyze the best point of $\beta$. In order to get the <strong>explicit solution</strong>, we suppose $x$ is <strong>orthogonal</strong>. If we take differential on $\beta_i$, there may be two scenarios,</p>
<ol>
<li><strong>Gradient exists</strong> and $\beta_i\ne 0$.</li>
<li><strong>Gradient doesn’t exist</strong> so according to the subgradient, $\beta_i=0\in\partial f$.</li>
</ol>
<p>For situation 1, </p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L}{\partial \beta_i} = (-2x^Ty+2x^Tx\beta_i)+\lambda*\text{sign}(\beta_i)&=0\\
\Rightarrow x^Ty-x^Tx\beta_i&=-\frac{\lambda}{2}\text{sign}(\beta_i)\\
\beta_i^{OLS}-\beta_i&=-\frac{\lambda}{2}\text{sign}(\beta_i)\\
\Rightarrow \beta_i = \begin{cases}
\beta^{OLS}_i-\frac{\lambda}{2}, \text{ if }\beta^{OLS}_i>\frac{\lambda}{2}\\
\beta^{OLS}_i + \frac{\lambda}{2}, \text{ if }\beta^{OLS}_i<\frac{\lambda}{2}
\end{cases}
\end{aligned}</script><p>For situation 2,</p>
<script type="math/tex; mode=display">
\begin{aligned}
0 = \beta_i\in\partial L &= (-2x^Ty+2x^Tx\beta_i)+\lambda e\\
-2\beta_i^{OLS}+2\beta_i+\lambda e&=0\\
\beta_i^{OLS}&=\frac{\lambda}{2}e
\end{aligned}</script><p>, where $e\in[-1,1]$, calculated from $\partial ||\beta||$. Thus, $\beta_i=0$ if $-\frac{\lambda}{2}\le \beta_i^{OLS}\le \frac{\lambda}{2}$.</p>
<p>To sum up,</p>
<script type="math/tex; mode=display">
\beta_i = \begin{cases}
\beta^{OLS}_i-\frac{\lambda}{2}, \text{ if }\beta^{OLS}_i>\frac{\lambda}{2}\\
\beta^{OLS}_i + \frac{\lambda}{2}, \text{ if }\beta^{OLS}_i<-\frac{\lambda}{2}\\
0, \text{ if }|\beta_i^{OLS}|\le \frac{\lambda}{2}
\end{cases}</script><p>As you can see, $\beta_i$ is a <strong>biased</strong> estimator. Simply illustrated below,</p>
<div align=center>
    <img src = 'https://i.loli.net/2021/05/11/G7J2pnAMtXIog9K.gif' alt='Fig6: Lasso Estimator'>
</div>


<p>As you can see, if $\lambda=0$, lasso is unbiased because of OLS. If $\lambda\to\infty$, then <strong>all</strong> $\beta_i$-s <strong>would be shrink to zero</strong>. <strong>Larger</strong> $\lambda$ <strong>leads to a larger bias and smaller variance</strong>. It is the characteristic that Lasso regression can <strong>select</strong> variables. The core idea is that those variables that smaller than $\frac{\lambda}{2}$ in absolute value have no capacity to explain changes of $y$. </p>
<p>Like the ridge regression, lasso has the chance to decrease the MSE by the bias-variance tradeoff since that the <strong>coefficients should be estimated</strong> is less than OLS. And the dropped ones are mainly <strong>linear dependent (or multilinearity) ones</strong>, which will increase variance substantially. </p>
<h3 id="Comparison-and-Improvement"><a href="#Comparison-and-Improvement" class="headerlink" title="Comparison and Improvement"></a>Comparison and Improvement</h3><p>By the way, the algebra form of ridge regression,</p>
<script type="math/tex; mode=display">
\min ||y-x\beta||^2 \ \ \text{s.t } \sum_{i=1}||\beta_i||^2\le C\\ 
\Rightarrow \min L= ||y-x\beta||^2+\lambda||\beta||^2</script><p>, which is added a <strong>L2-norm</strong>. Let’s rewrite $||y-x\beta||^2$, if $\hat{\beta}$ is the estimated one.</p>
<script type="math/tex; mode=display">
\begin{aligned}
(y-x\hat{\beta})^T(y-x\hat{\beta}) &=y^Ty-2y^Tx\hat{\beta}+(x\hat{\beta})^T(x\hat{\beta})\\
&=(x\beta)^T(x\beta)-2(x\beta)^Tx\hat{\beta}+(x\hat{\beta})^T(x\hat{\beta})\\
&=(\beta-\hat{\beta})^Tx^Tx(\beta-\hat{\beta})> 0
\end{aligned}</script><p>It gives the quadratic form and $(\beta-\hat{\beta})$ is <strong>positive definite</strong> for $x^Tx$. The function should be drawn like below,</p>
<div align=center>
    <img src = 'https://i.loli.net/2021/05/12/bCTGR2Le5xuAyI7.png' style='zoom:50%' alt='Fig7: Positive Definite Graph'>
</div>
If we let $z=(\beta-\hat{\beta})^Tx^Tx(\beta-\hat{\beta})$, where $z$ is any constant, we will get a ellipsoid graph on the $\beta1.\beta2$ plane. Either ridge or lasso has a constraint, we draw them on the same plane.

<div align=center>
    <img src = 'https://i.loli.net/2021/05/12/Apo4QbaJsFrHGVn.png' style='zoom:25%' alt='Fig8: Left is Lasso'>
</div>

<p>From this viewpoint, it tells why Lasso generates <strong>sparsity</strong> solutions (many $\beta$-s are 0). This is because <strong>L1-norm</strong> is not smooth and the $(0,\beta_2)$ or $(\beta_1,0)$ are easy to touch the ellipsoid. </p>
<p>Someone plug Lasso into the Ridge regression and call it as <strong>Elastic Net</strong>. It can be written as,</p>
<script type="math/tex; mode=display">
\min L= ||y-x\beta||^2+\lambda_1||\beta||^2+\lambda_2||\beta||,\ \ \lambda_1+\lambda_2=1</script><p>The elastic net combines all the advantages of ridge and lasso so that you can choose it when you know nothing about the dataset. Of course, you can choose the potential type of regression by adjusting $\lambda_1$ and $\lambda_2$. The figure of its constraint is,</p>
<div align=center>
    <img src='https://i.loli.net/2021/05/12/rup943k5ZAf27oF.png' style='zoom:50%'>
</div>

<p>Thus, it is able to get sparse solution and smooth solution by <strong>an implemented weighting mechanism</strong>. It is the improvement.</p>
<div align=center style="font-size:40px;font-weight:bold">
    Thanks For Reading!
</div>





<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h3 id="List-of-Works"><a href="#List-of-Works" class="headerlink" title="List of Works"></a>List of Works</h3><ol>
<li>MIT Course 18.06, Linear Algebra, Gilbert Strang</li>
<li>Jiaming Mao, <a target="_blank" rel="noopener" href="https://jiamingmao.github.io/data-analysis/assets/Lectures/Model_Selection_and_Regularization.pdf">Data Analysis for Economics, Regularization</a></li>
<li>Upenn, <a target="_blank" rel="noopener" href="https://online.stat.psu.edu/stat508/lesson/5/5.1">Ridge Regression</a></li>
<li>Stackexchange, <a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">Relationship between PCA and SVD</a></li>
<li>Robert Tibshirani, Regression Shrinkage and Selection via the Lasso</li>
<li>Wikipedia, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Subgradient_method">Subgradient</a></li>
<li>Ryan Tibshirani, <a target="_blank" rel="noopener" href="https://www.stat.cmu.edu/~ryantibs/convexopt-S15/lectures/22-coord-desc.pdf">Convex Optimization - Coordinate Descent</a></li>
</ol>
<h3 id="Pictures"><a href="#Pictures" class="headerlink" title="Pictures"></a>Pictures</h3><ul>
<li><p>Cover: <a class="tag is-dark is-medium" href="https://www.pixiv.net/artworks/80461312" target="_blank"><br><span class="icon"><i class="fas fa-camera"></i></span>&nbsp;&nbsp; Pixiv by Katann </a></p>
</li>
<li><p>Some Figures: <a class="tag is-dark is-medium" href="https://online.stat.psu.edu/stat508/lesson/5/5.1" target="_blank"><br><span class="icon"><i class="fas fa-camera"></i></span>&nbsp;&nbsp; Ridge Regression, Upenn</a> <a class="tag is-dark is-medium" href="https://jiamingmao.github.io/data-analysis/assets/Lectures/Model_Selection_and_Regularization.pdf" target="_blank"><br><span class="icon"><i class="fas fa-camera"></i></span>&nbsp;&nbsp; Regularization, Jiaming Mao</a></p>
</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Ridge and Lasso Regression</p><p><a href="http://example.com/2021/05/07/Report-Ridge-Lasso/">http://example.com/2021/05/07/Report-Ridge-Lasso/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Junwei Lin</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-05-07</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-05-12</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/">微观计量</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/05/19/Hands-on-Decision-Tree-And-Adaboost/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Hands on Decision Tree And Adaboost</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/05/04/Challenge-Ordered-Logit/"><span class="level-item">Challenge: Ordered Logit</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://i.loli.net/2021/04/01/s3S5IyEexAjClVm.png" alt="Chizuru7"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chizuru7</p><p class="is-size-6 is-block">A Junior in XMU</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">1</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Chizuru7"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Mail" href="mailto:362296573@qq.com"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://s.team/p/cfhj-vnmw/QDPBQJDG"><i class="fab fa-steam"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Problems-of-Ordinary-Least-Squares"><span class="level-left"><span class="level-item">1</span><span class="level-item">Problems of Ordinary Least Squares</span></span></a></li><li><a class="level is-mobile" href="#Ridge-Regression"><span class="level-left"><span class="level-item">2</span><span class="level-item">Ridge Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Biased-Estimator"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Biased Estimator</span></span></a></li><li><a class="level is-mobile" href="#Relationship-with-SVD"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Relationship with SVD</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#SVD-Decomposition"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">SVD Decomposition</span></span></a></li><li><a class="level is-mobile" href="#Linkage-to-PCA"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">Linkage to PCA</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Variance-Decrease"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">Variance Decrease</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Lasso-Regression"><span class="level-left"><span class="level-item">3</span><span class="level-item">Lasso Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Biased-Estimator-1"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Biased Estimator</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Subgradient"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">Subgradient</span></span></a></li><li><a class="level is-mobile" href="#Estimator"><span class="level-left"><span class="level-item">3.1.2</span><span class="level-item">Estimator</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Comparison-and-Improvement"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Comparison and Improvement</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Reference"><span class="level-left"><span class="level-item">4</span><span class="level-item">Reference</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#List-of-Works"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">List of Works</span></span></a></li><li><a class="level is-mobile" href="#Pictures"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">Pictures</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2021/05/19/Hands-on-Decision-Tree-And-Adaboost/"><img src="/gallery/Chenswire.png" alt="Hands on Decision Tree And Adaboost"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-19T11:13:48.000Z">2021-05-19</time></p><p class="title"><a href="/2021/05/19/Hands-on-Decision-Tree-And-Adaboost/">Hands on Decision Tree And Adaboost</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/05/07/Report-Ridge-Lasso/"><img src="/gallery/Chenswire.png" alt="Ridge and Lasso Regression"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-07T11:06:03.000Z">2021-05-07</time></p><p class="title"><a href="/2021/05/07/Report-Ridge-Lasso/">Ridge and Lasso Regression</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/05/04/Challenge-Ordered-Logit/"><img src="/gallery/Ordered_Logit.png" alt="Challenge: Ordered Logit"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-05-04T06:56:34.000Z">2021-05-04</time></p><p class="title"><a href="/2021/05/04/Challenge-Ordered-Logit/">Challenge: Ordered Logit</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/03/31/Report-Regression-Spline/"><img src="/gallery/amiya_Splines.png" alt="Report - Regression Splines"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-03-31T09:38:41.000Z">2021-03-31</time></p><p class="title"><a href="/2021/03/31/Report-Regression-Spline/">Report - Regression Splines</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/03/29/VC%20Dimension/"><img src="/gallery/VCcover.jpg" alt="Report - VC Dimension"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-03-28T16:00:00.000Z">2021-03-29</time></p><p class="title"><a href="/2021/03/29/VC%20Dimension/">Report - VC Dimension</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">三月 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"><span class="tag">微观计量</span><span class="tag">6</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png" alt="Chizuru7&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Junwei Lin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>