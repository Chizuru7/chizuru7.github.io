<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>VC Dimension - Chizuru7&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Chizuru7&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Chizuru7&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a"><meta property="og:type" content="blog"><meta property="og:title" content="VC Dimension"><meta property="og:url" content="http://example.com/2021/03/29/VC%20Dimension/"><meta property="og:site_name" content="Chizuru7&#039;s Blog"><meta property="og:description" content="VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2021/03/30/arXfqtZmjxR5ksO.png"><meta property="og:image" content="https://i.loli.net/2021/03/30/z17csQRy8pVOGkY.png"><meta property="og:image" content="https://i.loli.net/2021/03/30/OIxkwWBr1CKytNl.png"><meta property="og:image" content="https://i.loli.net/2021/03/30/SIi9CTG4Ug8Xmns.png"><meta property="og:image" content="https://i.loli.net/2021/03/30/5Xgt3dmHNFhYz1O.png"><meta property="og:image" content="https://i.loli.net/2021/03/30/UlX5i9GPBkDbjt8.png"><meta property="article:published_time" content="2021-03-28T16:00:00.000Z"><meta property="article:modified_time" content="2021-03-31T08:52:11.790Z"><meta property="article:author" content="Junwei Lin"><meta property="article:tag" content="微观计量"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.loli.net/2021/03/30/arXfqtZmjxR5ksO.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2021/03/29/VC%20Dimension/"},"headline":"Chizuru7's Blog","image":["https://i.loli.net/2021/03/30/arXfqtZmjxR5ksO.png","https://i.loli.net/2021/03/30/z17csQRy8pVOGkY.png","https://i.loli.net/2021/03/30/OIxkwWBr1CKytNl.png","https://i.loli.net/2021/03/30/SIi9CTG4Ug8Xmns.png","https://i.loli.net/2021/03/30/5Xgt3dmHNFhYz1O.png","https://i.loli.net/2021/03/30/UlX5i9GPBkDbjt8.png"],"datePublished":"2021-03-28T16:00:00.000Z","dateModified":"2021-03-31T08:52:11.790Z","author":{"@type":"Person","name":"Junwei Lin"},"description":"VC dimension is a basic and important concept because it lays a good foundation in machine learning about the feasibility. The VC dimension tells how strong the ability will a model be. In general, a"}</script><link rel="canonical" href="http://example.com/2021/03/29/VC%20Dimension/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Chizuru7&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-03-28T16:00:00.000Z" title="2021/3/29 上午12:00:00">2021-03-29</time>发表</span><span class="level-item"><time dateTime="2021-03-31T08:52:11.790Z" title="2021/3/31 下午4:52:11">2021-03-31</time>更新</span><span class="level-item"> Junwei Lin </span><span class="level-item">17 分钟读完 (大约2528个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">VC Dimension</h1><div class="content"><p>VC dimension is a basic and important concept because it lays a good foundation in machine learning about the <strong>feasibility</strong>. The VC dimension tells how strong the ability will a model be. In general, a large VC dimension implies a powerful and complex model. However, it may not always be a good signal. If the VC dimension is far away large, then there will be problems in out-sample error, which may present as the overfitting problem. This report aims at discussing why the VC dimension is required and how it comes into being based on the process of machine learning. Notice that the thinking track of the report is based on the lecture Jiaming Mao presents. Figures are from the slides and the Internet. </p>
<p>Let’s talk about the feasibility of machine learning first.</p>
<span id="more"></span>
<hr>
<h2 id="Feasibility-Conditions"><a href="#Feasibility-Conditions" class="headerlink" title="Feasibility Conditions"></a>Feasibility Conditions</h2><h3 id="Process-of-Machine-Learning"><a href="#Process-of-Machine-Learning" class="headerlink" title="Process of Machine Learning"></a>Process of Machine Learning</h3><p>Machine learning is predicting the value of $y$ based on the value of $x$. And the picture below shows the process of a learning. $f(x)$ is the underlying mechanism, so called <strong>target function</strong>. And $\mathcal{H}$ is a set of candidate formula (also called hypothesis set or model), from which, through the algorithm $\mathcal{A}$ (calculating the loss), we can choose a <strong>“best”</strong> hypothesis $g$ that approximately equals to $f(x)$. The error emerged within the training data is called <strong>in-sample error</strong> and the one generated in testing data is called <strong>out-sample error</strong>. </p>
<div align=center>
<img src="https://i.loli.net/2021/03/30/arXfqtZmjxR5ksO.png" style="zoom:33%" title = "Machine Learning Process" />
</div>

<p>In such a process, we’d like to get $E_{out}=0$ if possible. Nevertheless, we take a compromise and make it as $E_{out}\approx0$ for there always are some disturbing terms. Now, a problem is that we can only observe $E_{in}$, which is the experience from the training data. Thus, we wonder whether $E_{out}\approx E_{in}\approx0$ such that $E_{in}$ can be a measurement. And it remains a problem here.</p>
<h3 id="Hoeffding’s-Inequality"><a href="#Hoeffding’s-Inequality" class="headerlink" title="Hoeffding’s Inequality"></a>Hoeffding’s Inequality</h3><p>Here we have an example, which is drawing marbles. The bin contains many many differently-colored marbles, and in this case we’d like to figure out the probability of red marbles in the bin, denoted by $\mu$. It is impossible for us to count every marbles and what we can do is to draw samples. Then, we denote the probability of red marbles in the samples as $\nu$.</p>
<div align=center>
<img src="https://i.loli.net/2021/03/30/z17csQRy8pVOGkY.png" style="zoom:33%" title = "Drawing Marbles" />
</div>

<blockquote><p>According to the law of large numbers, intuitively, the larger the sample is, the closer the distance between $\mu$ and $\nu$. One of the pattern is the Hoeffding’s inequality, (if the proof is wanted, <a target="_blank" rel="noopener" href="https://blog.csdn.net/liubai01/article/details/79947975">click here</a>.)</p>
<script type="math/tex; mode=display">
Pr(|\mu-\nu|>\epsilon)<2\exp(-2\epsilon^2N)</script></blockquote>
<p>And from the inequality we can see that when $N$ gets larger and larger, the upper bound of the distance between $\mu$ and $\nu$ is closer to be zero. Say, we are more and more confident to get the right probability of red marbles in bin.</p>
<h3 id="Feasible"><a href="#Feasible" class="headerlink" title="Feasible?"></a>Feasible?</h3><p>Let’s answer the question remained above. You start to draw marbles now (drawn i.i.d.), and you <strong>pre-experimentally</strong> set a hypothesis that “All marbles I draw is green”. In this case, $\mu$ is actually the $E_{out}$ and $\nu$ is the $E_{in}$, because the fraction of red marbles is the in-sample error (your hypothesis is wrong when catching a red one.) And then, the Hoeffding’s Inequality can be written as,</p>
<script type="math/tex; mode=display">
Pr(|E_{out}-E_{in}|>\epsilon)<2\exp(-2\epsilon^2N)</script><p>If the upper bound is close to zero and $\epsilon$ is sufficiently small, and the goal, which is $E_{in}\approx E_{out}$, can be achieved. Here, one thing should be reminded is that how $E_{in}$ comes into being. $E_{in}$ here is generated based on the specific hypothesis, we call it $h$.  This is <strong>verification</strong> because $h$ is given, you don’t have the process that <strong>choose</strong> a hypothesis.</p>
<p>Of course, there are a lot of candidate formula that can be applied, you may set it as “All marbles are yellow” etc. even though they generate the same answers, and they are contained in $\mathcal{H}$. In practice, there are even infinite hypothesis in $\mathcal{H}$ and we <strong>don’t know </strong>which hypothesis is the <strong>best</strong> one, $g$. Therefore, what will the upper bound be if we expand the Hoeffding’s Inequality for any hypothesis in $\mathcal{H}$?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </p>
<p>From probability theory we can see that,</p>
<script type="math/tex; mode=display">
\begin{aligned}
Pr[(|E_{out}(h_1)-E_{in}(h_1)|>\epsilon)\cup\cdots] &\le Pr[(|E_{out}(h_1)-E_{in}(h_1)|>\epsilon)\ \text{or}\ (|E_{out}(h_2)-E_{in}(h_2)|>\epsilon)\cdots]\\
&\le Pr(|E_{out}(h_1)-E_{in}(h_1)|>\epsilon) + Pr(|E_{out}(h_2)-E_{in}(h_2)|>\epsilon) + \cdots\\
&\le \sum_{i=1}^M 2\exp(-2\epsilon^2N) = 2M\exp(-2\epsilon^2N)
\end{aligned}</script><p>, $M$ denotes the number of hypothesis in $\mathcal{H}$. Clearly, if $M$ is large, then the model is complex. And let’s summarize the inequality as,</p>
<script type="math/tex; mode=display">
\forall h\in \mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\epsilon)\le2M\exp(-2\epsilon^2N)</script><p>To sum up, there are two conditions if machine learning is feasible.</p>
<ul>
<li>The Algorithm $\mathcal{A}$ can find a $g$ such that $E_{in}\approx0$, and it is consistent with the process “train”.<ul>
<li>Actually, the more complex the model is, the smaller the $E_{in}$ is. </li>
</ul>
</li>
<li>$M$ is finite and a $N$ is sufficiently large to make $E_{out}\approx E_{in}$, and it is consistent with the process “test”.</li>
</ul>
<p>Next, we’d like to focus on the second condition, and mainly talk about the $M$.</p>
<hr>
<h2 id="VC-Dimension"><a href="#VC-Dimension" class="headerlink" title="VC Dimension"></a>VC Dimension</h2><p>Generally, $\mathcal{H}$ contains infinite $h$. For example, a simple linear model $y=ax+b$ generates infinite $h$, because the parameters are changeable. How do we shrink the upper bound by substitute $M$?</p>
<h3 id="Effective-Number-of-Hypothesis"><a href="#Effective-Number-of-Hypothesis" class="headerlink" title="Effective Number of Hypothesis"></a>Effective Number of Hypothesis</h3><p>In the process that expanding Hoeffding’s inequality, the inequality below is used,</p>
<script type="math/tex; mode=display">
Pr(A_1\ \cup\ A_2\ \cup\ A_3\ \cdots) \le \sum_{i=1}^M Pr(A_i)</script><p>If and only if $A_i$s are independent, we choose the equal sign. Actually, there are many intersection part in $\mathcal{H}$. A simple example is shown in the graph. In the two-dimensional space, we can simply divide $\mathcal{H}$ into two parts. One judges that $x_1$ is positive, like $h_1$. Another one marks $x_1$ as negative, like $h_2$. Here, the effective number of hypothesis is two.</p>
<div align=center>
    <img src="https://i.loli.net/2021/03/30/OIxkwWBr1CKytNl.png" style="zoom: 50%;" title = 'An Example'/>
</div>

<p>Under a specific sample, the effective number is fixed. Thus, the inequality can be written as,</p>
<script type="math/tex; mode=display">
\forall h\in \mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\epsilon)\le2*\text{effective}(M)*\exp(-2\epsilon^2N)</script><h3 id="Growth-Function"><a href="#Growth-Function" class="headerlink" title="Growth Function"></a>Growth Function</h3><h4 id="Dichotomy"><a href="#Dichotomy" class="headerlink" title="Dichotomy"></a>Dichotomy</h4><p>Firstly, we’d like to introduce a binary target function and hypothesis set that contains $h:\mathcal{X}\to\{-1,+1\}$. </p>
<blockquote><p>And,</p>
<script type="math/tex; mode=display">
h(X_1,X_2,\cdots,X_n) = (h(X_1), h(X_2), \cdots, h(X_n))</script><p>is called one dichotomy. </p>
</blockquote>
<p>Generally speaking, dichotomy represents a result which <strong>marks</strong> all points in the sample. $\mathcal{H}(X_1,X_2,\cdots,X_n)$ denotes the number of all dichotomies generated by hypothesis set $\mathcal{H}$, given $n$ points.</p>
<p>For instance, if  three points are in the training set,</p>
<ul>
<li>One dichotomy is $(h_1(X_1), h_1(X_2), \cdots) = (+1,+1,+1)$. </li>
<li>Another dichotomy can be $(h_2(X_1), h_2(X_2), \cdots) = (-1,+1,+1)$.</li>
<li>…</li>
</ul>
<p>$2^3=8$ is the maximum dichotomies that $\mathcal{H}(X_1,X_2,\cdots)$ can value. Say, the number of dichotomies a $\mathcal{H}$ can generate at most is $2^n$.</p>
<h4 id="Shatter"><a href="#Shatter" class="headerlink" title="Shatter"></a>Shatter</h4><blockquote>
<p> Given n points, (and their locations are fixed,) if a hypothesis set $\mathcal{H}$ can generate <strong>exactly</strong> $2^n$ dichotomies, we call $\mathcal{H}$ shatters these points.</p>
</blockquote>
<p>An example is under two-dimensional dataset and the hypothesis set contains all <strong>linear model</strong>. There are three points, <strong>no matter what </strong>points are black, there always be a line that can separate black points from white points. (See the figure below.) As we mentioned above, three points have eight dichotomies. And black and white points are like the $+1,-1$ marks. If the model can tell the marks apart in every scenario, then we can say that the model, or $\mathcal{H}$, shatters the points.</p>
<div align=center>
    <img src="https://i.loli.net/2021/03/30/SIi9CTG4Ug8Xmns.png" style="zoom:50%;" title = "Linear Model Shatters 3 Points"/>
</div>

<p>However, if there are four points, then the linear model cannot shatter them. We call the <strong>smallest</strong> number of points as <strong>Break Point</strong>. Say, 4 is the break point for the two-dimensional linear regressor.</p>
<h4 id="Growth-Function-1"><a href="#Growth-Function-1" class="headerlink" title="Growth Function"></a>Growth Function</h4><blockquote><p>We define the growth function as, </p>
<script type="math/tex; mode=display">
m_\mathcal{H}(n)=\max_{X_1,X_2,\cdots,X_n \in \mathcal{X}} |\mathcal{H}(X_1,X_2,\cdots,X_n)|</script></blockquote>
<p>That is, $m_\mathcal{H}(n)$ is the maximum possible number of dichotomies $\mathcal{H}$ can generate on a dataset of n points. Obviously, $m_\mathcal{H}(n)\le2^n$. The larger the $m_\mathcal{H}(n)$ is, the more powerful the model is. And $m_\mathcal{H}(n)$ can tell us whether the dataset of size $s$ can be shattered by $\mathcal{H}$.</p>
<ul>
<li>If $m_\mathcal{H}(n)=10\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$)</li>
</ul>
<p>Now, can we simply exchange $\text{effective}(M)$ to $m_\mathcal{H}(n)$ to shrink the upper bound? </p>
<h3 id="VC-bound"><a href="#VC-bound" class="headerlink" title="VC bound"></a>VC bound</h3><p>No, because the upper bound for $m_\mathcal{H}(n)$ is still an explosively increasing function. It is hard to tell whether the result can converge to zero. <strong>Sauer’s Lemma</strong> solves the problem.</p>
<blockquote><p>If the break point $k$ exists, then $m_\mathcal{H}(n)$ is a polynomial. According to Sauer’s Lemma, </p>
<script type="math/tex; mode=display">
m_\mathcal{H}(n) \le \sum_{i=0}^{k-1} \begin{pmatrix}
N\\
i
\end{pmatrix}\le\begin{cases}
N^{k-1}+1,\\
(\frac{eN}{k-1})^{k-1}, \text{if } N\ge k-1
\end{cases}</script></blockquote>
<p>And the polynomial is much easier to converge and it releases a signal that it is feasible to make the upper bound to be zero! </p>
<blockquote><p>Through some complicated transformations, the inequality can be written as, (the proof is <a target="_blank" rel="noopener" href="https://nowak.ece.wisc.edu/SLT09/lecture19.pdf">here</a>)</p>
<script type="math/tex; mode=display">
\forall h\in \mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\epsilon)\le4*m_\mathcal{H}(2N)*\exp(-\frac{1}{8}\epsilon^2N)</script></blockquote>
<p>This inequality is the <strong>VC bound</strong> and the exponential part has a higher rate of convergence than the polynomial part. Thus, the inequality tells that if there is a break point, when $N$ is sufficiently large, for <strong>any hypothesis</strong> $h$, $E_{out}\to E_{in}$ is very possible. Then the machine learning in the space is feasible, which meets the second condition.</p>
<p>If the significance level is $\delta$, and the upper bound should be $\delta$. Then,</p>
<script type="math/tex; mode=display">
\Rightarrow Pr(|E_{out}(h)-E_{in}(h)|>\epsilon)\le\delta\\
\epsilon = \sqrt{\frac{8}{N}\ln\frac{4m_\mathcal{H}(2N)}{\delta}}\\
E_{in}(h)-\sqrt{\frac{8}{N}\ln\frac{4m_\mathcal{H}(2N)}{\delta}}\le E_{out}(h)\le E_{in}(h) + \sqrt{\frac{8}{N}\ln\frac{4m_\mathcal{H}(2N)}{\delta}}，\text{probability 1} -\delta</script><p>The bound is called <strong>VC generalization bound</strong>, which is usually applied in training model because we are required to select a best hypothesis. But, we will not apply it onto the testing. Instead, we use the original Hoeffding’s Inequality for there is only one hypothesis and the VC bound is much looser.</p>
<p>Why is VC bound so loose? </p>
<ul>
<li>The basic Hoeffding’s inequality used in the proof already has a slack.</li>
<li>Using $m_\mathcal{H}(N)$ to quantify the number of dichotomies on N points, regardless of which $N$ points are in the dataset, gives us a <strong>worst-case</strong> estimate (because we expanded the inequality to all $h$).</li>
<li>Bound $m_\mathcal{H}(N)$ by a simple polynomial.</li>
</ul>
<h3 id="VC-Dimension-1"><a href="#VC-Dimension-1" class="headerlink" title="VC Dimension"></a>VC Dimension</h3><p>Now, we can talk about the definition of <strong>VC dimension</strong>.</p>
<blockquote>
<p>The <strong>Vapnik-Chervonenkis (VC) dimension</strong> of $\mathcal{H}$, denoted $d_{vc}(\mathcal{H})$ is the size of the <strong>largest</strong> data set that $\mathcal{H}$ can shatter. </p>
</blockquote>
<p>Remember that we have already talked about this problem in part <strong>Growth Function</strong>. If $m_\mathcal{H}(n)=10\le2^s$, then it can shatter the dataset of size $s$. ($s=1,2,3$) And here, $d_{vc}(\mathcal{H})=3$. If $m_\mathcal{H}(n)=\infty$, then $d_{vc}(\mathcal{H})=\infty$. <strong>In general, $d_{vc}(\mathcal{H})=k-1$, where $k$ is the break point.</strong> If the VC dimension is large, the model $\mathcal{H}$ is complex because a large sample can be shattered by it. Thus, VC dimension tells the <strong>ability of learning</strong> for the model. </p>
<p>The definition of VC dimension has <strong>nothing to do</strong> with the <strong>locations</strong> of points in dataset, which leads to a problem. For example, again, under two-dimensional plane, there are three points. However, this time, they are on the same line (See the graph). A linear model cannot shatter the dataset now.</p>
<div align=center>
    <img src="https://i.loli.net/2021/03/30/5Xgt3dmHNFhYz1O.png" style='zoom:25%' title="Three Dots Locates on One Line">
</div>

<p>Therefore, VC dimension tells you the largest dataset $\mathcal{H}$ can shatter, but <strong>not every same-sized dataset can be shattered</strong>.</p>
<p>The VC bound can also be written as, (the first inequality in Sauer’s Lemma is usually used.)</p>
<script type="math/tex; mode=display">
\forall h\in \mathcal{H}, Pr(|E_{out}(h)-E_{in}(h)|>\epsilon)\le4*((2N)^{d_{vc}(\mathcal{H})}+1)*\exp(-\frac{1}{8}\epsilon^2N)</script><h4 id="Approximation-Generalization-Tradeoff"><a href="#Approximation-Generalization-Tradeoff" class="headerlink" title="Approximation-Generalization Tradeoff"></a>Approximation-Generalization Tradeoff</h4><p>Recall the two conditions if machine learning is feasible, and now, the two conditions should be,</p>
<ul>
<li>The algorithm $\mathcal{A}$ can find the $g$ that makes $E_{in}\approx0$.<ul>
<li>Actually, the larger the VC dimension is (the more complex the model is), the smaller the $E_{in}$ is. </li>
</ul>
</li>
<li>The VC dimension are <strong>finite</strong> to make the polynomial part easy to converge and $N$ should be sufficiently large so that $E_{out}\approx E_{in}$.</li>
</ul>
<p>Also, recall the relationship that we talked in VC bound,</p>
<script type="math/tex; mode=display">
E_{in}(h)-\sqrt{\frac{8}{N}\ln\frac{4((2N)^{d_{vc}(\mathcal{H})}+1)}{\delta}}\le E_{out}(h)\le E_{in}(h) + \sqrt{\frac{8}{N}\ln\frac{4((2N)^{d_{vc}(\mathcal{H})}+1)}{\delta}}，\text{with probability 1} -\delta</script><p>Let $R = \sqrt{\frac{8}{N}\ln\frac{4((2N)^{d_{vc}(\mathcal{H})}+1)}{\delta}}  $.</p>
<p>If $N$ holds constant, then if $d_{vc}(\mathcal{H})$ increases, then $R$ will increases, which means the bound will become loose. As a result, $E_{out}$ may get away from $E_{in}$. Nevertheless, $\mathcal{H}$ will be more complicated. Say, $E_{in}$ decreases, even reaches the scenario that $E_{in}=0$. We call it <strong>approximation-generalization tradeoff</strong>. When one condition gets too tight, another one will be hard to meet. Therefore, we’d better find a point where we can balance these two well. And based on the inequality, it is how the VC dimension functions to make machine learning feasible.</p>
<div align=center>
    <img src="https://i.loli.net/2021/03/30/UlX5i9GPBkDbjt8.png" style="zoom: 25%;" title="Tradeoff">
</div>

<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Starting from the two feasibility conditions, this report mainly discusses how the VC dimension comes into being and how should we understand VC dimension. VC dimension is important because it regulates the two conditions for machine to learn, which is the approximation-generalization tradeoff. Actually, mathematics is just a tool for us to shrink the bound and VC dimension is the aggregation of them, which can help us study further in machine learning.</p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol>
<li>Jiaming Mao, <a target="_blank" rel="noopener" href="https://github.com/jiamingmao/data-analysis">Data Analysis for Economics</a></li>
<li>Liubai01’s blog, <a target="_blank" rel="noopener" href="https://blog.csdn.net/liubai01/article/details/79947975">机器学习推导合集01</a></li>
<li><a target="_blank" rel="noopener" href="https://nowak.ece.wisc.edu/SLT09/lecture19.pdf">ECE 901 Lecture 19</a></li>
<li>Yaser &amp; Malik &amp; Hsuan-tien Lin, Learn from Data [M], California Institute of Technology</li>
<li>Hsuan-tien Lin, <a target="_blank" rel="noopener" href="https://github.com/RedstoneWill/HsuanTienLin_MachineLearning">Machine Learning Foundation</a></li>
</ol>
</div><div class="article-licensing box"><div class="licensing-title"><p>VC Dimension</p><p><a href="http://example.com/2021/03/29/VC Dimension/">http://example.com/2021/03/29/VC Dimension/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Junwei Lin</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-03-29</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-03-31</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/">微观计量</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/"><span class="level-item">Data Analysis for Economics Intro</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://i.loli.net/2021/03/30/Ugv8YthjHOplSbR.jpg" alt="Chizuru7"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chizuru7</p><p class="is-size-6 is-block">A Senior in XMU</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">1</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Chizuru7" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Chizuru7"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Feasibility-Conditions"><span class="level-left"><span class="level-item">1</span><span class="level-item">Feasibility Conditions</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Process-of-Machine-Learning"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Process of Machine Learning</span></span></a></li><li><a class="level is-mobile" href="#Hoeffding’s-Inequality"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">Hoeffding’s Inequality</span></span></a></li><li><a class="level is-mobile" href="#Feasible"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Feasible?</span></span></a></li></ul></li><li><a class="level is-mobile" href="#VC-Dimension"><span class="level-left"><span class="level-item">2</span><span class="level-item">VC Dimension</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Effective-Number-of-Hypothesis"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Effective Number of Hypothesis</span></span></a></li><li><a class="level is-mobile" href="#Growth-Function"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Growth Function</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Dichotomy"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">Dichotomy</span></span></a></li><li><a class="level is-mobile" href="#Shatter"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">Shatter</span></span></a></li><li><a class="level is-mobile" href="#Growth-Function-1"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">Growth Function</span></span></a></li></ul></li><li><a class="level is-mobile" href="#VC-bound"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">VC bound</span></span></a></li><li><a class="level is-mobile" href="#VC-Dimension-1"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">VC Dimension</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Approximation-Generalization-Tradeoff"><span class="level-left"><span class="level-item">2.4.1</span><span class="level-item">Approximation-Generalization Tradeoff</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Summary"><span class="level-left"><span class="level-item">3</span><span class="level-item">Summary</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">References</span></span></a></li></ul></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-28T16:00:00.000Z">2021-03-29</time></p><p class="title"><a href="/2021/03/29/VC%20Dimension/">VC Dimension</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-20T16:00:00.000Z">2021-03-21</time></p><p class="title"><a href="/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/">Data Analysis for Economics Intro</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">三月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"><span class="tag">微观计量</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Chizuru7&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Junwei Lin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>