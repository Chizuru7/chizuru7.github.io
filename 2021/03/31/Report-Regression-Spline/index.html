<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Report - Regression Splines - Chizuru7&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Chizuru7&#039;s Blog"><meta name="msapplication-TileImage" content="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Chizuru7&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline belongs to non-parametric family and it is the feature that endows the regression high f"><meta property="og:type" content="blog"><meta property="og:title" content="Report - Regression Splines"><meta property="og:url" content="http://example.com/2021/03/31/Report-Regression-Spline/"><meta property="og:site_name" content="Chizuru7&#039;s Blog"><meta property="og:description" content="Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline belongs to non-parametric family and it is the feature that endows the regression high f"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/gallery/amiya_Splines.png"><meta property="article:published_time" content="2021-03-31T09:38:41.000Z"><meta property="article:modified_time" content="2021-04-17T07:19:53.219Z"><meta property="article:author" content="Junwei Lin"><meta property="article:tag" content="ÂæÆËßÇËÆ°Èáè"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/gallery/amiya_Splines.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2021/03/31/Report-Regression-Spline/"},"headline":"Chizuru7's Blog","image":["http://example.com/gallery/amiya_Splines.png"],"datePublished":"2021-03-31T09:38:41.000Z","dateModified":"2021-04-17T07:19:53.219Z","author":{"@type":"Person","name":"Junwei Lin"},"description":"Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline belongs to non-parametric family and it is the feature that endows the regression high f"}</script><link rel="canonical" href="http://example.com/2021/03/31/Report-Regression-Spline/"><link rel="icon" href="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png" alt="Chizuru7&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Chizuru7"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="ÁõÆÂΩï" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="ÊêúÁ¥¢" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/gallery/amiya_Splines.png" alt="Report - Regression Splines"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-03-31T09:38:41.000Z" title="2021/3/31 ‰∏ãÂçà5:38:41">2021-03-31</time>ÂèëË°®</span><span class="level-item"><time dateTime="2021-04-17T07:19:53.219Z" title="2021/4/17 ‰∏ãÂçà3:19:53">2021-04-17</time>Êõ¥Êñ∞</span><span class="level-item">25 ÂàÜÈíüËØªÂÆå (Â§ßÁ∫¶3745‰∏™Â≠ó)</span></div></div><h1 class="title is-3 is-size-4-mobile">Report - Regression Splines</h1><div class="content"><p>Regression Spline is a widely-used method in data analysis for some non-linear relationships. Regression Spline <strong>belongs to non-parametric family</strong> and it is the feature that endows the regression high flexibility. This report aims to tell you a story about</p>
<ul>
<li>Why we need Regression Splines,</li>
<li>How we make the regression ‚ÄúSpline‚Äù. </li>
</ul>
<p>However, before the story, it‚Äôs inevitable to talk about some basic regression. Let‚Äôs briefly review them.</p>
<span id="more"></span> 
<p>Intuitively, we simulate dots based on $f(x)=\sin(x), x\in[-\pi,2\pi]$ with the Gaussian-distributed error. For the graph, a code will be attached. If you ‚Äòd like to see the code, please just unfold it. <span id="jump"></span></p>
<figure class="highlight python"><figcaption><span>Click to Unfold</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">15</span>)</span><br><span class="line"><span class="comment">## Generate 150 dots</span></span><br><span class="line">x = np.linspace(-np.pi,<span class="number">2</span>*np.pi,<span class="number">150</span>)</span><br><span class="line">y0 = np.sin(x)</span><br><span class="line">e = np.random.normal(<span class="number">0</span>,<span class="number">1</span>,<span class="number">150</span>)</span><br><span class="line">y = y0+e <span class="comment"># Add error term</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.grid(ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">ax.scatter(x,y,color=<span class="string">&#x27;lightgrey&#x27;</span>)</span><br><span class="line">ax.plot(x,y0,color=<span class="string">&#x27;r&#x27;</span>) <span class="comment"># oringinal trace</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align=center>
<img src='https://i.loli.net/2021/04/03/K6qiIhHWwFgcmuQ.png' style='zoom:75%' alt='Fig1: Noisy Dots and True Function'>
</div>


<hr>
<h2 id="From-Linear-to-Polynomial"><a href="#From-Linear-to-Polynomial" class="headerlink" title="From Linear to Polynomial"></a>From Linear to Polynomial</h2><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>At the beginning, we always apply the linear regression. Say, the hypothesis set $\mathcal{H}=\{h(x)\} $ consisting of linear functions, assuming there are $N$ samples and $p$ dimensions per sample.</p>
<script type="math/tex; mode=display">
h(x) = x'\beta,\ \ \text{where } x=\begin{pmatrix}
1\\
x_1\\
\vdots\\
x_p
\end{pmatrix}
,\ \beta=\begin{pmatrix}
\beta_0\\
\beta_1\\
\vdots\\
\beta_p
\end{pmatrix}</script><p>And the $\beta$ determines whether the function is the <strong>best</strong> one. Of course, L2 loss is always chosen to measure best. The calculation of $E_{in}$ can be written below,</p>
<script type="math/tex; mode=display">
\begin{aligned}
E_{in} &= \frac{1}{N}\sum_{i=1}^N (x_i'\beta-y_i)^2\\
&= \frac{1}{N}\left|\begin{array}{c} 
    x_1'\beta - y_1 \\ 
    \vdots \\ 
    x_N'\beta - y_N  
\end{array}\right|^2\\
&=\frac{1}{N}\left|X\beta-\vec{y}\right|^2,\ \text{where } 
X = \begin{bmatrix}
1 & x_{11} & \cdots & x_{1p}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{N1} & \cdots & x_{Np}
\end{bmatrix}
\end{aligned}</script><p>To minimize it, the gradient should be zero,</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \beta}\frac{1}{N}(\beta'X'X\beta-2\beta'X'\vec{y}+\vec{y}'\vec{y})=\vec{0}\\
\frac{2}{N}(X'X\beta-X'\vec{y})=\vec{0}\\
\beta = (X'X)^{-1}X'\vec{y}</script><p><strong>Be sure</strong> to remember the equation that generates best $\beta$, it will be the <strong>foundation</strong> for the following regressions. And the estimator is <strong>BLUE</strong> (best linear unbiased estimator), proved by Gaussian-Markov Theorem.</p>
<figure class="highlight python"><figcaption><span>Click to Unfold</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = np.ones((<span class="number">150</span>,<span class="number">2</span>))</span><br><span class="line">X[:,<span class="number">1</span>] = x <span class="comment"># Matrix X is generated</span></span><br><span class="line">Y = np.zeros((<span class="number">150</span>,<span class="number">1</span>))</span><br><span class="line">Y[:,<span class="number">0</span>] = y <span class="comment"># Matrix Y is generated</span></span><br><span class="line">pseudo = np.dot(np.linalg.inv(np.dot(X.T,X)), X.T) <span class="comment"># Calculate the pseudo-inverse</span></span><br><span class="line">beta = np.dot(pseudo,Y)</span><br><span class="line">y_lin = beta[<span class="number">0</span>]+beta[<span class="number">1</span>]*x</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.grid(ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">ax.scatter(x, y, color=<span class="string">&#x27;lightgrey&#x27;</span>)</span><br><span class="line">ax.plot(x, y0, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Original&#x27;</span>) <span class="comment"># oringinal trace</span></span><br><span class="line">ax.plot(x, y_lin, color=<span class="string">&#x27;y&#x27;</span>, label=<span class="string">&#x27;Linear&#x27;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align=center>
    <img src="https://i.loli.net/2021/04/03/Csq7G3aKMJoz94r.png" alt="Fig2: Linear Regression" style="zoom:75%;" title="Fig2: Linear Regression"/>
</div>



<hr>
<h3 id="Polynomial"><a href="#Polynomial" class="headerlink" title="Polynomial"></a>Polynomial</h3><h4 id="Basis-Expansion"><a href="#Basis-Expansion" class="headerlink" title="Basis Expansion"></a>Basis Expansion</h4><p>Actually, there are many underlying functions that are not typically linear. Then, the question is how to deal with the non-linear terms. Our solution is simply to regard each non-linear term as <strong>a whole ‚Äúdegree-one‚Äù term</strong>. </p>
<p>For example, the model we‚Äôd like to apply is $h(x)=\beta_0+\beta_1x+\beta_2x^2$. The only non-linear part is $x^2$, and we use $\phi(x)=x^2$ to replace it. The result is $h(x)=\beta_0+\beta_1x+\beta_2\phi(x)$. Practically, we this method can be applied on linear terms to build a kind of consistency. Say, $\phi$ can be any function including dummies and constants.</p>
<blockquote><p>$\phi(x)$ is called <strong>basis function</strong>. A <strong>linear basis function model</strong> is defined as,</p>
<script type="math/tex; mode=display">
y=\sum_{i=1}^M \beta_i\phi_i</script><footer><strong>Jiaming Mao</strong><cite><a target="_blank" rel="noopener" href="https://github.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf">github.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf</a></cite></footer></blockquote>
<p>Linear basis function model ensures that non-linear regression can be calculated by OLS because it shows the consistent linear form. And, $\hat{\beta}=(\Phi‚Äô\Phi)^{-1}\Phi‚ÄôY$, where $\Phi=(\phi_1,\cdots,\phi_M)$‚Äô.</p>
<h4 id="Polynomial-Regression"><a href="#Polynomial-Regression" class="headerlink" title="Polynomial Regression"></a>Polynomial Regression</h4><p>We assume that the dots we simulated is underlying a cubic polynomial function, which is, specifically, $h(x)=\beta_0+\beta_1x+\beta_2x^2+\beta_3 x^3$. Follow the procedure, we can get a regression curve shown in the first graph. However, when we increases the polynomial degree, the regression curve tends to be more and more abnormal. Actually, it‚Äôs a problem called <strong>overfitting</strong>, where $E_{out}$ is large while $E_{in}$ is very small. (In order to roughly compare the regression models, $R^2$ is also attached on figures.)</p>
<blockquote><p>In statistics, <strong>overfitting</strong> is ‚Äúthe production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably‚Äù.</p>
<footer><strong>Leineber</strong><cite>D.J (2007). Stupid data miner tricks</cite></footer></blockquote> 
<figure class="highlight python"><figcaption><span>Click to Unfold</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polynomial_regression</span>(<span class="params">x,y,degree=<span class="number">3</span></span>):</span> <span class="comment">#define a polynomial regression function</span></span><br><span class="line">    Y = np.zeros((<span class="built_in">len</span>(y),<span class="number">1</span>))</span><br><span class="line">    Y[:,<span class="number">0</span>] = y <span class="comment"># Matrix Y is generated</span></span><br><span class="line">    polynomial_X = np.ones((<span class="built_in">len</span>(x),degree+<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(degree):</span><br><span class="line">        polynomial_X[:,i+<span class="number">1</span>] = np.power(x,i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    polynomial_pseudo = np.dot(np.linalg.inv(np.dot(polynomial_X.T,polynomial_X)), polynomial_X.T) <span class="comment"># Calculate the pseudo-inverse</span></span><br><span class="line">    polynomial_beta = np.dot(polynomial_pseudo,Y)</span><br><span class="line">    <span class="keyword">return</span> np.dot(polynomial_X, polynomial_beta)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">R2</span>(<span class="params">y,predict_y</span>):</span> <span class="comment"># R-square is attached</span></span><br><span class="line">    my = np.mean(y)</span><br><span class="line">    denominator = np.<span class="built_in">sum</span>((y-my)**<span class="number">2</span>)</span><br><span class="line">    numerator = np.<span class="built_in">sum</span>((predict_y-my)**<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> numerator/denominator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">d3y = polynomial_regression(x,y,<span class="number">3</span>)</span><br><span class="line">d5y = polynomial_regression(x,y,<span class="number">5</span>)</span><br><span class="line">d7y = polynomial_regression(x,y,<span class="number">7</span>)</span><br><span class="line">d9y = polynomial_regression(x,y,<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>,<span class="number">2</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line">        axes[i][j].scatter(x,y,color=<span class="string">&#x27;lightgrey&#x27;</span>)</span><br><span class="line">        axes[i][j].plot(x,y0,color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;Original trace&#x27;</span>)</span><br><span class="line">        axes[i][j].grid()</span><br><span class="line">axes[<span class="number">0</span>][<span class="number">0</span>].plot(x, d3y, color=<span class="string">&#x27;g&#x27;</span>, label=<span class="string">&#x27;3 Degree Polynomial&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>][<span class="number">0</span>].legend()</span><br><span class="line">axes[<span class="number">0</span>][<span class="number">0</span>].annotate(<span class="string">&#x27;$R^2=&#123;:.5f&#125;$&#x27;</span>.<span class="built_in">format</span>(R2(y,d3y)), xy=(-<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">axes[<span class="number">0</span>][<span class="number">1</span>].plot(x, d5y, color=<span class="string">&#x27;g&#x27;</span>, label=<span class="string">&#x27;5 Degree Polynomial&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>][<span class="number">1</span>].legend()</span><br><span class="line">axes[<span class="number">0</span>][<span class="number">1</span>].annotate(<span class="string">&#x27;$R^2=&#123;:.5f&#125;$&#x27;</span>.<span class="built_in">format</span>(R2(y,d5y)), xy=(-<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">axes[<span class="number">1</span>][<span class="number">0</span>].plot(x, d7y, color=<span class="string">&#x27;g&#x27;</span>, label=<span class="string">&#x27;7 Degree Polynomial&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>][<span class="number">0</span>].legend()</span><br><span class="line">axes[<span class="number">1</span>][<span class="number">0</span>].annotate(<span class="string">&#x27;$R^2=&#123;:.5f&#125;$&#x27;</span>.<span class="built_in">format</span>(R2(y,d7y)), xy=(-<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">axes[<span class="number">1</span>][<span class="number">1</span>].plot(x, d9y, color=<span class="string">&#x27;g&#x27;</span>, label=<span class="string">&#x27;9 Degree Polynomial&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>][<span class="number">1</span>].legend()</span><br><span class="line">axes[<span class="number">1</span>][<span class="number">1</span>].annotate(<span class="string">&#x27;$R^2=&#123;:.5f&#125;$&#x27;</span>.<span class="built_in">format</span>(R2(y,d9y)), xy=(-<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align=center>
    <img src="https://i.loli.net/2021/04/03/O84gMDLZ6CtsFTG.png" alt="Fig3: Polynomial Regression" style="zoom:75%;" title="Fig3: Polynomial Regression"/>
</div>

<p>A high-degree polynomial does fit samples well, but will the underlying mechanism really perform in that way? What if it meets some extrapolating data? Actually, we are always trapped in a thought that all samples follow a <strong>global</strong> mechanism. The global regression with high orders induces <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Runge%27s_phenomenon">Runge‚Äôs Phenomenon</a> (overfitting problem). How about break the domain into pieces? Say, <strong>piecewise function</strong>.</p>
<hr>
<h2 id="Regression-Spline"><a href="#Regression-Spline" class="headerlink" title="Regression Spline"></a>Regression Spline</h2><h3 id="Piecewise-Regression"><a href="#Piecewise-Regression" class="headerlink" title="Piecewise Regression"></a>Piecewise Regression</h3><blockquote><p>Piecewise regression <strong>breaks the input space</strong> into distinct regions and fit a <strong>different relationship</strong> in each region.</p>
<footer><strong>Jiaming Mao</strong><cite><a target="_blank" rel="noopener" href="https://github.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf">github.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf</a></cite></footer></blockquote>
<h4 id="Piecewise-Polynomials"><a href="#Piecewise-Polynomials" class="headerlink" title="Piecewise Polynomials"></a>Piecewise Polynomials</h4><p>As mentioned above, a <strong>piecewise polynomial function</strong> is obtained by dividing the domain of ùëã into contiguous intervals and representing the function by a <strong>separate degree-d polynomial</strong> in each interval. In mathematics, if there are $n$ knots,</p>
<script type="math/tex; mode=display">
h(x) = \begin{cases}
\beta_0\phi_0(x), & x<\xi_1\\
\beta_1\phi_1(x), & \xi_1\le x <\xi_2\\
\ \ \ \ \ \ \vdots\\
\beta_n\phi_n(x), & x\ge\xi_n
\end{cases}, \text{where } \phi_i(x) = \text{Polynomial}_i(d)</script><p>We use the simulated data as an example. If the function is piecewise, then we should determine the <strong>knots</strong> artificially. Knots decide where to cut and it‚Äôs obvious that the cut intervals influence the polynomials. The chosen knots divide the whole space into equal-length intervals or have some realistic meaning (like age intervals) may be familiar. The most popular way is to choose a potential place where data has high variability <strong>because in those regions the polynomial coefficients can change rapidly</strong>. Here, we choose the third method. After careful observations, $-0.5\pi.0.5\pi,1.5\pi$  are finally chosen (Check <a href="#jump">Fig1</a>).</p>
<p>Of course, we also should choose the degree of polynomial to fit data and generally, the degree of polynomial applies for each interval is consistent. Figure below represents the polynomial of degree <strong>0, 1, 3</strong>, respectively. And green curves are the piecewise regressions.</p>
<figure class="highlight python"><figcaption><span>Click to Unfold</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># divide interval based on knots</span></span><br><span class="line">x_s1 = x[x&lt;-<span class="number">.5</span>*np.pi]</span><br><span class="line">y_s1 = y[:<span class="built_in">len</span>(x_s1)]</span><br><span class="line">x_s2 = x[(x&gt;=-<span class="number">.5</span>*np.pi)&amp;(x&lt;<span class="number">.5</span>*np.pi)]</span><br><span class="line">y_s2 = y[<span class="built_in">len</span>(x_s1):<span class="built_in">len</span>(x_s1)+<span class="built_in">len</span>(x_s2)]</span><br><span class="line">x_s3 = x[(x&gt;=<span class="number">.5</span>*np.pi)&amp;(x&lt;<span class="number">1.5</span>*np.pi)]</span><br><span class="line">y_s3 = y[<span class="built_in">len</span>(x_s1)+<span class="built_in">len</span>(x_s2):<span class="built_in">len</span>(x_s1)+<span class="built_in">len</span>(x_s2)+<span class="built_in">len</span>(x_s3)]</span><br><span class="line">x_s4 = x[x&gt;=<span class="number">1.5</span>*np.pi]</span><br><span class="line">y_s4 = y[<span class="built_in">len</span>(x_s1)+<span class="built_in">len</span>(x_s2)+<span class="built_in">len</span>(x_s3):]</span><br><span class="line"></span><br><span class="line">piece_x = [x_s1,x_s2,x_s3, x_s4]</span><br><span class="line">piece_y = [y_s1,y_s2,y_s3, y_s4]</span><br><span class="line">titles = [<span class="string">&#x27;Piecewise Constant&#x27;</span>,<span class="string">&#x27;Piecewise Linear&#x27;</span>, <span class="string">&#x27;Piecewise Cubic Polynomial&#x27;</span>]</span><br><span class="line">degrees = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>]</span><br><span class="line"><span class="comment"># piecewise regression plot</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>,<span class="number">3</span>,figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]:</span><br><span class="line">    axes[i].scatter(x,y,color=<span class="string">&#x27;lightgrey&#x27;</span>)</span><br><span class="line">    axes[i].plot(x,y0,label=<span class="string">&#x27;Orignial trace&#x27;</span>,color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    axes[i].grid(ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].axvline(-<span class="number">0.5</span>*np.pi,color=<span class="string">&#x27;black&#x27;</span>,ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].axvline(<span class="number">0.5</span>*np.pi,color=<span class="string">&#x27;black&#x27;</span>,ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].axvline(<span class="number">1.5</span>*np.pi,color=<span class="string">&#x27;black&#x27;</span>,ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].<span class="built_in">set</span>(title=titles[i])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        sy = polynomial_regression(piece_x[j],piece_y[j],degrees[i])</span><br><span class="line">        axes[i].plot(piece_x[j],sy,color=<span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<div align=center>
    <img src="https://i.loli.net/2021/04/04/Ov67sJ4BK5gGfFS.png" alt="Fig4: Piecewise Polynomial Regression" style="zoom:75%;" title="Fig4: Piecewise Polynomial Regression"/>
</div>


<p>Those curves can describe the relationships in their interval but at knots, they jump. In reality, the discrete underlying mechanisms are rare. How to make them <strong>continuous and smooth</strong>? Here comes the splines.</p>
<hr>
<h3 id="Splines"><a href="#Splines" class="headerlink" title="Splines"></a>Splines</h3><p>Before talking regression spline, spline interpolation, as the inspiration, enjoys the priority to be talked.</p>
<h4 id="Spline-Interpolation"><a href="#Spline-Interpolation" class="headerlink" title="Spline Interpolation"></a>Spline Interpolation</h4><p>Spline is a drawing tool with a high elasticity. In shipbuilding area, engineers pin down both ends of it and press it to make it bending. In this way, a beautiful smooth curve can be drawn. </p>
<p>In numerical analysis, mathematicians have jobs to construct new data within the range f a discrete set of known data points, which is interpolation. Interpolation aims to find a <strong>continuous</strong> function that <strong>goes through</strong> all points (Fig5). Mathematicians create spline interpolation combining piecewise thought and spline thought. <strong>Piecewise function actually fixes the two ends of ‚Äúspline‚Äù and spline takes the obligation to smooth the curve</strong>. </p>
<div align=center>
    <img src="https://i.loli.net/2021/04/17/yUV3B26GMJdqn7L.png" alt="Fig5: Spline Interpolation" style="zoom:33%;" title="Fig5: Spline Interpolation"/>
</div>

<p><strong>Cubic Interpolation</strong> is always chosen.<span id='smoothness proof'></span> There is a <a target="_blank" rel="noopener" href="https://www.math.ntnu.no/emner/TMA4215/2008h/cubicsplines.pdf">proof</a> that why cubic interpolation is the <strong>the smoothest one</strong> if we manually define smoothness as, (we will talk about smoothness later)</p>
<script type="math/tex; mode=display">
\mu (f) = \int_a^b (f''(x))^2 \mathrm{d}x</script><h4 id="Regression-Splines"><a href="#Regression-Splines" class="headerlink" title="Regression Splines"></a>Regression Splines</h4><p>Spline interpolation finishes the job to link all points smoothly. However, it works only when the data set is small, monotonic and has no disturbing terms. It doesn‚Äôt matter that we can <strong>operate like interpolation</strong> to link piecewise regressions smoothly, which produces regression splines.</p>
<p>Firstly, we should satisfy the <strong>continuity</strong> property, which is crucial in interpolation.</p>
<blockquote><p>In mathematics analysis, a <strong>continuous function</strong> is a function that does not have any abrupt changes in value.</p>
<footer><strong>Wikipedia</strong><cite><a target="_blank" rel="noopener" href="https://wikipedia.org/wiki/continuous_function">wikipedia.org/wiki/continuous_function</a></cite></footer></blockquote>
<p>So we hope that curves can link each other at knots to <strong>prevent the jump</strong>.</p>
<script type="math/tex; mode=display">
h(x) = \begin{cases}
\beta_0\phi_0(x), & x<\xi_1\\
\beta_1\phi_1(x), & \xi_1\le x <\xi_2\\
\hspace{4em} \vdots\\
\beta_n\phi_n(x), & x\ge\xi_{n}
\end{cases}, \text{where } \phi_i(x) = \text{Polynomial}_i(d)\\

s.t. \begin{cases}
\beta_0\phi_0(\xi_1)= \beta_1\phi_1(\xi_1)\\
\beta_1\phi_1(\xi_2) = \beta_2\phi_2(\xi_2)\\
\hspace{4.2em} \vdots\\
\beta_{n-1}\phi_{n-1}(\xi_{n}) =\beta_n\phi_n(\xi_{n})
\end{cases}</script><p>Next, we should satisfy the <strong>smoothness</strong> property.</p>
<blockquote><p>In mathematical analysis, the <strong>smoothness</strong> of a function is a property measured by <strong>the number of continuous derivatives</strong> it has over some domain. At the very minimum, a function could be considered ‚Äúsmooth‚Äù if it is differentiable everywhere (hence continuous).</p>
<footer><strong>Wikipedia</strong><cite><a target="_blank" rel="noopener" href="https://wikipedia.org/wiki/smoothness">wikipedia.org/wiki/smoothness</a></cite></footer></blockquote>
<p>For high-order polynomials ($d&gt;1$), their gradients changes by $x$. As it‚Äôs known to us all, gradient controls the change rate (tangent line). Therefore, we require the gradients (it can be up to $d-1$ order), are the same at knots to make the curve more smooth. So there is a definition.</p>
<blockquote><p>A degree-d <strong>regression spline</strong> is a piecewise degree-d polynomial, with continuity in derivatives up to degree $d-1$ (make it the most smooth) at each knot.</p>
<footer><strong>Jiaming Mao</strong><cite><a target="_blank" rel="noopener" href="https://github.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf">github.com/jiamingmao/data-analysis/tree/master/Lectures/Regression.pdf</a></cite></footer></blockquote>
<p>In mathematics, it can be written as,</p>
<script type="math/tex; mode=display">
h(x) = \begin{cases}
\beta_0\phi_0(x), & x<\xi_1\\
\beta_1\phi_1(x), & \xi_1\le x <\xi_2\\
\hspace{4em} \vdots\\
\beta_n\phi_n(x), & x\ge\xi_{n}
\end{cases}, \text{where } \phi_i(x) = \text{Polynomial}_i(d)\\
\hspace{1em}\\

s.t. \begin{cases}
\beta_0\phi_0(\xi_1)= \beta_1\phi_1(\xi_1)\\
\beta_1\phi_1(\xi_2) = \beta_2\phi_2(\xi_2)\\ 
\hspace{4.2em} \vdots \hspace{8em}\Bigg\}\text{Continuity Condition}\\
\beta_{n-1}\phi_{n-1}(\xi_{n}) =\beta_n\phi_n(\xi_{n})\\
\beta_0\phi^{(1)}_0(\xi_1)= \beta_1\phi^{(1)}_1(\xi_1)\\
\hspace{4.2em}\vdots\\
\beta_0\phi^{(d-1)}_0(\xi_1)= \beta_1\phi^{(d-1)}_1(\xi_1)\\
\beta_1\phi_1^{(1)}(\xi_2) = \beta_2\phi^{(1)}_2(\xi_2)\hspace{5em}\Bigg\}\text{Smoothness Condition}\\
\hspace{4.2em} \vdots\\
\beta_{n-1}\phi^{(d-1)}_{n-1}(\xi_{n}) =\beta_n\phi^{(d-1)}_n(\xi_{n})\\
\end{cases}</script><p>Actually, constraints can be eliminated by substituting values into the objective function. The final result is,</p>
<script type="math/tex; mode=display">
h(x) = \sum_{i=0}^d \beta_ix^i + \sum_{j=d+1}^{d+n}\beta_{j}(x-\xi_{j-d})_+^d</script><p>, where $d$ is the degree of polynomial and $n$ is the numbers of knots. $(x-\xi_{j-d})_+$ is an indicate function such that, </p>
<script type="math/tex; mode=display">
(x-\xi_{j-d})_+=\begin{cases}
0, &x<\xi_{j-d}\\
x-\xi_{j-d}, &x\ge\xi_{j-d}
\end{cases}</script><p>As you can see, obviously, there are $(d+n+1)\beta$ to be estimated. Thus, the regression spline has $d+n+1$ degrees of freedom.</p>
<p>Still, the generated sample is our example to illustrate $1,2,3$ degree splines respectively ($R^2$ is attached). Comparing the $R^2$, you will find that a <strong>cubic spline</strong> (degree-3 spline) is even a little bit better than <strong>degree-5</strong> polynomial regression (0.35315 vs 0.34987). What‚Äôs more, it has a lower order.</p>
<figure class="highlight python"><figcaption><span>Unclick to Unfold</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">indicator</span>(<span class="params">x, knot</span>):</span></span><br><span class="line">    <span class="keyword">if</span> x&lt;knot:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x-knot</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splines</span>(<span class="params">x,y,degree,knots</span>):</span></span><br><span class="line">    Y = np.zeros((<span class="built_in">len</span>(y),<span class="number">1</span>))</span><br><span class="line">    Y[:,<span class="number">0</span>] = y <span class="comment"># Matrix Y is generated</span></span><br><span class="line">    spline_X = np.zeros((<span class="built_in">len</span>(x),degree+<span class="number">1</span>+<span class="built_in">len</span>(knots)))</span><br><span class="line">    spline_X[:,<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(degree):</span><br><span class="line">        spline_X[:,i+<span class="number">1</span>] = np.power(x,i+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(knots)):</span><br><span class="line">        judgex = np.zeros(<span class="built_in">len</span>(x))</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">            judgex[k] = indicator(x[k], knots[j])**degree</span><br><span class="line">        spline_X[:,degree+<span class="number">1</span>+j] = judgex <span class="comment"># degree+1 -&gt; place for the 1st indicator</span></span><br><span class="line">    spline_pseudo = np.dot(np.linalg.inv(np.dot(spline_X.T,spline_X)), spline_X.T) <span class="comment"># Calculate the pseudo-inverse</span></span><br><span class="line">    spline_beta = np.dot(spline_pseudo,Y)</span><br><span class="line">    <span class="keyword">return</span> np.dot(spline_X, spline_beta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># b0 + b1x + b2x^2 + b3x^3 + b4(x+1)^3 + b5(x-1)^3</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>,<span class="number">3</span>,figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    axes[i].grid(ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].scatter(x, y,color=<span class="string">&#x27;lightgrey&#x27;</span>)</span><br><span class="line">    axes[i].plot(x, y0, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Original&#x27;</span>) <span class="comment"># oringinal trace</span></span><br><span class="line">    sply = splines(x,y,i+<span class="number">1</span>,[-<span class="number">0.5</span>*np.pi,<span class="number">.5</span>*np.pi,<span class="number">1.5</span>*np.pi])</span><br><span class="line">    axes[i].plot(x, sply, color=<span class="string">&#x27;g&#x27;</span>, label = <span class="string">&#x27;&#123;&#125; Degree Spline&#x27;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    axes[i].axvline(-<span class="number">0.5</span>*np.pi,color=<span class="string">&#x27;black&#x27;</span>,ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].axvline(<span class="number">0.5</span>*np.pi,color=<span class="string">&#x27;black&#x27;</span>,ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].axvline(<span class="number">1.5</span>*np.pi,color=<span class="string">&#x27;black&#x27;</span>,ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">    axes[i].legend()</span><br><span class="line">    axes[i].annotate(<span class="string">&#x27;$R^2=&#123;:.5f&#125;$&#x27;</span>.<span class="built_in">format</span>(R2(y,sply)), xy=(-<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align=center>
    <img src="https://i.loli.net/2021/04/04/kfnqFHY6s5cyoM8.png" alt="Fig6: Different Degree Splines" style="zoom:75%;" title="Fig6: Different Degree Splines"/>
</div>

<h4 id="Cubic-Spline"><a href="#Cubic-Spline" class="headerlink" title="Cubic Spline"></a>Cubic Spline</h4><p><strong>Cubic Spline</strong> is most popular model in Regression Spline because it can be used in nearly every case, where global polynomial plays a bad role, while remaining a relatively low order. Why cubic?</p>
<ul>
<li><p>It‚Äôs an <strong>empirical experience</strong>. Cube is a watershed that separates high and low orders, which ensures that it can explain a lot of high-order underlying mechanisms while somehow preventing Runge‚Äôs Phenomenon (Lower is better for order).</p>
</li>
<li><p>Inheriting the properties of <a href="#smoothness proof">cubic interpolation</a>, it is the smoothest one.</p>
</li>
</ul>
<h4 id="Natural-Spline"><a href="#Natural-Spline" class="headerlink" title="Natural Spline"></a>Natural Spline</h4><blockquote><p>We know that the behavior of polynomials that are fit to the data tends to be erratic near the boundaries. Such variability can be dangerous. These problems are resembled by splines, too. The polynomials fit beyond the boundary knots behave even more wildly than the corresponding global polynomials in that region. <strong>To smooth the polynomial beyond the boundary knots, we will use a special type of spline known as Natural Spline.</strong></p>
<footer><strong>Analytics Vidhya</strong></footer></blockquote>
<p>Actually, if we denote the spline function as $S(x)$, the approach to smooth ‚Äúthe boundary‚Äù is,</p>
<script type="math/tex; mode=display">
S''(\xi_1) = S''(\xi_n) = 0</script><p>, meaning that the function is linear beyond the boundary knots. This each equal-to-zero equation reduces degrees of freedom by 2. Say, the total degree of freedom is $d+n-3$. (Polynomial degree is $d$ and there are $n$ knots.) Below is a plot where you can compare cubic spline, natural cubic spline.</p>
<figure class="highlight python"><figcaption><span>Click to Unfold. Natural Spline Function Code is from Stackoverflow.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_natural_cubic_spline_model</span>(<span class="params">x, y, minval=<span class="literal">None</span>, maxval=<span class="literal">None</span>, n_knots=<span class="literal">None</span>, knots=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Get a natural cubic spline model for the data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For the knots, give (a) `knots` (as an array) or (b) minval, maxval and n_knots.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If the knots are not directly specified, the resulting knots are equally</span></span><br><span class="line"><span class="string">    space within the *interior* of (max, min).  That is, the endpoints are</span></span><br><span class="line"><span class="string">    *not* included as knots.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    x: np.array of float</span></span><br><span class="line"><span class="string">        The input data</span></span><br><span class="line"><span class="string">    y: np.array of float</span></span><br><span class="line"><span class="string">        The outpur data</span></span><br><span class="line"><span class="string">    minval: float </span></span><br><span class="line"><span class="string">        Minimum of interval containing the knots.</span></span><br><span class="line"><span class="string">    maxval: float </span></span><br><span class="line"><span class="string">        Maximum of the interval containing the knots.</span></span><br><span class="line"><span class="string">    n_knots: positive integer </span></span><br><span class="line"><span class="string">        The number of knots to create.</span></span><br><span class="line"><span class="string">    knots: array or list of floats </span></span><br><span class="line"><span class="string">        The knots.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    model: a model object</span></span><br><span class="line"><span class="string">        The returned model will have following method:</span></span><br><span class="line"><span class="string">        - predict(x):</span></span><br><span class="line"><span class="string">            x is a numpy array. This will return the predicted y-values.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> knots:</span><br><span class="line">        spline = NaturalCubicSpline(knots=knots)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        spline = NaturalCubicSpline(<span class="built_in">max</span>=maxval, <span class="built_in">min</span>=minval, n_knots=n_knots)</span><br><span class="line"></span><br><span class="line">    p = Pipeline([</span><br><span class="line">        (<span class="string">&#x27;nat_cubic&#x27;</span>, spline),</span><br><span class="line">        (<span class="string">&#x27;regression&#x27;</span>, LinearRegression(fit_intercept=<span class="literal">True</span>))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    p.fit(x, y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AbstractSpline</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Base class for all spline basis expansions.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, <span class="built_in">max</span>=<span class="literal">None</span>, <span class="built_in">min</span>=<span class="literal">None</span>, n_knots=<span class="literal">None</span>, n_params=<span class="literal">None</span>, knots=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> knots <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> n_knots:</span><br><span class="line">                n_knots = self._compute_n_knots(n_params)</span><br><span class="line">            knots = np.linspace(<span class="built_in">min</span>, <span class="built_in">max</span>, num=(n_knots + <span class="number">2</span>))[<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            <span class="built_in">max</span>, <span class="built_in">min</span> = np.<span class="built_in">max</span>(knots), np.<span class="built_in">min</span>(knots)</span><br><span class="line">        self.knots = np.asarray(knots)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">n_knots</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.knots)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaturalCubicSpline</span>(<span class="params">AbstractSpline</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Apply a natural cubic basis expansion to an array.</span></span><br><span class="line"><span class="string">    The features created with this basis expansion can be used to fit a</span></span><br><span class="line"><span class="string">    piecewise cubic function under the constraint that the fitted curve is</span></span><br><span class="line"><span class="string">    linear *outside* the range of the knots..  The fitted curve is continuously</span></span><br><span class="line"><span class="string">    differentiable to the second order at all of the knots.</span></span><br><span class="line"><span class="string">    This transformer can be created in two ways:</span></span><br><span class="line"><span class="string">      - By specifying the maximum, minimum, and number of knots.</span></span><br><span class="line"><span class="string">      - By specifying the cutpoints directly.  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If the knots are not directly specified, the resulting knots are equally</span></span><br><span class="line"><span class="string">    space within the *interior* of (max, min).  That is, the endpoints are</span></span><br><span class="line"><span class="string">    *not* included as knots.</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    min: float </span></span><br><span class="line"><span class="string">        Minimum of interval containing the knots.</span></span><br><span class="line"><span class="string">    max: float </span></span><br><span class="line"><span class="string">        Maximum of the interval containing the knots.</span></span><br><span class="line"><span class="string">    n_knots: positive integer </span></span><br><span class="line"><span class="string">        The number of knots to create.</span></span><br><span class="line"><span class="string">    knots: array or list of floats </span></span><br><span class="line"><span class="string">        The knots.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compute_n_knots</span>(<span class="params">self, n_params</span>):</span></span><br><span class="line">        <span class="keyword">return</span> n_params</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">n_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.n_knots - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X, **transform_params</span>):</span></span><br><span class="line">        X_spl = self._transform_array(X)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, pd.Series):</span><br><span class="line">            col_names = self._make_names(X)</span><br><span class="line">            X_spl = pd.DataFrame(X_spl, columns=col_names, index=X.index)</span><br><span class="line">        <span class="keyword">return</span> X_spl</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_names</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        first_name = <span class="string">&quot;&#123;&#125;_spline_linear&quot;</span>.<span class="built_in">format</span>(X.name)</span><br><span class="line">        rest_names = [<span class="string">&quot;&#123;&#125;_spline_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(X.name, idx)</span><br><span class="line">                      <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(self.n_knots - <span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">return</span> [first_name] + rest_names</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_transform_array</span>(<span class="params">self, X, **transform_params</span>):</span></span><br><span class="line">        X = X.squeeze()</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            X_spl = np.zeros((X.shape[<span class="number">0</span>], self.n_knots - <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">except</span> IndexError: <span class="comment"># For arrays with only one element</span></span><br><span class="line">            X_spl = np.zeros((<span class="number">1</span>, self.n_knots - <span class="number">1</span>))</span><br><span class="line">        X_spl[:, <span class="number">0</span>] = X.squeeze()</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">d</span>(<span class="params">knot_idx, x</span>):</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">ppart</span>(<span class="params">t</span>):</span> <span class="keyword">return</span> np.maximum(<span class="number">0</span>, t)</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">cube</span>(<span class="params">t</span>):</span> <span class="keyword">return</span> t*t*t</span><br><span class="line">            numerator = (cube(ppart(x - self.knots[knot_idx]))</span><br><span class="line">                         - cube(ppart(x - self.knots[self.n_knots - <span class="number">1</span>])))</span><br><span class="line">            denominator = self.knots[self.n_knots - <span class="number">1</span>] - self.knots[knot_idx]</span><br><span class="line">            <span class="keyword">return</span> numerator / denominator</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.n_knots - <span class="number">2</span>):</span><br><span class="line">            X_spl[:, i+<span class="number">1</span>] = (d(i, X) - d(self.n_knots - <span class="number">2</span>, X)).squeeze()</span><br><span class="line">        <span class="keyword">return</span> X_spl</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><figcaption><span>Click to Unfold (Plot Code)</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> statsmodels.api <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> patsy <span class="keyword">import</span> dmatrix</span><br><span class="line"><span class="keyword">import</span> statsmodels.formula.api <span class="keyword">as</span> smf</span><br><span class="line">natural = get_natural_cubic_spline_model(x, y, minval=<span class="built_in">min</span>(x), maxval=<span class="built_in">max</span>(x), knots=[-<span class="number">.5</span>*np.pi, <span class="number">.5</span>*np.pi, <span class="number">1.5</span>*np.pi])</span><br><span class="line">pred3 = natural.predict(x)</span><br><span class="line">fig,ax = plt.subplots(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">ax.grid(ls=<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">ax.scatter(x,y,color=<span class="string">&#x27;lightgrey&#x27;</span>)</span><br><span class="line">ax.plot(x,y0,color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;Original Trace&#x27;</span>)</span><br><span class="line">ax.plot(x,pred3,color=<span class="string">&#x27;green&#x27;</span>,label=<span class="string">&#x27;Natural Cubic Spline&#x27;</span>)</span><br><span class="line">sply = splines(x,y,i+<span class="number">1</span>,[-<span class="number">0.5</span>*np.pi,<span class="number">.5</span>*np.pi,<span class="number">1.5</span>*np.pi])</span><br><span class="line">ax.plot(x, sply, color=<span class="string">&#x27;navy&#x27;</span>, label = <span class="string">&#x27;Cubic Spline&#x27;</span>)</span><br><span class="line">ax.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<div align=center>
    <img src="https://i.loli.net/2021/04/04/lOfPnsW8NeQgDSH.png" alt="Fig7: Cubic Spline and Natural Cubic Spline" style="zoom:75%;" title="Fig7: Cubic Spline and Natural Cubic Spline"/>
</div>

<p>Clearly, natural spline enforces the flatness beyond the boundary. It may be useful in many real-life cases like wage estimation for the old are generally not able to earn a lot.</p>
<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this report, we briefly go over the OLS estimator in <strong>matrix expression</strong>. And <strong>basis linear expansion</strong> is covered to introduce the polynomial regression. <strong>Polynomial regression</strong> leads some problems like overfitting because of high-degree global estimation. <strong>Piecewise function</strong> thereby comes for help. To satisfy the <strong>continuity</strong> and <strong>smoothness</strong>, <strong>splines</strong> is a powerful tool. That‚Äôs why we need ‚ÄúRegression Splines‚Äù. Then, we finally talk about two famous splines, <strong>cubic splines and natural splines</strong>.</p>
<p>How to realize Splines? Add constraints about <strong>derivatives</strong>. Codes and figures are attached to help you understand better.</p>
<p><strong>Thanks for Reading!</strong></p>
<h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ol>
<li>Jiaming Mao,  <a target="_blank" rel="noopener" href="https://github.com/jiamingmao/data-analysis">Data Analysis for Economics</a></li>
<li>Hsuan-tien Lin, <a target="_blank" rel="noopener" href="https://github.com/RedstoneWill/HsuanTienLin_MachineLearning">Machine Learning Foundation</a></li>
<li>Norwegian University of Science and Technology, <a target="_blank" rel="noopener" href="https://www.math.ntnu.no/emner/TMA4215/2008h/cubicsplines.pdf">Natural Cubic Splines</a></li>
<li>Analytics Vidhya‚Äôs Blog, <a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes/">Regression Splines</a></li>
<li>Stackoverflow, <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/51321100/python-natural-smoothing-splines">Python Natural Smoothing Splines</a></li>
<li>Wikipedia, <a target="_blank" rel="noopener" href="https://wikipedia.org/wiki/smoothness">Splines</a></li>
</ol>
<h4 id="Picture-References"><a href="#Picture-References" class="headerlink" title="Picture References"></a>Picture References</h4><ul>
<li>Cover: $  $  <a class="tag is-dark is-medium" href="https://www.pixiv.net/artworks/78167001" target="_blank"><br><span class="icon"><i class="fas fa-camera"></i></span>&nbsp;&nbsp; Pixiv by T5 </a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Report - Regression Splines</p><p><a href="http://example.com/2021/03/31/Report-Regression-Spline/">http://example.com/2021/03/31/Report-Regression-Spline/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>‰ΩúËÄÖ</h6><p>Junwei Lin</p></div></div><div class="level-item is-narrow"><div><h6>ÂèëÂ∏É‰∫é</h6><p>2021-03-31</p></div></div><div class="level-item is-narrow"><div><h6>Êõ¥Êñ∞‰∫é</h6><p>2021-04-17</p></div></div><div class="level-item is-narrow"><div><h6>ËÆ∏ÂèØÂçèËÆÆ</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/">ÂæÆËßÇËÆ°Èáè</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/05/04/Challenge-Ordered-Logit/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Challenge: Ordered Logit</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/03/29/VC%20Dimension/"><span class="level-item">Report - VC Dimension</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://i.loli.net/2021/04/01/s3S5IyEexAjClVm.png" alt="Chizuru7"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chizuru7</p><p class="is-size-6 is-block">A Senior in XMU</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Earth, Solar System</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">ÊñáÁ´†</p><a href="/archives"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">ÂàÜÁ±ª</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Ê†áÁ≠æ</p><a href="/tags"><p class="title">1</p></a></div></div></nav><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Chizuru7"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Mail" href="mailto:362296573@qq.com"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Steam" href="https://s.team/p/cfhj-vnmw/QDPBQJDG"><i class="fab fa-steam"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">ÁõÆÂΩï</h3><ul class="menu-list"><li><a class="level is-mobile" href="#From-Linear-to-Polynomial"><span class="level-left"><span class="level-item">1</span><span class="level-item">From Linear to Polynomial</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Linear-Regression"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Linear Regression</span></span></a></li><li><a class="level is-mobile" href="#Polynomial"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">Polynomial</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Basis-Expansion"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">Basis Expansion</span></span></a></li><li><a class="level is-mobile" href="#Polynomial-Regression"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">Polynomial Regression</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Regression-Spline"><span class="level-left"><span class="level-item">2</span><span class="level-item">Regression Spline</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Piecewise-Regression"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Piecewise Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Piecewise-Polynomials"><span class="level-left"><span class="level-item">2.1.1</span><span class="level-item">Piecewise Polynomials</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Splines"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Splines</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Spline-Interpolation"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">Spline Interpolation</span></span></a></li><li><a class="level is-mobile" href="#Regression-Splines"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">Regression Splines</span></span></a></li><li><a class="level is-mobile" href="#Cubic-Spline"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">Cubic Spline</span></span></a></li><li><a class="level is-mobile" href="#Natural-Spline"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">Natural Spline</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Summary"><span class="level-left"><span class="level-item">3</span><span class="level-item">Summary</span></span></a><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">References</span></span></a></li><li><a class="level is-mobile" href="#Picture-References"><span class="level-left"><span class="level-item">3.1.2</span><span class="level-item">Picture References</span></span></a></li></ul></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">ÊúÄÊñ∞ÊñáÁ´†</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-05-04T06:56:34.000Z">2021-05-04</time></p><p class="title"><a href="/2021/05/04/Challenge-Ordered-Logit/">Challenge: Ordered Logit</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/03/31/Report-Regression-Spline/"><img src="/gallery/amiya_Splines.png" alt="Report - Regression Splines"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-03-31T09:38:41.000Z">2021-03-31</time></p><p class="title"><a href="/2021/03/31/Report-Regression-Spline/">Report - Regression Splines</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2021/03/29/VC%20Dimension/"><img src="/gallery/VCcover.jpg" alt="Report - VC Dimension"></a></figure><div class="media-content"><p class="date"><time dateTime="2021-03-28T16:00:00.000Z">2021-03-29</time></p><p class="title"><a href="/2021/03/29/VC%20Dimension/">Report - VC Dimension</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-20T16:00:00.000Z">2021-03-21</time></p><p class="title"><a href="/2021/03/21/Data%20Analysis%20for%20Econ%20Intro/">Data Analysis for Economics Intro</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">ÂΩíÊ°£</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">‰∫îÊúà 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">‰∏âÊúà 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Ê†áÁ≠æ</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E5%BE%AE%E8%A7%82%E8%AE%A1%E9%87%8F/"><span class="tag">ÂæÆËßÇËÆ°Èáè</span><span class="tag">4</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">ËÆ¢ÈòÖÊõ¥Êñ∞</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="ËÆ¢ÈòÖ"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">ÈìæÊé•</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://i.loli.net/2021/04/03/wDoVkMUd9LFYgqA.png" alt="Chizuru7&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Junwei Lin</span>¬†¬†Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>¬†&amp;¬†<a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="ÂõûÂà∞È°∂Á´Ø" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "Ê≠§ÁΩëÁ´ô‰ΩøÁî®CookieÊù•ÊîπÂñÑÊÇ®ÁöÑ‰ΩìÈ™å„ÄÇ",
          dismiss: "Áü•ÈÅì‰∫ÜÔºÅ",
          allow: "ÂÖÅËÆ∏‰ΩøÁî®Cookie",
          deny: "ÊãíÁªù",
          link: "‰∫ÜËß£Êõ¥Â§ö",
          policy: "CookieÊîøÁ≠ñ",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="ÊÉ≥Ë¶ÅÊü•Êâæ‰ªÄ‰πà..."></div><a class="searchbox-close" href="javascript:;">√ó</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"ÊÉ≥Ë¶ÅÊü•Êâæ‰ªÄ‰πà...","untitled":"(Êó†Ê†áÈ¢ò)","posts":"ÊñáÁ´†","pages":"È°µÈù¢","categories":"ÂàÜÁ±ª","tags":"Ê†áÁ≠æ"});
        });</script></body></html>